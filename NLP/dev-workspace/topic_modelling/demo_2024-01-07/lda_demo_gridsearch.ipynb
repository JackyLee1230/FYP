{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo ipynb for LDA\n",
    "\n",
    "Creating training pipeline on different situations (15/01/2024)\n",
    "\n",
    "- For all games (using 0.1 and 0.5 of the whole dataset for pipeline developments)\n",
    "- For top 11 genres\n",
    "\n",
    "- (if possible): per game, focus on large and small (indie) games\n",
    "\n",
    "Do the same thing for all three model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "import gensim\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nltk stopwords\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# download spacy stopwords\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training conditions\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class TRAINING_CONDS(Enum):\n",
    "    ALL_GAMES = 1\n",
    "    ALL_GAMES_LARGE = 2\n",
    "    BY_GENRE = 3\n",
    "    ALL_GAMES_TINY = 4\n",
    "    BY_GAME = 5\n",
    "\n",
    "# training condition\n",
    "training_cond = TRAINING_CONDS.BY_GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 75499 entries, 57735 to 133233\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   index         75499 non-null  int64 \n",
      " 1   app_id        75499 non-null  int64 \n",
      " 2   app_name      75499 non-null  object\n",
      " 3   review_text   75499 non-null  object\n",
      " 4   review_score  75499 non-null  int64 \n",
      " 5   review_votes  75499 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "\n",
    "GENRES = ['Action', 'Indie', 'Adventure', 'RPG', 'Strategy', 'Simulation', 'Free to Play', 'Causal', 'Massively Multiplayer', 'Racing', 'Sports']\n",
    "GAMES = ['Terraria', 'PAYDAY 2', 'Dota 2', 'Rocket League', 'Undertale', 'Left 4 Dead 2', 'Warframe', 'Portal 2', 'Grand Theft Auto V', 'Fallout: New Vegas']\n",
    "\n",
    "training_genre_id = 9\n",
    "training_game_id = 0\n",
    "\n",
    "if training_cond == TRAINING_CONDS.ALL_GAMES:\n",
    "    dataset_path = Path('../dataset_cleaned_heartless_sampled_for_demo.pkl')\n",
    "elif training_cond == TRAINING_CONDS.ALL_GAMES_LARGE:\n",
    "    dataset_path = Path('../dataset_cleaned_heartless_sampled_for_demo_large.pkl')\n",
    "elif training_cond == TRAINING_CONDS.ALL_GAMES_TINY:\n",
    "    dataset_path = Path('../dataset_cleaned_heartless_sampled_for_demo_tiny.pkl')\n",
    "elif training_cond == TRAINING_CONDS.BY_GENRE:\n",
    "    dataset_path = Path(f'../../dataset/topic_modelling/top_11_genres/{training_genre_id:02}_{GENRES[training_genre_id]}.pkl')\n",
    "elif training_cond == TRAINING_CONDS.BY_GAME:\n",
    "    dataset_path = Path(f'../../dataset/topic_modelling/top_10_games/{training_game_id:02}_{GAMES[training_game_id]}.pkl')\n",
    "\n",
    "# dataset_path = Path('../../dataset/topic_modelling/top_10_games/00_Terraria.pkl')\n",
    "\n",
    "# dataset_path = Path('../dataset_cleaned_heartless_sampled_for_demo.pkl')\n",
    "\n",
    "\n",
    "dataset = pd.read_pickle(dataset_path)\n",
    "\n",
    "dataset.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../sa')\n",
    "\n",
    "%autoreload 2\n",
    "import str_cleaning_functions\n",
    "\n",
    "def cleaning(df, review):\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links2(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.clean(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.deEmojify(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_non_letters(x))\n",
    "    df[review] = df[review].apply(lambda x: x.lower())\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_stopword(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))\n",
    "\n",
    "def cleaning_strlist(str_list):\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.remove_links(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.remove_links2(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.clean(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.deEmojify(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.remove_non_letters(x), str_list))\n",
    "    str_list = list(map(lambda x: x.lower(), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.unify_whitespaces(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.remove_stopword(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.unify_whitespaces(x), str_list))\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply data preprocessing\n",
    "\n",
    "cleaning(dataset, 'review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['review_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['werewolf riding unicorn shooting rainbows gun build teleporters find hair dresser spider cavern get sword shoots cats take lord moon using yoyo summon sharknado minion shoots sharks enemies find sky temples air wyverns spawn buy music box wizard go record music like playing base whenever want go build castle made entirely white marble would seem thing minecraft game dimension trust get used start learning game terraria simply one satisfying sandbox experiences may sound rude compared minecraft imagination',\n",
       "       'copies game go around giving people look sad', 'introduction',\n",
       "       ...,\n",
       "       'game game start newb get pro hours entertainment even get bored get mods newb',\n",
       "       'far one greatest games played yet',\n",
       "       'game awesome eye cthulhu possible'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# t = nltk.word_tokenize(X[0])\n",
    "# tt = nltk.pos_tag(t)\n",
    "# tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do lemmatization, but not stemming (as part of speech is important in topic modelling)\n",
    "# use nltk wordnet for lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# from https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word\n",
    "\n",
    "# from: https://www.cnblogs.com/jclian91/p/9898511.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None     # if none -> created as noun by wordnet\n",
    "    \n",
    "def lemmatization(text):\n",
    "   # use nltk to get PoS tag\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "    # then we only need adj, adv, verb, noun\n",
    "    # convert from nltk Penn Treebank tag to wordnet tag\n",
    "    wn_tagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1])), tagged))\n",
    "\n",
    "    # lemmatize by the PoS\n",
    "    lemmatized = list(map(lambda x: lemma.lemmatize(x[0], pos=x[1] if x[1] else wordnet.NOUN), wn_tagged))\n",
    "    # lemma.lemmatize(wn_tagged[0], pos=wordnet.NOUN)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the data\n",
    "\n",
    "X_lemmatized = list(map(lambda x: lemmatization(x), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['werewolf',\n",
       " 'rid',\n",
       " 'unicorn',\n",
       " 'shooting',\n",
       " 'rainbow',\n",
       " 'gun',\n",
       " 'build',\n",
       " 'teleporters',\n",
       " 'find',\n",
       " 'hair',\n",
       " 'dresser',\n",
       " 'spider',\n",
       " 'cavern',\n",
       " 'get',\n",
       " 'sword',\n",
       " 'shoot',\n",
       " 'cat',\n",
       " 'take',\n",
       " 'lord',\n",
       " 'moon',\n",
       " 'use',\n",
       " 'yoyo',\n",
       " 'summon',\n",
       " 'sharknado',\n",
       " 'minion',\n",
       " 'shoot',\n",
       " 'sharks',\n",
       " 'enemy',\n",
       " 'find',\n",
       " 'sky',\n",
       " 'temple',\n",
       " 'air',\n",
       " 'wyverns',\n",
       " 'spawn',\n",
       " 'buy',\n",
       " 'music',\n",
       " 'box',\n",
       " 'wizard',\n",
       " 'go',\n",
       " 'record',\n",
       " 'music',\n",
       " 'like',\n",
       " 'play',\n",
       " 'base',\n",
       " 'whenever',\n",
       " 'want',\n",
       " 'go',\n",
       " 'build',\n",
       " 'castle',\n",
       " 'make',\n",
       " 'entirely',\n",
       " 'white',\n",
       " 'marble',\n",
       " 'would',\n",
       " 'seem',\n",
       " 'thing',\n",
       " 'minecraft',\n",
       " 'game',\n",
       " 'dimension',\n",
       " 'trust',\n",
       " 'get',\n",
       " 'use',\n",
       " 'start',\n",
       " 'learning',\n",
       " 'game',\n",
       " 'terrarium',\n",
       " 'simply',\n",
       " 'one',\n",
       " 'satisfy',\n",
       " 'sandbox',\n",
       " 'experience',\n",
       " 'may',\n",
       " 'sound',\n",
       " 'rude',\n",
       " 'compare',\n",
       " 'minecraft',\n",
       " 'imagination']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lemmatized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter empty list of strings in X_lemmatized, as they are not useful for topic modelling\n",
    "\n",
    "X_lemmatized = list(filter(lambda x: len(x) > 0, X_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a custom stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a sklearn CountVectorizer and select top 2000 words for tm\n",
    "# ref: https://stackoverflow.com/questions/21552518/using-scikit-learn-vectorizers-and-vocabularies-with-gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 1), stop_words='english', max_features=2000000)      # default value of max_features in gensim.corpora.Dictionary is 2M\n",
    "corpus_vect = vect.fit_transform(X_lemmatized)\n",
    "\n",
    "corpus = gensim.matutils.Sparse2Corpus(corpus_vect, documents_columns=False)\n",
    "\n",
    "# transform scikit vocabulary into gensim dictionary\n",
    "id2word = gensim.Dictionary.from_corpus(\n",
    "    corpus_vect,\n",
    "    id2word=dict((id, word) for word, id in vect.vocabulary_.items())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim to build a dictionary and train our LDAModel\n",
    "\n",
    "# it will create a dictionary object, to extract the words from the corpus (i.e. creating tokens)\n",
    "# id2word = gensim.corpora.Dictionary(X_lemmatized, prune_at=2000000)     # 2M for the default value of the func\n",
    "\n",
    "id2word.filter_extremes(no_below=50)\n",
    "# then calling doc2bow on the dictionary object to create a (word (i.e. token), frequency) mapping.\n",
    "# corpus = [id2word.doc2bow(text) for text in X_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lematized data, as separate pickle file\n",
    "\n",
    "X_lemmatized_file = dataset_path.parent.joinpath('cleaned_lemmatized', dataset_path.stem + '_cleaned_lemmatized.pkl')\n",
    "\n",
    "if not X_lemmatized_file.parent.exists():\n",
    "    X_lemmatized_file.parent.mkdir()\n",
    "\n",
    "with open(X_lemmatized_file, \"wb\") as f:\n",
    "    pickle.dump(X_lemmatized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the lematized data, as separate pickle file\n",
    "# for convenient hyperparameter selection\n",
    "\n",
    "# X_lemmatized_file = dataset_path.parent.joinpath('cleaned_lemmatized', dataset_path.stem + '_cleaned_lemmatized.pkl')\n",
    "\n",
    "# with open(X_lemmatized_file, \"rb\") as f:\n",
    "#     X_lemmatized = pickle.load(f)\n",
    "\n",
    "# X_lemmatized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_message(message):\n",
    "    '''Print message with a timestamp in front of it\n",
    "\n",
    "    Timestamp format: YYYY-MM-DD HH:MM:SS,mmm\n",
    "    '''\n",
    "    print(f'{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S,%f\")[:-3]} - {message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_LdaMulticore_params(corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, \n",
    "        passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, \n",
    "        eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, \n",
    "        minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=np.float32):\n",
    "    \n",
    "    hyperparameters = dict()\n",
    "    hyperparameters['corpus'] = corpus\n",
    "    hyperparameters[\"num_topics\"] = num_topics\n",
    "    hyperparameters['id2word'] = id2word\n",
    "    hyperparameters[\"workers\"] = workers\n",
    "    hyperparameters[\"chunksize\"] = chunksize\n",
    "    hyperparameters[\"passes\"] = passes\n",
    "    hyperparameters[\"alpha\"] = alpha\n",
    "    hyperparameters[\"eta\"] = eta\n",
    "    hyperparameters[\"decay\"] = decay\n",
    "    hyperparameters[\"offset\"] = offset\n",
    "    hyperparameters[\"eval_every\"] = eval_every\n",
    "    hyperparameters[\"iterations\"] = iterations\n",
    "    hyperparameters[\"gamma_threshold\"] = gamma_threshold\n",
    "    hyperparameters['minimum_probability'] = minimum_probability\n",
    "    hyperparameters[\"random_state\"] = random_state\n",
    "    hyperparameters['minimum_phi_value'] = minimum_phi_value\n",
    "    hyperparameters['per_word_topics'] = per_word_topics\n",
    "    hyperparameters['dtype'] = dtype\n",
    "\n",
    "    if \"alpha\" in hyperparameters:\n",
    "        if isinstance(hyperparameters[\"alpha\"], float):\n",
    "            hyperparameters[\"alpha\"] = [\n",
    "                hyperparameters[\"alpha\"]\n",
    "            ] * hyperparameters[\"num_topics\"]\n",
    "\n",
    "    return hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "\n",
    "from eval_metrics import compute_inverted_rbo, compute_topic_diversity, compute_pairwise_jaccard_similarity, \\\n",
    "                        METRICS, SEARCH_BEHAVIOUR, COHERENCE_MODEL_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_config_dict(config_path:Path, model_name:str, hyperparameters:dict, search_space_dict:dict, \n",
    "                      metrics:list[METRICS], monitor:METRICS,\n",
    "                      search_behaviour:SEARCH_BEHAVIOUR, search_rs:int, search_n_iter:int):\n",
    "    # init dict for config.json\n",
    "\n",
    "    _hyperparameters = hyperparameters\n",
    "    _search_space_dict = search_space_dict\n",
    "\n",
    "    if not config_path.exists():\n",
    "        config = {}\n",
    "        config['model'] = model_name\n",
    "        config.update(hyperparameters)\n",
    "\n",
    "        config.pop('corpus', '')\n",
    "        config.pop('id2word', '')\n",
    "        \n",
    "        # remove hyperparameters that are in the search space\n",
    "        for key in search_space_dict.keys():\n",
    "            config.pop(key, '')\n",
    "\n",
    "        config['dtype'] = str(config['dtype'])      # datatype is not json serializable, so convert to str\n",
    "\n",
    "        # store the search behaviour\n",
    "        config['search_behaviour'] = search_behaviour.value\n",
    "\n",
    "        if search_behaviour == SEARCH_BEHAVIOUR.RANDOM_SEARCH:\n",
    "            config['search_rs'] = search_rs\n",
    "            config['search_n_iter'] = search_n_iter\n",
    "\n",
    "        # store the search space\n",
    "        config['search_space'] = search_space_dict\n",
    "\n",
    "        # store the metrics types\n",
    "        config['metrics'] = list(map(lambda x: x.value, metrics))\n",
    "\n",
    "        # store the monitor metric\n",
    "        config['monitor'] = monitor.value\n",
    "\n",
    "        config['gensim_version'] = str(gensim.__version__)\n",
    "\n",
    "        # save the file\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "\n",
    "        _print_message('Created config.json at: ' + str(config_path))\n",
    "        # print('Created config.json at:', config_path)\n",
    "    else:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # check whether search behaviour in _search_behaviour is same in _config\n",
    "        if 'search_behaviour' not in config.keys():\n",
    "            raise Exception('search_behaviour is not found in config.json. Please modify the hyperparameters passed in.')\n",
    "        \n",
    "        if config['search_behaviour'] != search_behaviour.value:\n",
    "            raise Exception(f'search_behaviour is different in config.json. Please modify the hyperparameters passed in.')\n",
    "        \n",
    "        if config['search_behaviour'] == SEARCH_BEHAVIOUR.RANDOM_SEARCH.value:\n",
    "            if 'search_rs' not in config.keys():\n",
    "                raise Exception('search_rs is not found in config.json. Please modify the hyperparameters passed in.')\n",
    "            \n",
    "            if config['search_rs'] != search_rs:\n",
    "                raise Exception(f'search_rs is different in config.json. Please modify the hyperparameters passed in.')\n",
    "            \n",
    "            if 'search_n_iter' not in config.keys():\n",
    "                raise Exception('search_n_iter is not found in config.json. Please modify the hyperparameters passed in.')\n",
    "            \n",
    "            if config['search_n_iter'] != search_n_iter:\n",
    "                raise Exception(f'search_n_iter is different in config.json. Please modify the hyperparameters passed in.')\n",
    "\n",
    "        # check whether hyperparameters in _hyperparameters are same in _config\n",
    "        # for every key in _hyperparameters, check whether the value is the same in _config\n",
    "        for key in config.keys() & _hyperparameters.keys():\n",
    "            if key == 'dtype':      # edge case for dtype\n",
    "                # convert the _hyperparameters[key] to str for comparison only\n",
    "                if str(_hyperparameters[key]) != config[key]:\n",
    "                    raise Exception(f'hyperparameters {key} is different in config.json. Please modify the hyperparameters passed in.')\n",
    "\n",
    "            else:\n",
    "                if _hyperparameters[key] != config[key]:\n",
    "                    raise Exception(f'hyperparameters {key} is different in config.json. Please modify the hyperparameters passed in.')\n",
    "            \n",
    "        # check search space\n",
    "        if 'search_space' not in config.keys():\n",
    "            raise Exception('search_space is not found in config.json. Please modify the hyperparameters passed in.')\n",
    "        \n",
    "        # check whether search space in _search_space_dict is same in _config\n",
    "        # for every key in _search_space_dict, check whether the value is the same in _config\n",
    "        for key in config['search_space'].keys() & _search_space_dict.keys():\n",
    "            if _search_space_dict[key] != config['search_space'][key]:\n",
    "                raise Exception(f'search_space {key} is different in config.json. Please modify the hyperparameters passed in.')\n",
    "            \n",
    "        # check the metrics and monitor\n",
    "        if 'metrics' not in config.keys():\n",
    "            raise Exception('metrics is not found in config.json. Please modify the hyperparameters passed in.')\n",
    "        \n",
    "        # check whether metrics in _metrics is same in _config\n",
    "        # for every key in _metrics, check whether the value is the same in _config\n",
    "        if config['metrics'] != list(map(lambda x: x.value, metrics)):\n",
    "            raise Exception(f'metrics is different in config.json. Please modify the hyperparameters passed in.')\n",
    "        \n",
    "        # check whether monitor in _monitor is same in _config\n",
    "        # for every key in _monitor, check whether the value is the same in _config\n",
    "        if config['monitor'] != monitor.value:\n",
    "            raise Exception(f'monitor is different in config.json. Please modify the hyperparameters passed in.')\n",
    "\n",
    "\n",
    "        _print_message('Loaded existing config.json from: ' + str(config_path))\n",
    "        _print_message('Hyperparameters and search space are checked to be consistent with config.json')\n",
    "        # print('Loaded existing config.json from:', config_path)\n",
    "        # print('Hyperparameters and search space are checked to be consistent with config.json')\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_result_dict(result_path:Path, monitor_type:str):\n",
    "    # init dict for result.json\n",
    "\n",
    "    if not result_path.exists():\n",
    "        result = {}\n",
    "        result['best_metric'] = -float('inf')\n",
    "        result['best_model_checkpoint'] = \"\"\n",
    "        result['best_hyperparameters'] = dict()\n",
    "        result[\"monitor_type\"] = monitor_type\n",
    "        result[\"log_history\"] = list()\n",
    "    else:\n",
    "        with open(result_path, 'r') as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        # check whether metric_type in result.json is same as metric_type passed in\n",
    "        if 'monitor_type' not in result.keys():\n",
    "            raise Exception('metric_type is not found in result.json. Please modify the metric_type passed in.')\n",
    "        elif result['monitor_type'] != monitor_type:\n",
    "            raise Exception(f'metric_type is different in result.json. Please modify the metric_type passed in.')\n",
    "\n",
    "        _print_message('Loaded existing result.json from: ' + str(result_path))\n",
    "        _print_message('metric_type is checked to be consistent with result.json')\n",
    "        # print('Loaded existing result.json from:', result_path)\n",
    "        # print('metric_type is checked to be consistent with result.json')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referencing octis to calculate topics, topic-document-matrix and topic-word-matrix\n",
    "\n",
    "def _get_topic_word_matrix(model):\n",
    "    return model.get_topics()\n",
    "\n",
    "def _get_topics(model, id2word, result, top_words=10):\n",
    "    if top_words > 0:\n",
    "        topics_output = []\n",
    "        for topic in result[\"topic-word-matrix\"]:\n",
    "            top_k = np.argsort(topic)[-top_words:]\n",
    "            top_k_words = list(reversed([id2word[i] for i in top_k]))\n",
    "            topics_output.append(top_k_words)\n",
    "\n",
    "        return topics_output\n",
    "    \n",
    "def _get_topic_document_matrix(model, corpus, num_topics):\n",
    "    \"\"\"\n",
    "    Return the topic representation of the\n",
    "    corpus\n",
    "    \"\"\"\n",
    "    doc_topic_tuples = []\n",
    "    for document in corpus:\n",
    "        doc_topic_tuples.append(\n",
    "            model.get_document_topics(document, minimum_probability=0))\n",
    "\n",
    "    topic_document = np.zeros((\n",
    "        num_topics,\n",
    "        len(doc_topic_tuples)))\n",
    "\n",
    "    for ndoc in range(len(doc_topic_tuples)):\n",
    "        document = doc_topic_tuples[ndoc]\n",
    "        for topic_tuple in document:\n",
    "            topic_document[topic_tuple[0]][ndoc] = topic_tuple[1]\n",
    "    return topic_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_lda_model(model_path:Path):\n",
    "    if not model_path.exists():\n",
    "        raise Exception(f'Cannot find model checkpoint at {model_path}')\n",
    "    else:\n",
    "        lda_model = gensim.models.ldamodel.LdaModel.load(str(model_path.joinpath('lda_multicore')))\n",
    "        return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "from gensim.models import CoherenceModel\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "\n",
    "def model_search(X, hyperparameters:dict, search_space:dict, save_folder:Path, \n",
    "                metrics:list[METRICS]=[METRICS.C_NPMI], monitor:METRICS=METRICS.C_NPMI, \n",
    "                save_each_models=True, run_from_checkpoints=False,\n",
    "                search_behaviour=SEARCH_BEHAVIOUR.GRID_SEARCH, search_rs=42, search_n_iter=10):\n",
    "    \"\"\"\n",
    "    Perform grid search for LDA model hyperparameter selection\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : List of input texts\n",
    "    hyperparameters : dict of hyperparameters\n",
    "    search_space : dict of search space for hyperparameters\n",
    "    save_each_models : save each model or not\n",
    "    save_path : folder to save the model\n",
    "    run_from_checkpoints : whether to run from checkpoints or not\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    best_model : best model\n",
    "    best_model_path : path to the best model\n",
    "    best_hyperparameters : best hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    config_json_path = save_folder.joinpath('config.json')\n",
    "    result_json_path = save_folder.joinpath('result.json')\n",
    "\n",
    "    if monitor not in metrics:\n",
    "        raise Exception('monitor is not in metrics. Please modify the metrics passed in.')\n",
    "\n",
    "    if run_from_checkpoints:\n",
    "        if not save_folder.exists():\n",
    "            _print_message('Save folder:' + str(save_folder.resolve()) + ' does not exist. Function terminates.')\n",
    "            # print('Save folder:' + str(save_folder.resolve()) + ' does not exist. Function terminates.')\n",
    "            raise Exception('No checkpoints found. Function terminates.')\n",
    "        \n",
    "        # check for existing configs\n",
    "        if not config_json_path.exists():\n",
    "            raise Exception('No config.json found. Function terminates.')\n",
    "        \n",
    "        # check for existing results\n",
    "        if not result_json_path.exists():\n",
    "            _print_message('No result.json is found. Assuming no existing checkpoints.')\n",
    "            # print('No result.json is found. Assuming no existing checkpoints.')\n",
    "    else:\n",
    "        if save_folder.exists():\n",
    "            raise Exception('Checkpoints found. Please delete the checkpoints or set run_from_checkpoints=True. Function terminates.')\n",
    "        \n",
    "    if not save_folder.exists():\n",
    "        save_folder.mkdir()\n",
    "\n",
    "    # init / load existing json files\n",
    "    # also doing consistency checks for hyperparameters and search space\n",
    "    config = _init_config_dict(config_json_path, 'lda_multicore', hyperparameters, search_space, metrics, monitor,\n",
    "                               search_behaviour, search_rs, search_n_iter)\n",
    "\n",
    "    result = _init_result_dict(result_json_path, monitor.value)\n",
    "    \n",
    "    \n",
    "    _print_message(f'Search folder: {save_folder}')\n",
    "    # print(f'Search folder: {save_folder}')\n",
    "\n",
    "\n",
    "    # init\n",
    "    best_model_path = result['best_model_checkpoint']\n",
    "    best_metric_score = result['best_metric']\n",
    "    best_model = _load_lda_model(Path(best_model_path)) if best_model_path != \"\" else None\n",
    "    best_hyperparameters = result['best_hyperparameters']\n",
    "\n",
    "\n",
    "    _print_message(f'Best model checkpoint: {best_model_path}')\n",
    "    _print_message(f'Best metric score: {best_metric_score}')\n",
    "    _print_message(f'Best model: {best_model}')\n",
    "    # print(f'Best model checkpoint: {best_model_path}')\n",
    "    # print(f'Best metric score: {best_metric_score}')\n",
    "    # print(f'Best model: {best_model}')\n",
    "\n",
    "    # use sklearn to generate the search space instead of generating my myself\n",
    "    if search_behaviour == SEARCH_BEHAVIOUR.GRID_SEARCH:\n",
    "        search_iterator = ParameterGrid(search_space)\n",
    "    elif search_behaviour == SEARCH_BEHAVIOUR.RANDOM_SEARCH:\n",
    "        search_iterator = ParameterSampler(search_space, n_iter=search_n_iter, random_state=search_rs)\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    for search_space_dict in search_iterator:\n",
    "\n",
    "        # create the model folder by using search_space keys and current hyperparams values\n",
    "        model_name = '_'.join([f'{key}_{value}' for key, value in search_space_dict.items()])\n",
    "\n",
    "        model_path = save_folder.joinpath(\n",
    "            'lda_multicore_' + model_name\n",
    "        )\n",
    "\n",
    "        # check whether the current search space is already trained\n",
    "        # by comparing the folder name\n",
    "\n",
    "        if model_path.exists():\n",
    "            print(f'Skipping current search space: {search_space_dict}')\n",
    "            continue\n",
    "\n",
    "\n",
    "        ##########\n",
    "        # Training starts\n",
    "        ##########\n",
    "\n",
    "        _print_message(f'Training with current search space: {search_space_dict}')\n",
    "        # print(f'Training with current search space: {search_space_dict}')\n",
    "\n",
    "        # update existing hyperparams dict\n",
    "        hyperparameters.update(search_space_dict)\n",
    "\n",
    "        # train the model\n",
    "        model = gensim.models.ldamulticore.LdaMulticore(**hyperparameters)\n",
    "\n",
    "        ##########\n",
    "        # Training ends\n",
    "        ##########\n",
    "\n",
    "        ##########\n",
    "        # Evaluation starts\n",
    "        ##########\n",
    "\n",
    "        result_octis = {}\n",
    "        result_octis['topic-word-matrix'] = _get_topic_word_matrix(model)\n",
    "        result_octis['topics'] = _get_topics(model, hyperparameters['id2word'], result_octis, top_words=10)\n",
    "        result_octis['topic-document-matrix'] = _get_topic_document_matrix(model, corpus, hyperparameters['num_topics'])\n",
    "\n",
    "        _print_message('Computing evaluation metrics')\n",
    "        # print('Computing evaluation metrics')\n",
    "\n",
    "        metrics_score = dict()\n",
    "\n",
    "        # compute various metrics\n",
    "        for metric in metrics:\n",
    "            if metric in COHERENCE_MODEL_METRICS:\n",
    "                # compute the coherence\n",
    "                coherencemodel = CoherenceModel(model=model, texts=X, dictionary=id2word, coherence=metric.value)\n",
    "                score = coherencemodel.get_coherence()                \n",
    "\n",
    "            elif metric == METRICS.TOPIC_DIVERSITY:\n",
    "                # compute the coherence\n",
    "                score = compute_topic_diversity(result_octis, topk=10)\n",
    "\n",
    "            elif metric == METRICS.INVERTED_RBO:\n",
    "                # compute the coherence\n",
    "                score = compute_inverted_rbo(result_octis, topk=10)\n",
    "\n",
    "            elif metric == METRICS.PAIRWISE_JACCARD_SIMILARITY:\n",
    "                # compute the coherence\n",
    "                score = compute_pairwise_jaccard_similarity(result_octis, topk=10)\n",
    "\n",
    "            else:\n",
    "                raise Exception(f'Unknown metric: {metric.value}')\n",
    "            \n",
    "            metrics_score[metric.value] = score\n",
    "\n",
    "            _print_message(f'Evaluation metric ({metric.value}): {score}')\n",
    "            # print(f'Evaluation metric ({metric.value}): {score}')\n",
    "            \n",
    "        # get the monitor score\n",
    "        monitor_score = metrics_score[monitor.value]\n",
    "\n",
    "        ##########\n",
    "        # Evaluation ends\n",
    "        ##########\n",
    "\n",
    "        ##########\n",
    "        # Save models\n",
    "        ##########\n",
    "\n",
    "        if not model_path.exists():\n",
    "            model_path.mkdir(parents=True)\n",
    "\n",
    "        # save the model\n",
    "        if save_each_models:\n",
    "            model.save(str(model_path.joinpath('lda_multicore')))\n",
    "\n",
    "            _print_message('Model saved at: ' + str(model_path))\n",
    "            print('Model saved at:', model_path)\n",
    "\n",
    "        ##########\n",
    "        # Save models ends\n",
    "        ##########\n",
    "            \n",
    "        ###########\n",
    "        # Update result dict and json file\n",
    "        ###########\n",
    "            \n",
    "        # init\n",
    "\n",
    "        model_hyperparameters = deepcopy(hyperparameters)\n",
    "        model_hyperparameters.pop('corpus', '')     # pop as it is not json serializable\n",
    "        model_hyperparameters.pop('id2word', '')    # pop as it is not json serializable\n",
    "        model_hyperparameters['dtype'] = str(model_hyperparameters['dtype'])\n",
    "            \n",
    "        if monitor_score > best_metric_score:\n",
    "            best_metric_score = monitor_score\n",
    "            best_model = model\n",
    "            best_model_path = model_path\n",
    "            best_hyperparameters = model_hyperparameters\n",
    "            \n",
    "        # update\n",
    "            \n",
    "        model_log_history = dict()\n",
    "        model_log_history.update(metrics_score)         # add the metrics score values to the log history\n",
    "        model_log_history['model_name'] = model_name\n",
    "        model_log_history['hyperparameters'] = model_hyperparameters\n",
    "\n",
    "        result['best_metric'] = best_metric_score\n",
    "        result['best_model_checkpoint'] = str(best_model_path)      # relative path\n",
    "        result['best_hyperparameters'] = best_hyperparameters\n",
    "        result[\"log_history\"].append(model_log_history)\n",
    "\n",
    "        # print(result)\n",
    "\n",
    "        # save result\n",
    "        with open(result_json_path, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        _print_message('Saved result.json at: ' + str(result_json_path))\n",
    "        # print(\"Saved result.json at:\", result_json_path)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    _print_message('Search ends')\n",
    "    # print('Search ends')\n",
    "    return best_model, best_model_path, best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-30 16:43:26,475 - Created config.json at: lda_multicore_grid_search_20240130_164326/config.json\n",
      "2024-01-30 16:43:26,476 - Search folder: lda_multicore_grid_search_20240130_164326\n",
      "2024-01-30 16:43:26,476 - Best model checkpoint: \n",
      "2024-01-30 16:43:26,476 - Best metric score: -inf\n",
      "2024-01-30 16:43:26,476 - Best model: None\n",
      "\n",
      "\n",
      "2024-01-30 16:43:26,476 - Training with current search space: {'num_topics': 10}\n",
      "2024-01-30 16:43:53,037 - Computing evaluation metrics\n",
      "2024-01-30 16:44:02,009 - Evaluation metric (c_npmi): 0.015634950114236927\n",
      "2024-01-30 16:44:10,818 - Evaluation metric (c_v): 0.5005047221786995\n",
      "2024-01-30 16:44:11,816 - Evaluation metric (u_mass): -2.83701467999183\n",
      "2024-01-30 16:44:20,416 - Evaluation metric (c_uci): -0.31160543475794117\n",
      "2024-01-30 16:44:20,416 - Evaluation metric (topic_diversity): 0.75\n",
      "2024-01-30 16:44:20,418 - Evaluation metric (inverted_rbo): 0.8694496359077778\n",
      "2024-01-30 16:44:20,419 - Evaluation metric (pairwise_jaccard_similarity): 0.06392806635324698\n",
      "2024-01-30 16:44:20,435 - Model saved at: lda_multicore_grid_search_20240130_164326/lda_multicore_num_topics_10\n",
      "Model saved at: lda_multicore_grid_search_20240130_164326/lda_multicore_num_topics_10\n",
      "2024-01-30 16:44:22,202 - Saved result.json at: lda_multicore_grid_search_20240130_164326/result.json\n",
      "\n",
      "\n",
      "\n",
      "2024-01-30 16:44:22,202 - Training with current search space: {'num_topics': 20}\n",
      "2024-01-30 16:44:54,066 - Computing evaluation metrics\n",
      "2024-01-30 16:45:05,098 - Evaluation metric (c_npmi): -0.002573204311625498\n",
      "2024-01-30 16:45:15,181 - Evaluation metric (c_v): 0.4861298516565874\n",
      "2024-01-30 16:45:16,299 - Evaluation metric (u_mass): -3.084086328399321\n",
      "2024-01-30 16:45:25,139 - Evaluation metric (c_uci): -0.7219900221727618\n",
      "2024-01-30 16:45:25,139 - Evaluation metric (topic_diversity): 0.665\n",
      "2024-01-30 16:45:25,149 - Evaluation metric (inverted_rbo): 0.8871314557932707\n",
      "2024-01-30 16:45:25,149 - Evaluation metric (pairwise_jaccard_similarity): 0.06730985099487609\n",
      "2024-01-30 16:45:25,166 - Model saved at: lda_multicore_grid_search_20240130_164326/lda_multicore_num_topics_20\n",
      "Model saved at: lda_multicore_grid_search_20240130_164326/lda_multicore_num_topics_20\n",
      "2024-01-30 16:45:26,869 - Saved result.json at: lda_multicore_grid_search_20240130_164326/result.json\n",
      "\n",
      "\n",
      "\n",
      "2024-01-30 16:45:26,869 - Search ends\n"
     ]
    }
   ],
   "source": [
    "# grid search / random search\n",
    "\n",
    "hyperparameters = _init_LdaMulticore_params(\n",
    "    corpus=corpus, num_topics=20, id2word=id2word, \n",
    "    workers=3, chunksize=2024, random_state=42, passes=5)\n",
    "\n",
    "# create search_space dict\n",
    "search_space = dict()\n",
    "\n",
    "search_space['num_topics'] = [10, 20]\n",
    "# search_space['decay'] = [0.7, 0.8, 0.9]\n",
    "# search_space['offset'] = [16, 64, 128]\n",
    "\n",
    "search_behaviour = SEARCH_BEHAVIOUR.GRID_SEARCH\n",
    "# search_behaviour = SEARCH_BEHAVIOUR.RANDOM_SEARCH\n",
    "\n",
    "\n",
    "training_datetime = datetime.now()\n",
    "# training_datetime = datetime(2024, 1, 19, 11, 44, 53)\n",
    "\n",
    "training_folder = Path(f'lda_multicore_{search_behaviour.value}_{training_datetime.strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "\n",
    "best_model, best_model_path, best_hyperparameters = model_search(\n",
    "    X_lemmatized, hyperparameters, search_space, \n",
    "    training_folder,\n",
    "    metrics=[METRICS.C_NPMI, METRICS.C_V, METRICS.UMASS, METRICS.C_UCI, METRICS.TOPIC_DIVERSITY, METRICS.INVERTED_RBO, METRICS.PAIRWISE_JACCARD_SIMILARITY],\n",
    "    monitor=METRICS.C_NPMI,\n",
    "    search_behaviour=search_behaviour, \n",
    "    # search_rs=42, \n",
    "    # search_n_iter=10,\n",
    "    run_from_checkpoints=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_TOPICS = 20\n",
    "\n",
    "# # Online LDA, how to effective train LDA models\n",
    "# # https://papers.nips.cc/paper_files/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html\n",
    "\n",
    "# lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "#                                              id2word=id2word,\n",
    "#                                              num_topics=N_TOPICS,         # later can use grid search to find the best number of topics\n",
    "#                                              random_state=42,\n",
    "#                                              chunksize=2048,                # chunk size affects memory consumption, and updating speed (like DL batch_size). https://groups.google.com/g/gensim/c/FE7_FYSconA\n",
    "#                                              passes=10,                     # no. of passes over the whole corpus. If larger chunksize, then the passes should be larger too.\n",
    "#                                             #  alpha='auto',\n",
    "#                                              workers=3)     # workers = no. of cores (physical cores, but not logical threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the best model from grid search\n",
    "\n",
    "but also load the id2word object and the corpus corresponding to the model (for separate evaluation and inference)\n",
    "\n",
    "https://stackoverflow.com/questions/60840809/gensim-how-to-load-corpus-from-saved-lda-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model checkpoint path: lda_multicore_grid_search_20240130_164326/lda_multicore_num_topics_10\n"
     ]
    }
   ],
   "source": [
    "# load the best model from training folder\n",
    "\n",
    "training_folder = Path(f'lda_multicore_grid_search_{training_datetime.strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "training_result_json_path = training_folder.joinpath('result.json')\n",
    "with open(training_result_json_path, 'r') as f:\n",
    "    training_result = json.load(f)\n",
    "\n",
    "best_model_checkpoint_path = Path(training_result['best_model_checkpoint'])\n",
    "\n",
    "best_id2word = gensim.corpora.Dictionary.load(str(best_model_checkpoint_path.joinpath('lda_multicore.id2word')))\n",
    "best_corpus = [id2word.doc2bow(text) for text in X_lemmatized]      # recreate the corpus given the id2word (gensim Dictionary)\n",
    "best_model = gensim.models.ldamulticore.LdaMulticore.load(str(best_model_checkpoint_path.joinpath('lda_multicore')))\n",
    "\n",
    "print('Best model checkpoint path:', best_model_checkpoint_path)\n",
    "\n",
    "lda_model = best_model\n",
    "id2word = best_id2word\n",
    "corpus = best_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelcheng/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/sklearn/manifold/_mds.py:298: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el907601203339360039007792\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el907601203339360039007792_data = {\"mdsDat\": {\"x\": [0.06753355212351107, -0.14586489710563066, -0.1300331663565952, -0.0669458998928459, -0.26422058149538324, 0.252269747962756, 0.16120864116023806, -0.23072062071112948, 0.33554942790650305, 0.021223796408576343], \"y\": [0.20131324795863412, 0.03139011482510751, 0.24526603334408087, 0.11366399820634523, 0.1538663703312527, 0.13587965765789026, -0.07448466284591389, -0.26287725433226494, -0.15177055517310864, -0.39224694997202303], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [28.02634740328375, 13.795711908387498, 13.51428625673748, 13.34845081424169, 13.046182534409482, 8.644869689429633, 3.9023028134850777, 2.143332394454524, 1.8222006577297907, 1.756315527841081]}, \"tinfo\": {\"Term\": [\"game\", \"minecraft\", \"world\", \"play\", \"get\", \"cool\", \"fun\", \"best\", \"like\", \"good\", \"environment\", \"generate\", \"emphasis\", \"array\", \"aesthetic\", \"selection\", \"procedurally\", \"customizable\", \"finite\", \"mineral\", \"variety\", \"equipment\", \"progression\", \"structure\", \"resource\", \"material\", \"biomes\", \"system\", \"combat\", \"unique\", \"enemy\", \"craft\", \"explore\", \"weapon\", \"different\", \"build\", \"item\", \"boss\", \"building\", \"world\", \"fight\", \"many\", \"game\", \"terrarium\", \"minecraft\", \"otherworld\", \"penny\", \"dlc\", \"cent\", \"devs\", \"hype\", \"reinstall\", \"meme\", \"alpha\", \"unplayable\", \"dev\", \"release\", \"dollar\", \"developer\", \"update\", \"year\", \"patch\", \"sale\", \"content\", \"come\", \"worth\", \"buy\", \"back\", \"game\", \"new\", \"terrarium\", \"still\", \"time\", \"adictive\", \"itch\", \"grate\", \"gb\", \"definently\", \"sibling\", \"ad\", \"bla\", \"extreamly\", \"exceptional\", \"best\", \"funnest\", \"ever\", \"co\", \"offline\", \"friends\", \"friend\", \"play\", \"alone\", \"solo\", \"one\", \"hour\", \"great\", \"game\", \"fun\", \"time\", \"tickle\", \"pickle\", \"liked\", \"dank\", \"finaly\", \"dosent\", \"teir\", \"tmodloader\", \"boring\", \"reson\", \"bored\", \"bore\", \"gon\", \"tired\", \"na\", \"im\", \"stop\", \"get\", \"start\", \"playing\", \"never\", \"hard\", \"time\", \"play\", \"game\", \"really\", \"first\", \"good\", \"fun\", \"hour\", \"chinese\", \"beter\", \"verry\", \"timesink\", \"mutch\", \"steroid\", \"swag\", \"liek\", \"kindle\", \"pony\", \"gamplay\", \"soooo\", \"mincraft\", \"minecraft\", \"realy\", \"awesome\", \"mac\", \"like\", \"love\", \"recomend\", \"good\", \"recommend\", \"people\", \"game\", \"really\", \"fun\", \"well\", \"much\", \"terrarium\", \"unicorn\", \"worm\", \"eyeball\", \"rainbow\", \"cat\", \"bee\", \"slit\", \"shark\", \"rid\", \"dink\", \"giant\", \"demon\", \"slime\", \"shoot\", \"zombie\", \"eye\", \"moon\", \"flesh\", \"fly\", \"kill\", \"die\", \"wall\", \"sword\", \"get\", \"go\", \"fight\", \"would\", \"make\", \"game\", \"ranger\", \"shield\", \"summoner\", \"language\", \"relly\", \"mage\", \"mouse\", \"de\", \"ankh\", \"agian\", \"v\", \"warrior\", \"archer\", \"net\", \"beast\", \"melee\", \"mana\", \"class\", \"expert\", \"mode\", \"magic\", \"use\", \"range\", \"armor\", \"make\", \"weapon\", \"awsome\", \"zone\", \"nuff\", \"crust\", \"apple\", \"pie\", \"heat\", \"pastry\", \"uh\", \"salt\", \"sugar\", \"u\", \"butter\", \"dont\", \"please\", \"let\", \"simulator\", \"wont\", \"launch\", \"help\", \"need\", \"say\", \"get\", \"know\", \"game\", \"delete\", \"pls\", \"cancer\", \"data\", \"click\", \"pun\", \"trash\", \"horribly\", \"glitch\", \"freeze\", \"crash\", \"file\", \"sand\", \"idk\", \"fix\", \"box\", \"plz\", \"save\", \"open\", \"bug\", \"world\", \"character\", \"lose\", \"ign\", \"gud\", \"r\", \"gr\", \"le\", \"gg\", \"crack\", \"k\", \"alright\", \"badgei\", \"ok\", \"cool\", \"n\", \"guess\", \"nice\", \"step\", \"pretty\", \"game\", \"good\", \"yeah\"], \"Freq\": [106126.0, 20814.0, 9568.0, 30148.0, 23164.0, 3510.0, 20802.0, 9114.0, 19244.0, 13622.0, 342.6435215236418, 685.8068974053097, 142.09393527374243, 101.70305349165328, 87.97317229632083, 100.69926834131532, 73.7389717676175, 72.94441592546337, 57.88597891867035, 104.4238077914598, 1161.56658109561, 652.0192812730132, 1123.5071663908893, 461.14939222274904, 593.7362947716148, 954.7124279237161, 269.1280629881322, 1338.421576887525, 1725.9468471323771, 1395.2843691641187, 2502.744049981362, 4103.021739157715, 3811.7210277466083, 4182.77874434523, 3473.3871773119927, 5732.141889649237, 6047.711188989397, 7083.404311780463, 2614.3823710033894, 6373.7335040936105, 4442.592859058995, 4256.431082102428, 15664.44465078513, 5791.380339892839, 4567.47740710933, 262.32295351819647, 275.85862199877613, 225.02887306270085, 145.54674839891194, 726.4795022402118, 113.9475987448814, 71.33648598203582, 67.79979032368439, 50.53356566844982, 47.010533055921215, 164.2730497658703, 1118.7400702167934, 567.1070017223851, 967.9293572137635, 5771.200046203874, 2444.3017283787153, 510.6983232554276, 1492.2745101289229, 3921.4376208721856, 2778.618108165985, 2900.1449558652143, 3759.800056594755, 1849.1016971470087, 16838.564302228246, 3242.856703246065, 3866.667690906438, 2535.0252934273904, 3088.705213068252, 53.74185122265862, 44.780060895005974, 29.25558514281569, 28.041579740735745, 26.147001799487104, 26.545918391694226, 25.47428099432011, 25.042752421988144, 23.79869822167464, 23.0442208572702, 7886.737092516459, 59.83057081678596, 5025.625465406129, 423.19539061586977, 123.51398447665139, 1129.9038845884154, 5939.151227996848, 16219.3112359817, 742.6292008688247, 478.51394278345157, 5732.155121736887, 6142.295267492457, 5510.249379001991, 27939.34950921711, 7138.574692844579, 2980.8143980918985, 179.13839687834331, 177.2263802105094, 70.19464916303264, 45.797428470420776, 36.3022508118538, 60.480182769770664, 32.023407597931495, 32.231473482889534, 994.9195948626037, 28.419474467684058, 410.44790556498424, 1377.5300900185537, 352.3818392201429, 118.22566727401995, 610.897919177295, 986.8504497437199, 1250.5404191361724, 12473.56608055873, 2987.8482901935226, 1149.4888740006165, 2049.60962007418, 1724.0136928869683, 4170.200734445459, 7116.917004872169, 15520.3791514057, 3023.5153450775347, 2159.9336106231494, 3454.9788931610647, 3946.248133140659, 2797.180481316723, 110.2473222192186, 62.228180616283296, 55.346558967791644, 52.92936801456674, 46.5363809413242, 42.26107862803625, 28.289216116981617, 27.34926049639025, 22.69699495484813, 26.38393167546223, 36.12735474024055, 125.16546462836489, 166.95202929148934, 15085.394772845084, 274.1296449588831, 4204.5670751294, 520.3394279954891, 10935.057584185244, 5112.678305005289, 687.3097657923194, 6094.7540234657745, 2935.740080982013, 2218.9211719892132, 26780.666450615878, 3913.079936961208, 6971.343157874936, 3635.691565350922, 3974.9613739186348, 3680.6928413329165, 628.3331501929175, 383.05380694922655, 396.28031110045686, 353.51685144996947, 324.0624299273855, 368.0721526256381, 283.1716537964886, 250.4286876928288, 282.6047446657382, 225.48321211880136, 1189.0703529342734, 384.88302050591506, 1010.5831817039038, 840.7066623541676, 819.1414030281638, 1139.3635518431943, 1225.7431153843868, 893.9107941650572, 901.528121711399, 3715.469881826335, 1790.7070506213552, 1116.4640345980547, 1065.8600333304787, 2429.8387477442043, 1627.1923494825833, 1465.907430937662, 1502.5259567745331, 1421.7043429226321, 1484.89794046957, 208.64061448222122, 65.3149561896126, 283.63497369687497, 53.913331756494074, 43.60627153248594, 331.61827304063803, 147.38798913334583, 38.66464146572429, 38.37341152268436, 35.00504221286564, 241.01260508774766, 179.4775910737251, 51.36061847785658, 66.26694997814472, 156.92640091259273, 373.24602064655335, 166.4183408343375, 653.9430149399337, 812.5984406677111, 1199.575060422142, 546.7056322276136, 801.8268859644379, 347.6608120442856, 560.6888152650254, 673.3305472648011, 552.794257907512, 885.854787567835, 273.329411246437, 117.14182573725137, 81.27146056135498, 73.52841249784323, 63.98740091388855, 57.075878717067354, 54.94301471515087, 52.0154808620428, 71.4548442937698, 101.39369880247878, 1666.2836774594984, 77.00698647748159, 1657.0873207773166, 671.9797373436294, 976.3740232224752, 143.24176967229423, 276.5283687678503, 214.67338952215013, 496.83321283434555, 653.0439150887983, 685.0716463949522, 810.5450009875417, 425.27118530869365, 534.1456551985995, 314.1976336266232, 177.72396804337035, 88.0018342507321, 91.20293239465555, 889.9089988259519, 68.76853534244795, 77.47707597615019, 43.85071890485766, 233.54080661380783, 49.92181266450103, 420.4439915372207, 200.44206448854624, 190.29611926528332, 105.10248258600546, 713.9029366231224, 223.22976139801204, 227.19025148984304, 356.77606101478847, 738.3464208606478, 337.1693708541795, 1597.811514741218, 523.4652358082292, 295.54580794509053, 413.96827035277624, 230.1838021039558, 253.47098953937558, 149.24740330711597, 165.39787452791793, 117.62511671939836, 189.12395045647338, 210.91640562295822, 250.91963183640354, 75.74848894667063, 737.5379167922842, 3229.2734525811406, 203.65873409973582, 414.41096627797344, 841.5648277259999, 312.8146090273166, 774.2738872160571, 954.9094074317139, 497.6934996681892, 275.0615247326885], \"Total\": [106126.0, 20814.0, 9568.0, 30148.0, 23164.0, 3510.0, 20802.0, 9114.0, 19244.0, 13622.0, 343.80676434956143, 688.6568728372673, 143.17514113297278, 102.7193882473007, 88.97653333770963, 101.89729187069052, 74.748741634032, 74.12956110141606, 58.87451087921942, 106.38284993818486, 1187.4024011007884, 664.8187755888157, 1152.803529788643, 471.2412790645178, 615.4880087036729, 1005.1053037081213, 276.7128319865602, 1442.9725556284066, 1900.9178239312369, 1527.862113573057, 2861.883769620514, 4899.078105784315, 4713.52811035184, 5275.37232791932, 4284.151567344003, 7749.56894662677, 8428.155488819128, 10550.506745838708, 3191.177537706978, 9568.941035827327, 6463.946347271826, 7842.743369829986, 106126.72331258333, 16887.885729661037, 20814.21599183326, 263.51641513626345, 277.1616396373381, 226.35381914531467, 146.76488569003058, 736.2019399081593, 115.52729034610978, 72.33832108522789, 69.01146093627219, 51.539756929877505, 48.01724853636617, 168.83038890007407, 1172.2829610097738, 593.581946528423, 1052.692106231039, 6839.151920431167, 3072.6913793867816, 570.9829435147417, 1856.2193681396195, 6043.930467850184, 4606.375995128394, 4920.595660659028, 7899.087188862588, 3141.5407556804603, 106126.72331258333, 7971.195924829428, 16887.885729661037, 6555.543930529008, 12784.226361580973, 54.74659894055505, 45.784647556080216, 30.275330709777684, 29.046944602570044, 27.151421398291443, 27.570865386647156, 26.478746369732068, 26.047117651575846, 24.817251714175544, 24.050343117946316, 9114.426725138754, 63.29134259674331, 6021.504726119637, 532.2082988561144, 143.39684815472177, 1575.0617345168382, 9793.790443520658, 30148.804687669708, 1065.3206495491143, 655.5768203649233, 11492.798595602748, 12745.239400093895, 13164.200109597721, 106126.72331258333, 20802.63002350928, 12784.226361580973, 180.13417744268523, 178.22211792516805, 71.27180610280759, 46.880013803829634, 37.31820808290117, 62.26648459860304, 33.01976837036521, 33.25565005334934, 1029.297214754635, 29.41630701688987, 429.8638499524836, 1641.1153110531766, 402.1546186643578, 131.43838741391895, 780.8891470008103, 1318.3211420230223, 1743.9755442682317, 23164.249953070608, 5003.955926362497, 1650.2770895994684, 3556.722815682233, 3249.093936664353, 12784.226361580973, 30148.804687669708, 106126.72331258333, 9121.201565350138, 5070.104437204696, 13622.773103280277, 20802.63002350928, 12745.239400093895, 111.23986343501362, 63.224601149340174, 56.33977166486459, 53.9232743075396, 47.55511586236727, 43.30982101090971, 29.281562658339002, 28.357459962580332, 23.68899042502411, 27.564402421870746, 37.84238622990825, 136.5704982424032, 186.17357991836093, 20814.21599183326, 313.6106988617753, 5962.037143792004, 646.3937420222126, 19244.259330795685, 9202.882174064891, 993.0975433765552, 13622.773103280277, 6097.3053551907615, 4431.63767834239, 106126.72331258333, 9121.201565350138, 20802.63002350928, 9356.615241675529, 11398.835420199292, 16887.885729661037, 629.5568066009388, 384.1003828663803, 397.39164995400654, 354.53077595498075, 325.0616050910481, 369.34782414632065, 284.1546906347041, 251.46965521783284, 283.79215739840055, 226.4663365395419, 1206.4727252694124, 387.61103871617667, 1036.0091636044945, 863.468176056549, 861.9721632293156, 1224.397414161334, 1384.5434965134937, 980.0661799642726, 991.7084199549155, 4977.853581116884, 2293.319397050759, 1381.813436405276, 1317.8552520517007, 23164.249953070608, 7464.503906112806, 6463.946347271826, 8801.653661913228, 10414.850835880257, 106126.72331258333, 210.4264963332962, 66.30953856939352, 288.50579218799, 54.9162800164284, 44.63105566664035, 339.43821815404607, 150.89876654038747, 39.66194863369771, 39.36494764864057, 35.99522721343272, 249.85723294830387, 186.88689672488428, 52.954833486688855, 68.76310134712857, 169.79359580497896, 460.34217471221496, 192.90955782561775, 989.0935036334554, 1653.9469137134806, 2905.8724471206538, 993.4664367981779, 3251.2004745323507, 722.2541258452711, 2657.563335496251, 10414.850835880257, 5275.37232791932, 886.9327813179455, 275.0450687541264, 118.16224522797454, 82.28079481411436, 74.54899766141274, 65.01015882821666, 58.0641124741974, 55.93233275943602, 53.07815542787649, 73.0707103516346, 104.40978231782476, 2026.2643638212223, 80.03604891991486, 2324.0623718323504, 1198.838213742481, 2021.8797306137587, 188.480889131937, 535.2671942883294, 371.26378531630047, 1925.152003303857, 3635.3669673505, 6611.2733139948205, 23164.249953070608, 3579.195396539707, 106126.72331258333, 315.56580524654817, 178.78544554063419, 88.97785989835991, 92.36508391749406, 902.7017373279512, 69.76726016978154, 79.16805522526766, 44.82530374757115, 238.80396753788978, 51.07507553521717, 436.1870035491255, 206.44052772923274, 199.22696056088742, 108.24662679775085, 826.7159282423328, 262.765264673538, 270.02158278318495, 523.4589249884923, 1542.5077474665875, 552.9998262579563, 9568.941035827327, 2772.764947955807, 1411.5652061134256, 415.3970349521031, 231.17235051332457, 255.1158270267702, 150.23265319417527, 166.5131722804026, 118.63697682306716, 190.87735278300585, 212.94562315552886, 253.47913817295654, 76.72408922084371, 766.8312123831913, 3510.8774700641475, 217.74775897498895, 527.6976643061871, 2060.5238810718297, 431.7267889554356, 3033.9851842261805, 106126.72331258333, 13622.773103280277, 633.1780283596095], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.1396, -6.4457, -8.0198, -8.3542, -8.4992, -8.3641, -8.6757, -8.6866, -8.9178, -8.3278, -5.9187, -6.4962, -5.9521, -6.8425, -6.5898, -6.1149, -7.3811, -5.777, -5.5227, -5.7354, -5.1511, -4.6568, -4.7304, -4.6375, -4.8234, -4.3224, -4.2688, -4.1108, -5.1075, -4.2163, -4.5773, -4.6201, -3.3171, -4.3121, -4.5495, -6.6979, -6.6476, -6.8512, -7.287, -5.6793, -7.5317, -8.0001, -8.0509, -8.3448, -8.4171, -7.1659, -5.2475, -5.9269, -5.3923, -3.6068, -4.466, -6.0317, -4.9594, -3.9933, -4.3378, -4.295, -4.0354, -4.745, -2.536, -4.1833, -4.0073, -4.4295, -4.232, -8.2627, -8.4451, -8.8708, -8.9132, -8.9831, -8.968, -9.0092, -9.0263, -9.0772, -9.1094, -3.2739, -8.1553, -3.7246, -6.199, -7.4305, -5.217, -3.5575, -2.5529, -5.6367, -6.0762, -3.593, -3.5239, -3.6325, -2.0091, -3.3736, -4.2469, -7.0464, -7.0571, -7.9832, -8.4103, -8.6426, -8.1322, -8.7681, -8.7616, -5.3319, -8.8874, -6.2173, -5.0065, -6.3698, -7.4619, -5.8196, -5.34, -5.1032, -2.8032, -4.2322, -5.1874, -4.6091, -4.7821, -3.8988, -3.3643, -2.5846, -4.2203, -4.5567, -4.0869, -3.954, -4.2982, -7.5089, -8.0808, -8.198, -8.2427, -8.3714, -8.4677, -8.8691, -8.9029, -9.0894, -8.9389, -8.6246, -7.382, -7.0939, -2.5901, -6.598, -3.8677, -5.9571, -2.9119, -3.6721, -5.6788, -3.4964, -4.2269, -4.5068, -2.0162, -3.9395, -3.362, -4.0131, -3.9238, -4.0008, -5.357, -5.8519, -5.818, -5.9322, -6.0191, -5.8918, -6.154, -6.2769, -6.156, -6.3818, -4.7192, -5.8471, -4.8818, -5.0658, -5.0918, -4.7619, -4.6888, -5.0045, -4.996, -3.5798, -4.3097, -4.7822, -4.8285, -4.0045, -4.4055, -4.5099, -4.4852, -4.5405, -4.497, -5.6641, -6.8255, -5.357, -7.0173, -7.2295, -5.2007, -6.0116, -7.3498, -7.3573, -7.4492, -5.5198, -5.8146, -7.0658, -6.811, -5.9489, -5.0824, -5.8902, -4.5217, -4.3044, -3.915, -4.7008, -4.3178, -5.1535, -4.6755, -4.4924, -4.6897, -3.6189, -4.7948, -5.6421, -6.0077, -6.1078, -6.2468, -6.3611, -6.3992, -6.4539, -6.1364, -5.7865, -2.9871, -6.0616, -2.9927, -3.8953, -3.5216, -5.4409, -4.7832, -5.0364, -4.1972, -3.9238, -3.876, -3.7078, -4.3528, -4.1248, -4.4931, -5.0629, -5.7658, -5.7301, -3.452, -6.0124, -5.8932, -6.4624, -4.7898, -6.3327, -4.2019, -4.9426, -4.9946, -5.5882, -3.6724, -4.835, -4.8174, -4.3661, -3.6387, -4.4226, -2.8668, -3.9827, -4.5543, -4.1805, -4.7675, -4.6711, -5.2007, -5.098, -5.4388, -4.9639, -4.8549, -4.6812, -5.8789, -3.603, -2.1263, -4.8899, -4.1795, -3.4711, -4.4607, -3.5544, -3.3447, -3.9964, -4.5893], \"loglift\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2686, 1.2679, 1.2644, 1.2621, 1.2607, 1.2602, 1.2584, 1.2559, 1.2551, 1.2534, 1.25, 1.2526, 1.2463, 1.2504, 1.236, 1.2206, 1.2442, 1.1968, 1.1755, 1.1813, 1.1379, 1.0947, 1.0597, 1.04, 1.0622, 0.9705, 0.9401, 0.8736, 1.0727, 0.8657, 0.897, 0.6609, -0.6412, 0.2018, -0.2446, 1.9763, 1.9761, 1.9749, 1.9725, 1.9675, 1.967, 1.9669, 1.9631, 1.9611, 1.9596, 1.9534, 1.9341, 1.9352, 1.8969, 1.811, 1.752, 1.8692, 1.7626, 1.5482, 1.4753, 1.4521, 1.2384, 1.4508, 0.1399, 1.0814, 0.5066, 1.0307, 0.5604, 1.9829, 1.9792, 1.9672, 1.9662, 1.9637, 1.9635, 1.9627, 1.9621, 1.9595, 1.9587, 1.8567, 1.9452, 1.8206, 1.7722, 1.8522, 1.6693, 1.5012, 1.3815, 1.6406, 1.6866, 1.3058, 1.2715, 1.1305, 0.6668, 0.9319, 0.5454, 2.0082, 2.0082, 1.9985, 1.9904, 1.9862, 1.9847, 1.9831, 1.9825, 1.9798, 1.9793, 1.9676, 1.8387, 1.8816, 1.9078, 1.7683, 1.7242, 1.6812, 1.3948, 1.4981, 1.6521, 1.4626, 1.38, 0.8935, 0.5701, 0.0913, 0.9096, 1.1605, 0.6418, 0.3515, 0.4972, 2.0277, 2.0208, 2.0189, 2.0181, 2.015, 2.0122, 2.0022, 2.0005, 1.9939, 1.9929, 1.9903, 1.9495, 1.9277, 1.7148, 1.9021, 1.6874, 1.8197, 1.4714, 1.4489, 1.6686, 1.2324, 1.3058, 1.3449, 0.6597, 1.1904, 0.9434, 1.0914, 0.9832, 0.5132, 2.4463, 2.4455, 2.4454, 2.4453, 2.4451, 2.4447, 2.4447, 2.4441, 2.444, 2.4439, 2.4337, 2.4411, 2.4234, 2.4215, 2.3972, 2.3762, 2.3264, 2.3562, 2.3529, 2.1557, 2.2008, 2.235, 2.236, 0.1934, 0.9249, 0.9644, 0.6804, 0.4568, -1.8211, 3.2351, 3.2285, 3.2266, 3.2252, 3.2204, 3.2203, 3.2201, 3.2181, 3.2181, 3.2157, 3.2076, 3.2032, 3.213, 3.2066, 3.1648, 3.0339, 3.0959, 2.8298, 2.5329, 2.3588, 2.6463, 1.8437, 2.5125, 1.6876, 0.5049, 0.9878, 3.8416, 3.8366, 3.8341, 3.8305, 3.829, 3.827, 3.8256, 3.825, 3.8226, 3.8204, 3.8135, 3.6472, 3.8042, 3.5046, 3.2639, 3.1149, 3.5683, 3.1824, 3.295, 2.4883, 2.126, 1.5758, 0.4901, 1.7126, -1.4489, 4.0008, 3.9992, 3.9941, 3.9925, 3.9909, 3.9907, 3.9835, 3.9831, 3.9828, 3.9823, 3.9684, 3.9756, 3.9593, 3.9756, 3.8584, 3.8421, 3.8324, 3.6218, 3.2684, 3.5104, 2.2152, 2.338, 2.4415, 4.0385, 4.0377, 4.0355, 4.0354, 4.0352, 4.0334, 4.0327, 4.0324, 4.0318, 4.0292, 4.003, 3.9583, 3.9751, 3.8003, 3.1465, 3.7198, 2.6762, -0.6688, 0.7324, 3.2082]}, \"token.table\": {\"Topic\": [3, 3, 1, 7, 1, 2, 3, 4, 5, 6, 2, 6, 10, 7, 8, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 6, 7, 8, 9, 10, 10, 1, 6, 7, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 6, 7, 3, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 6, 2, 1, 2, 3, 4, 6, 7, 8, 9, 5, 1, 2, 3, 4, 7, 7, 9, 1, 3, 5, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 4, 5, 6, 7, 8, 9, 10, 2, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 6, 9, 8, 1, 4, 9, 7, 3, 9, 6, 7, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 9, 6, 2, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 4, 5, 1, 1, 2, 3, 4, 5, 6, 7, 1, 1, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 3, 1, 2, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 9, 4, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 8, 9, 1, 4, 6, 7, 1, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 3, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 6, 3, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 4, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 8, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 2, 4, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 8, 9, 1, 7, 1, 2, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 6, 7, 1, 7, 2, 1, 2, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 9, 1, 4, 6, 7, 9, 1, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 6, 7, 10, 1, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 8, 1, 2, 3, 4, 1, 2, 4, 5, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 8, 1, 2, 3, 4, 7, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 5, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 6, 9, 10, 6, 1, 2, 3, 6, 7, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 9, 2, 1, 2, 3, 4, 6, 7, 4, 1, 4, 6, 7, 9, 6, 1, 2, 3, 4, 5, 7, 8, 10, 1, 6, 9, 1, 2, 3, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 7, 6, 7, 3, 6, 7, 8, 9, 10, 1, 4, 6, 7, 10, 6, 1, 3, 4, 5, 7, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 6, 7, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 6, 8, 10, 1, 7, 5, 1, 6, 7, 1, 2, 3, 4, 5, 6, 7, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 4, 9, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 6, 1, 2, 3, 4, 5, 6, 7, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 1, 2, 3, 5, 6, 7, 5, 1, 4, 6, 7, 9, 1, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 10, 1, 6, 7, 9, 1, 8], \"Freq\": [0.944153459945429, 0.9863626425202098, 0.9890248214773304, 0.9723511340119748, 0.17835005834197948, 0.002816053552768097, 0.6974425965688987, 0.10043924338206213, 0.002816053552768097, 0.016896321316608582, 0.9895273675696244, 0.003945097838062198, 0.9902195573536117, 0.9653258106469828, 0.9926357472449706, 0.9630848903116608, 0.5779730550478791, 0.0022577072462807777, 0.0007525690820935926, 0.0052679835746551485, 0.0030102763283743703, 0.1949153922622405, 0.21109562752725272, 0.0003762845410467963, 0.004139129951514759, 0.9929965680328163, 0.033545581011391525, 0.05904022258004908, 0.12697002412811692, 0.05048609942214424, 0.7052958407645068, 0.021972355562461448, 0.0020127348606834915, 0.00016772790505695763, 0.00033545581011391526, 0.00016772790505695763, 0.998948306638797, 0.03246814475208255, 0.5885647024176534, 0.04711064140498253, 0.19130740192158446, 0.09804106454550418, 0.026738472148773867, 0.0003183151446282603, 0.015279126942156496, 0.0003183151446282603, 0.9905624266355318, 0.03533702182084336, 0.03533702182084336, 0.9246520709787347, 0.9963507998200994, 0.024795854617675857, 0.06824345828404595, 0.8653314396885375, 0.004169214493237533, 0.011081333258341866, 0.020626640124438322, 0.0036206336388641736, 0.001974891075744095, 0.00010971617087467193, 0.00010971617087467193, 0.9806309391110654, 0.9721269449949657, 0.007227709628215358, 0.014455419256430716, 0.003613854814107679, 0.9597990969448976, 0.04021655246007355, 0.006093417039405084, 0.10297874796594592, 0.8396728680300205, 0.008530783855167118, 0.0012186834078810166, 0.0006093417039405083, 0.0012186834078810166, 0.016284225809576145, 0.002326317972796592, 0.025589497700762513, 0.9537903688466027, 0.025259953711424265, 0.004857683406043128, 0.9666789978025824, 0.0019430733624172512, 0.6713421611519893, 0.0072982276448825606, 0.015070366175796457, 0.0715605437907316, 0.15051409740355204, 0.07146576161352533, 0.011942554327989645, 0.00047391088603133513, 0.00018956435441253405, 9.478217720626702e-05, 0.04186245854704769, 0.011417034149194824, 0.087530595143827, 0.007611356099463217, 0.8486662050901487, 0.056057883796765455, 0.19710675270475594, 0.032549738978767034, 0.04882460846815055, 0.0018083188321537243, 0.00904159416076862, 0.04339965197168938, 0.609403446435805, 0.7396540426283997, 0.0028388675746379615, 0.014968574484454706, 0.00903276046475715, 0.07368151750537617, 0.15226653354876338, 0.006064853454908372, 0.0011613549168973478, 0.00025807887042163285, 0.8191333666375362, 0.005953915059721954, 0.01911520098121259, 0.0028202755546051362, 0.12691239995723114, 0.021308748634794362, 0.0037603674061401815, 0.0003133639505116818, 0.0006267279010233636, 0.962066481780569, 0.024988739786508285, 0.017723566869523186, 0.4760043673529084, 0.19002195622253074, 0.08051563235011962, 0.1763494903517557, 0.035320536832835495, 0.005570263873278716, 0.009747961778237751, 0.0005063876248435196, 0.008355395809918073, 0.9890100762203438, 0.9967341418536627, 0.9947883604008249, 0.6369093785976938, 0.00396715920983841, 0.01334408097854738, 0.07032691326531727, 0.02524555860806261, 0.0609499914966083, 0.00036065083725803727, 0.18862038788595348, 0.9888541445779646, 0.2992639208655581, 0.011121294356490336, 0.011121294356490336, 0.017187454914575972, 0.6612115008313345, 0.013293427390004463, 0.9859291980919976, 0.19729117382362982, 0.7948015859751945, 0.001878963560225046, 0.001878963560225046, 0.9079824378891382, 0.019990343360247537, 0.002104246669499741, 0.0005260616673749352, 0.06628377008924184, 0.0031563700042496114, 0.11549209195311716, 0.6032942171761515, 0.04949661083705021, 0.10528884322793576, 0.0015196327888568047, 0.09009251533936771, 0.018018503067873542, 0.011071610318813863, 0.004558898366570414, 0.0015196327888568047, 0.18961171146755934, 0.6487500180316755, 0.11019319357538788, 0.04235654287582477, 0.008272762280434526, 0.000330910491217381, 0.000330910491217381, 0.05838996141219774, 0.003133119880654513, 0.010823505042261044, 0.003417948960714014, 0.0002848290800595012, 0.003702778040773515, 0.0002848290800595012, 0.9197130995121293, 0.005238966202223189, 0.9901646122201827, 0.8375045082779167, 0.0051030009034725606, 0.017554323107945607, 0.002041200361389024, 0.07042141246792133, 0.044498167878280724, 0.022453203975279265, 0.0002041200361389024, 0.0002041200361389024, 0.00229259467123801, 0.00229259467123801, 0.027511136054856118, 0.00229259467123801, 0.9628897619199641, 0.9844338546193205, 0.984762339279593, 0.9812283800189977, 0.9852207797622592, 0.9833102342042946, 0.9575925922477156, 0.9950381022895531, 0.9932637658493297, 0.005159811770645869, 0.9713891028058164, 0.005923104285401319, 0.011846208570802638, 0.005923104285401319, 0.056996722636040685, 0.9195471251947898, 0.021848743677148928, 0.0009499453772673448, 0.0013583229624805789, 0.9861424707609002, 0.0054332918499223155, 0.0054332918499223155, 0.04273281782120248, 0.0013081474843225247, 0.00043604916144084164, 0.14607646908268193, 0.7809640481405473, 0.02747109717077302, 0.00043604916144084164, 0.8106622619220535, 0.04248215711771199, 0.019373730993242282, 0.01213775917648914, 0.07002553371051427, 0.0032678582398239994, 0.041781901780606846, 0.00023341844570171424, 0.9935251456708848, 0.9940190134611975, 0.0033693747117766026, 0.9552177307886668, 0.005054062067664904, 0.03200905976187772, 0.0016846873558883013, 0.0004302810510251385, 0.0012908431530754155, 0.0021514052551256926, 0.226758113890248, 0.046470353510714955, 0.006454215765377078, 0.0030119673571759697, 0.7129757015486545, 0.9636002479790887, 0.016060004132984813, 0.9917922823496198, 0.8745987613368023, 0.0024459414020605737, 0.001048260600883103, 0.0017471010014718383, 0.001048260600883103, 0.07093230065975663, 0.04787056744032837, 0.9976534366591427, 0.9807183911473283, 0.0015041693115756568, 0.013537523804180912, 0.0030083386231513137, 0.012621429934337314, 0.09897858211664526, 0.8346750901313071, 0.004816071948628712, 0.00913392955774411, 0.03138750338933885, 0.007639286539204164, 0.0008303572325221918, 0.00016607144650443835, 0.9563273125545326, 0.11910902240368221, 0.0006046143269222447, 0.007859986249989181, 0.3724424253841027, 0.007859986249989181, 0.4915514477877849, 0.8087360276112694, 0.007849746333057966, 0.04030950819678415, 0.02906527696294436, 0.07595159965499329, 0.036915023296002325, 0.0008486212251954557, 0.00021215530629886393, 0.9670692096132171, 0.0334858596774181, 0.0016334565696301511, 0.0065338262785206045, 0.0008167282848150756, 0.9302535164043712, 0.016334565696301512, 0.0008167282848150756, 0.005717097993705529, 0.004900369708890453, 0.9964980392663821, 0.68735100220552, 0.0012376340350313212, 0.010055776534629485, 0.008508733990840333, 0.06265522302346063, 0.2267964369194896, 0.0027846765788204727, 0.0006188170175156606, 0.004844010093364997, 0.004844010093364997, 0.004844010093364997, 0.009688020186729995, 0.004844010093364997, 0.004844010093364997, 0.9688020186729994, 0.9646765439548218, 0.9851461886279875, 0.2132105982014028, 0.17731390174196218, 0.05167546413392001, 0.42602672721094365, 0.025048793683236037, 0.09151685251198048, 0.012425779543652522, 0.0007889383837239697, 0.0019723459593099244, 0.00019723459593099242, 0.003628816014683848, 0.03386894947038258, 0.0024192106764558984, 0.08225316299950056, 0.008467237367595646, 0.006048026691139747, 0.8636582114947557, 0.025508481479190877, 0.009183053332508715, 0.9121832976958658, 0.05203730221754939, 0.03529263168058123, 0.9095415364538362, 0.05445148887861103, 0.9789510730243388, 0.08566652562543471, 0.0017357937254259715, 0.6064046432532261, 0.19175415390293965, 0.07300544198115115, 0.03798325093285067, 0.0018378992386863227, 0.0002042110265207025, 0.0009189496193431614, 0.0002042110265207025, 0.13269321158647301, 0.0019046872476527227, 0.7174321966158589, 0.12189998384977425, 0.015237497981221782, 0.009523436238263613, 0.11801392406756155, 0.010335231639318017, 0.3431777612701922, 0.18968755371511115, 0.33510185933807396, 0.0022593297071997528, 0.0007210626725105594, 0.00014421253450211188, 0.00028842506900422375, 0.00024035422417018645, 0.9479969540587266, 0.015799949234312112, 0.031599898468624224, 0.14759713209898695, 0.1586688015458913, 0.2632607427038813, 0.14624026367315357, 0.2523492591128045, 0.013992705641406769, 0.0026572006672570428, 0.0050317204124654645, 0.0011966825700058315, 0.008998676018547787, 0.026425394897789577, 0.9513142163204248, 0.9639568079570954, 0.9961419497255273, 0.0029042039350598466, 0.10796809761019144, 0.09341981736443594, 0.05663900202501846, 0.5385022189482319, 0.049774976627169426, 0.10490302966523998, 0.007986444645296048, 0.03501084652613565, 0.0047055268450663205, 0.0011655892185026666, 0.9946308744531047, 0.013261800009964674, 0.9855175132404997, 0.004187535116397659, 0.012562605349192976, 0.9798832172370522, 0.23055785376314755, 0.18474101123729253, 0.05747200422102864, 0.22881627787766182, 0.020764943250022004, 0.21796492043732774, 0.027061409912931902, 0.015942117720984635, 0.014736411338725292, 0.00187554326129231, 0.8752852352387942, 0.004973211563856785, 0.10941065440484928, 0.007459817345785177, 0.07215858273109606, 0.08126098787723635, 0.07891198009758724, 0.2536194337089897, 0.44741257553004116, 0.017177119388684107, 0.009689657091052573, 0.0005138454517982426, 0.002716040245219282, 0.036556433570789255, 0.9917950381094444, 0.9578755812115438, 0.13870952923821817, 0.09616991457589497, 0.41855942283821584, 0.08545904731270287, 0.2541741976073812, 0.005469379028012984, 0.0010634903665580802, 0.00022789079283387433, 0.00022789079283387433, 7.596359761129144e-05, 0.9949286733005858, 0.10043629825362975, 0.015160195962812038, 0.07390595531870868, 0.011370146972109028, 0.015160195962812038, 0.784540141075523, 0.2751536328056478, 0.00984889960825585, 0.01908224299099571, 0.5306094663947839, 0.03354781429062149, 0.036933373530959435, 0.09387232439118856, 0.0009233343382739859, 0.0003077781127579953, 0.9816734910970993, 0.2607586305592965, 0.019219261614928228, 0.010908229565229534, 0.27790013416180004, 0.0010388790062123366, 0.032205249192582434, 0.0753187279503944, 0.25816143304376565, 0.0638910588820587, 0.0005194395031061683, 0.9815884404884626, 0.101449644013001, 0.1733980767739615, 0.4819054242288106, 0.21945448902116302, 0.0072968421447866146, 0.007689145485904174, 0.0018045953691407757, 0.0035307300700580394, 0.0007846066822351198, 0.0025892020513758954, 0.9867798306224084, 0.009238163161133978, 0.018476326322267957, 0.9700071319190677, 0.996636868262037, 0.020480593945847902, 0.03489286375959272, 0.7486794897982176, 0.08874924043200756, 0.03944410685867003, 0.0022756215495386557, 0.06068324132103082, 0.0015170810330257705, 0.003792702582564426, 0.9828622125982487, 0.7175947344615717, 0.036662826215050535, 0.024085934374288864, 0.08637714396296696, 0.08839419265440987, 0.017322888761803816, 0.026696232680862044, 0.00023729984605210704, 0.002491648383547124, 0.00011864992302605352, 0.0046960345330489895, 0.9908632864733367, 0.17115810783025004, 0.0008035591916913148, 0.0022097877771511155, 0.06629363331453347, 0.0038169061605337453, 0.7463055992833086, 0.005022244948070718, 0.0002008897979228287, 0.002611567372996773, 0.001406228585459801, 0.9709151630076946, 0.0986255179980599, 0.15645974526604403, 0.059231189279288096, 0.36935675578876825, 0.0790680498398044, 0.05587848045215858, 0.04749670838433479, 0.11874177096083699, 0.004470278436172687, 0.010337518883649338, 0.9833149656867819, 0.3986383963464427, 0.013467513390082524, 0.008080508034049513, 0.5791030757735485, 0.9909125971256227, 0.25224052265719343, 0.06281283603424229, 0.012859320762915745, 0.040061730069083666, 0.000494589260112144, 0.0830909956988402, 0.05638317565278442, 0.48271911786945254, 0.008408017421906448, 0.9521304106795321, 0.19642221272455226, 0.05731579381883099, 0.01564102805028842, 0.0865193079858147, 0.5682214010960263, 0.0517037305981295, 0.012055543214840245, 0.0035854848354481762, 0.003481557738768519, 0.005092427737303207, 0.9821555510888409, 0.12255898576328232, 0.052424074835161226, 0.07509394503414986, 0.41443356457526104, 0.0021253003311551845, 0.046756607285414066, 0.07226021125927629, 0.004250600662310369, 0.2096962993406449, 0.0014168668874367898, 0.017059872877917492, 0.10920491874080943, 0.15223491657300897, 0.1557120881150049, 0.5555868154445359, 0.007388989526741334, 0.0008692928854989805, 0.0007606312748116079, 0.0005433080534368628, 0.0007606312748116079, 0.14078106591086534, 0.018564536164070156, 0.8044632337763734, 0.015470446803391796, 0.020111580844409335, 0.0206223095268053, 0.9780866804141942, 0.26472962775432773, 0.0030197295941558296, 0.01811837756493498, 0.1630653980844148, 0.5505973626677463, 0.34863677427724865, 0.15439491408367279, 0.09390436938671143, 0.09131191747112738, 0.08180626044731916, 0.13653580088742706, 0.064619264414373, 0.005568970781625013, 0.020835632062286686, 0.002304401702741385, 0.12959440829053667, 0.005183776331621467, 0.8605068710491635, 0.5426672529375727, 0.09894496905064765, 0.18335420811189604, 0.049344978121908044, 0.0971598794028267, 0.017978402881625413, 0.007395371398115417, 0.0015300768409893966, 0.0016575832444051797, 0.9501491997671603, 0.00895428565225596, 0.0397968251211376, 0.18681755599248168, 0.8102668416883216, 0.9853435802901462, 0.09131263419575796, 0.00537133142327988, 0.8970123476877399, 0.00537133142327988, 0.2194173444626463, 0.045689926556596594, 0.0006245731285339177, 0.0012011021702575339, 0.724745049533396, 0.005332893635943451, 0.0012491462570678354, 0.001633498951550246, 4.8044086810301355e-05, 4.8044086810301355e-05, 0.9776011834654792, 0.009400011379475761, 0.2656689218292953, 0.0010323921832744636, 0.015485882749116954, 0.29182285713891504, 0.000688261455516309, 0.011700444743777254, 0.4129568733097854, 0.0003441307277581545, 0.004333558328148576, 0.10400539987556584, 0.8854904183850257, 0.004333558328148576, 0.000722259721358096, 0.01325391880830721, 0.9741630324105799, 0.006626959404153605, 0.20089771591420721, 0.1989676941892672, 0.1045720861876572, 0.13001328165277515, 0.34871983439256493, 0.00587779343504449, 0.010176478186047177, 0.0006140978215718124, 0.00017545652044908926, 0.00017545652044908926, 0.9883268949659617, 0.00918493953469218, 0.00459246976734609, 0.00459246976734609, 0.04133222790611481, 0.9368638325386024, 0.001280591494760475, 0.0102447319580838, 0.7824414032986502, 0.0051223659790419, 0.17159926029790365, 0.0256118298952095, 0.00256118298952095, 0.259121020920356, 0.03548472579482582, 0.06821869765206824, 0.2225359935504968, 0.09600131242166055, 0.07014422540837661, 0.040711158276234276, 0.17962423212419582, 0.021180805319392156, 0.006876884843958492, 0.02908536643662476, 0.959817092408617, 0.08519078255522705, 0.1408600067992368, 0.14029769140283266, 0.5763732813142424, 0.0101216771352745, 0.008715888644264154, 0.03205197759503592, 0.0002811576982020695, 0.005904311662243459, 0.0002811576982020695, 0.3205290679208432, 0.40683983063299195, 0.04139905769623415, 0.18203040217344166, 0.0005018067599543533, 0.032491987707044374, 0.00451626083958918, 0.000376355069965765, 0.011416103788961537, 0.24848049794679944, 0.010676896396151539, 0.061149497541595176, 0.09851863492812556, 0.15869750552461606, 0.00727970208828514, 0.006794388615732797, 0.00048531347255234265, 0.40863394388907254, 0.9901639882880342, 0.07671019371451779, 0.03486826987023536, 0.864733092781837, 0.027894615896188288, 0.0013040679407038697, 0.0013040679407038697, 0.010432543525630958, 0.006520339703519348, 0.016952883229150305, 0.9624021402394558, 0.17332592957490423, 0.19194628546297124, 0.49874710257196336, 0.035152447564388206, 0.020795631108635595, 0.05298970437305054, 0.017924267817485073, 0.005394682547010071, 0.003219407326441494, 0.0005220660529364585, 0.41555707000679737, 0.020745438752289416, 0.027876683323388902, 0.01685566898623515, 0.008427834493117575, 0.01685566898623515, 0.004538064727063309, 0.010372719376144708, 0.4784416812246747, 0.9942454623349392, 0.983331058916388, 0.07881146102718599, 0.8949479241087119, 0.0052540974018123986, 0.014010926404833063, 0.0052540974018123986, 0.99580880081797, 0.21391640490668223, 0.11214815742470578, 0.04535569344540415, 0.09545004142988037, 0.5007178296286159, 0.019857219020873458, 0.008574708213558994, 0.0018052017291703144, 0.0020308519453166034, 0.9931427258333828, 0.9844615234538049, 0.05708352347062889, 0.05608845914516761, 0.5379649431552178, 0.23606242681026485, 0.09101521696885861, 0.01170859022959442, 0.0030515305980812653, 0.0024876608136532056, 0.003018361787232556, 0.0014925964881919232, 0.03272177765802112, 0.06241376108844769, 0.16906251789977578, 0.6962467135012271, 0.03756944842217239, 0.0006059588455189096, 0.0006059588455189096, 0.006673127289649825, 0.11010660027922212, 0.015848677312918336, 0.09592620478871623, 0.0266925091585993, 0.0016682818224124563, 0.01751695913533079, 0.5605426923305853, 0.1626574776852145, 0.0016682818224124563, 0.995606770236475, 0.0074068153344835, 0.14813630668966998, 0.8406735404638772, 0.943245552799306, 0.20402209055542117, 0.07152309151942875, 0.020764768505640605, 0.24555162756670237, 0.19215650855219799, 0.007910388002148801, 0.0013183980003581336, 0.0003295995000895334, 0.0013183980003581336, 0.25511001306929887, 0.9899832208855391, 0.9750143636410241, 0.008674505014599859, 0.013879208023359773, 0.0017349010029199716, 0.0008674505014599858, 0.9890025755932743, 0.9917064062569972, 0.9985028776315652, 0.5067468456087552, 0.0013845542229747407, 0.0013845542229747407, 0.009691879560823185, 0.4818248695952098, 0.004752253244838958, 0.9932209281713422, 0.1175283752167414, 0.04023592696319411, 0.07301669579696807, 0.3315352674024496, 0.42900049647678085, 0.002631232280971822, 0.004385387134953037, 0.00010963467837382592, 0.001315616140485911, 0.00032890403512147775, 0.12116933554217994, 0.8736946825936133, 0.23159859928561954, 0.07350738151239229, 0.6917749465618288, 0.0020139008633532135, 0.11874097782943475, 0.052810213896516564, 0.28209182578263503, 0.059206482039262355, 0.48152418633593985, 0.004756199388195591, 0.00032801375091004074, 0.0004920206263650611, 0.9814991409096833, 0.024738054688622415, 0.9545476964333959, 0.0017060727371463733, 0.0025591091057195602, 0.01535465463431736, 0.9858606152775357, 0.9518529971802145, 0.9650878515912431, 0.0016247270228808806, 0.0032494540457617613, 0.02924508641185585, 0.0016247270228808806, 0.9972086705789812, 0.05764404888590289, 0.8037843078295992, 0.05495040174170182, 0.07865449661067123, 0.004848564859561926, 0.0005387294288402139, 0.9716615543810944, 0.013685374005367526, 0.010038801949140629, 0.0351358068219922, 0.9536861851683598, 0.09169773922768674, 0.026745173941408632, 0.013372586970704316, 0.07450441312249548, 0.09169773922768674, 0.017193326105191264, 0.0019103695672434736, 0.6820019355059201, 0.11798029864366474, 0.2504812494280882, 0.04613332190553557, 0.14566029178698608, 0.275892390674416, 0.03993179338708653, 0.012554313830030991, 0.1036109032960389, 0.005445244552784527, 0.0022688518969935525, 0.9911941538953832, 0.994155735344848, 0.9802511283045187, 0.9739791498058897, 0.024320525738315914, 0.979294614853706, 0.20161195214545027, 0.005305577688038165, 0.7586976093894575, 0.005305577688038165, 0.02122231075215266, 0.0009652424275098002, 0.0009652424275098002, 0.975860094212408, 0.021235333405215606, 0.0009652424275098002, 0.9959364012885907, 0.1357583081574767, 0.7306542652520375, 0.13118218541059545, 0.0015253742489604123, 0.0015253742489604123, 0.07322225611457235, 0.9152782014321543, 0.23841137243333438, 0.02298181712475562, 0.03437280474311275, 0.5971275614675634, 0.0015987351043308257, 0.08892964017840217, 0.009192726849902248, 0.001998418880413532, 0.005395730977116537, 0.2200465721153251, 0.002316279706477106, 0.030111636184202383, 0.020846517358293955, 0.7249955481273342, 0.9697569516489166, 0.08847473316423891, 0.3866956009850787, 0.19754272318567137, 0.23933940747360494, 0.0672713057334989, 0.013271209974635838, 0.004881364588371803, 0.0012203411470929507, 0.0012203411470929507, 0.00015254264338661883, 0.025229711588910096, 0.18348881155570979, 0.030390334413914435, 0.717326572675603, 0.018922283691682574, 0.013761660866678235, 0.0017202076083347793, 0.00917444057778549, 0.0005734025361115931, 0.9782674406519559, 0.019098496672164, 0.9673423098666623, 0.01915529326468638, 0.013864539667174535, 0.9843823163693919, 0.9562331193422824, 0.03035234707129335, 0.8088900494499678, 0.16010863080107243, 0.9272525626222381, 0.004158083240458466, 0.031878638176848245, 0.0034650693670487225, 0.0006930138734097445, 0.002079041620229233, 0.029799596556619012, 0.9691164287124303, 0.342908525833342, 0.2289807061643125, 0.07010942748863357, 0.060812822661171176, 0.21796689407572648, 0.026942389786594825, 0.019895918611639255, 0.022856620785990333, 0.009059748653514303, 0.00047371234789617275, 0.9937037076540008, 0.14517890621661958, 0.24162588432281137, 0.2331779738317581, 0.32618321062678, 0.016661156801799554, 0.030662786226786035, 0.003441741311169861, 0.0021119776227633237, 0.0007822139343567865, 0.00015644278687135732, 0.9828779999101332, 0.04564876455083941, 0.04564876455083941, 0.007608127425139902, 0.8977590361665084, 0.9622425046169597, 0.9726145195925477, 0.007896304295569046, 0.003454633129311458, 0.1041325128978168, 0.03503985031158765, 0.006415747240149851, 0.0009870380369461308, 0.822202684776127, 0.0009870380369461308, 0.01826020368350342, 0.9796873983433447, 0.997527138798889, 0.9130405077835553, 0.020289789061856783, 0.011781167842368455, 0.0019635279737280757, 0.04188859677286562, 0.002618037298304101, 0.00850862121948833, 0.9788149348958272, 0.03392233462557355, 0.8438180738111419, 0.03830884341336323, 0.08144284649329511, 0.0004386508787789683, 0.0004386508787789683, 0.0008773017575579366, 0.00014621695959298942, 0.0004386508787789683, 0.3164984774271757, 0.02276082344957337, 0.0612081603576365, 0.20392467496036684, 0.00984251824846416, 0.10734496464731225, 0.24667811360213301, 0.00369094434317406, 0.020607772582721836, 0.007689467381612625, 0.01600914231219238, 0.00800457115609619, 0.004002285578048095, 0.004002285578048095, 0.9645508243095909, 0.9786067460557272, 0.005053046881526991, 0.005053046881526991, 0.003368697921017994, 0.0008421744802544985, 0.007579570322290486, 0.9762197888760682, 0.03546064809405186, 0.006513180262172791, 0.8076343525094261, 0.14835577263838023, 0.0007236866957969768, 0.02675414963607902, 0.005350829927215804, 0.9577985569716289, 0.7929298142354689, 0.0022747209588395, 0.0024642810387427916, 0.009857124154971166, 0.026917531346267416, 0.06009054532934346, 0.10482672418652028, 0.00018956007990329166, 0.0003791201598065833, 0.22198198241056055, 0.20327863772021482, 0.06305699067030848, 0.07545463629362338, 0.38860206453769774, 0.023512776182148927, 0.018168963413478716, 0.002992535150455318, 0.0006412575322404253, 0.0022444013628414887, 0.007472903332546366, 0.0018682258331365915, 0.4651882324510113, 0.0018682258331365915, 0.5174985557788359, 0.005604677499409775, 0.6661134159082951, 0.012749582168310637, 0.04431002327347303, 0.021736992549250923, 0.0014630668061995811, 0.07252631167875066, 0.013585620343281826, 0.0004180190874855946, 0.16699862545049507, 0.00010450477187139865, 0.9971351685250387, 0.07112959977555311, 0.58935954099744, 0.12843973445185589, 0.10262985110472662, 0.10649117223539951, 0.00040645485586030347, 0.00020322742793015173, 0.0012193645675809103, 0.07430421885719134, 0.11304693847539049, 0.177921111208504, 0.10566196259508859, 0.2941492700631015, 0.17076336535528835, 0.016019716909577948, 0.00113615013543106, 0.029426288507664457, 0.017610327099181432, 0.03158669300609579, 0.04895937415944848, 0.003158669300609579, 0.20689283918992743, 0.12792610667468796, 0.12002943342316401, 0.004738003950914369, 0.003158669300609579, 0.017372681153352686, 0.4343170288338172, 0.006508951772433263, 0.7953939065913448, 0.13050448303728693, 0.05890601354052103, 0.00032544758862166316, 0.0026035807089733053, 0.0052071614179466106, 0.00032544758862166316, 0.034803908153608114, 0.9501466925935015, 0.002320260543573874, 0.010441172446082433, 0.0036357677835480093, 0.9925646049086065], \"Term\": [\"ad\", \"adictive\", \"aesthetic\", \"agian\", \"alone\", \"alone\", \"alone\", \"alone\", \"alone\", \"alone\", \"alpha\", \"alright\", \"alright\", \"ankh\", \"apple\", \"archer\", \"armor\", \"armor\", \"armor\", \"armor\", \"armor\", \"armor\", \"armor\", \"armor\", \"armor\", \"array\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awsome\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"badgei\", \"beast\", \"beast\", \"beast\", \"bee\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"beter\", \"biomes\", \"biomes\", \"biomes\", \"biomes\", \"bla\", \"bore\", \"bore\", \"bore\", \"bore\", \"bore\", \"bore\", \"bore\", \"bore\", \"bored\", \"bored\", \"bored\", \"bored\", \"boring\", \"boring\", \"boring\", \"boring\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"box\", \"box\", \"box\", \"box\", \"box\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"building\", \"butter\", \"butter\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"cancer\", \"cat\", \"cent\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"chinese\", \"class\", \"class\", \"class\", \"class\", \"class\", \"click\", \"click\", \"co\", \"co\", \"co\", \"co\", \"combat\", \"combat\", \"combat\", \"combat\", \"combat\", \"combat\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"crack\", \"crack\", \"craft\", \"craft\", \"craft\", \"craft\", \"craft\", \"craft\", \"craft\", \"craft\", \"craft\", \"crash\", \"crash\", \"crash\", \"crash\", \"crash\", \"crust\", \"customizable\", \"dank\", \"data\", \"de\", \"definently\", \"delete\", \"demon\", \"demon\", \"dev\", \"dev\", \"dev\", \"dev\", \"developer\", \"developer\", \"developer\", \"developer\", \"devs\", \"devs\", \"devs\", \"devs\", \"die\", \"die\", \"die\", \"die\", \"die\", \"die\", \"die\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dink\", \"dlc\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dosent\", \"dosent\", \"emphasis\", \"enemy\", \"enemy\", \"enemy\", \"enemy\", \"enemy\", \"enemy\", \"enemy\", \"environment\", \"equipment\", \"equipment\", \"equipment\", \"equipment\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"exceptional\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"explore\", \"explore\", \"explore\", \"explore\", \"explore\", \"explore\", \"explore\", \"explore\", \"extreamly\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eyeball\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"finaly\", \"finite\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fix\", \"fix\", \"fix\", \"fix\", \"fix\", \"fix\", \"fix\", \"flesh\", \"flesh\", \"flesh\", \"flesh\", \"fly\", \"fly\", \"fly\", \"freeze\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friends\", \"friends\", \"friends\", \"friends\", \"friends\", \"friends\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"funnest\", \"funnest\", \"funnest\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gamplay\", \"gamplay\", \"gb\", \"generate\", \"generate\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gg\", \"giant\", \"giant\", \"glitch\", \"glitch\", \"glitch\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"gon\", \"gon\", \"gon\", \"gon\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gr\", \"grate\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"gud\", \"guess\", \"guess\", \"guess\", \"guess\", \"guess\", \"guess\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"heat\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"horribly\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hype\", \"idk\", \"idk\", \"idk\", \"ign\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"itch\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"k\", \"k\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"kindle\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"language\", \"launch\", \"launch\", \"launch\", \"launch\", \"le\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"liek\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"liked\", \"lose\", \"lose\", \"lose\", \"lose\", \"lose\", \"lose\", \"lose\", \"lose\", \"lose\", \"lose\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mage\", \"mage\", \"magic\", \"magic\", \"magic\", \"magic\", \"magic\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"mana\", \"mana\", \"mana\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"material\", \"material\", \"material\", \"melee\", \"melee\", \"meme\", \"mincraft\", \"mincraft\", \"mincraft\", \"mincraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"mineral\", \"mineral\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"moon\", \"moon\", \"moon\", \"moon\", \"moon\", \"mouse\", \"mouse\", \"mouse\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"mutch\", \"n\", \"n\", \"n\", \"n\", \"n\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"net\", \"net\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nuff\", \"offline\", \"offline\", \"offline\", \"offline\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"otherworld\", \"pastry\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"penny\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"pickle\", \"pie\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"pls\", \"plz\", \"plz\", \"plz\", \"pony\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"procedurally\", \"progression\", \"progression\", \"progression\", \"progression\", \"progression\", \"pun\", \"r\", \"rainbow\", \"range\", \"range\", \"range\", \"range\", \"range\", \"ranger\", \"ranger\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"realy\", \"realy\", \"recomend\", \"recomend\", \"recomend\", \"recomend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"reinstall\", \"release\", \"release\", \"release\", \"release\", \"release\", \"relly\", \"reson\", \"resource\", \"resource\", \"resource\", \"resource\", \"resource\", \"rid\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"salt\", \"salt\", \"sand\", \"sand\", \"sand\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"selection\", \"shark\", \"shield\", \"shoot\", \"shoot\", \"sibling\", \"simulator\", \"simulator\", \"simulator\", \"simulator\", \"simulator\", \"slime\", \"slime\", \"slime\", \"slime\", \"slime\", \"slit\", \"solo\", \"solo\", \"solo\", \"solo\", \"solo\", \"soooo\", \"soooo\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"step\", \"step\", \"step\", \"step\", \"step\", \"steroid\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"structure\", \"structure\", \"sugar\", \"sugar\", \"summoner\", \"summoner\", \"swag\", \"sword\", \"sword\", \"sword\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"teir\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"tickle\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timesink\", \"tired\", \"tired\", \"tired\", \"tired\", \"tmodloader\", \"trash\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"uh\", \"unicorn\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unplayable\", \"update\", \"update\", \"update\", \"update\", \"update\", \"update\", \"update\", \"update\", \"update\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"v\", \"v\", \"v\", \"v\", \"v\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"variety\", \"verry\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"warrior\", \"warrior\", \"warrior\", \"weapon\", \"weapon\", \"weapon\", \"weapon\", \"weapon\", \"weapon\", \"weapon\", \"weapon\", \"weapon\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wont\", \"wont\", \"wont\", \"wont\", \"wont\", \"wont\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worm\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"zombie\", \"zombie\", \"zombie\", \"zombie\", \"zone\", \"zone\"]}, \"R\": 10, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 9, 7, 2, 8, 10, 1, 4, 6, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el907601203339360039007792\", ldavis_el907601203339360039007792_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el907601203339360039007792\", ldavis_el907601203339360039007792_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el907601203339360039007792\", ldavis_el907601203339360039007792_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.067534  0.201313       1        1  28.026347\n",
       "8     -0.145865  0.031390       2        1  13.795712\n",
       "6     -0.130033  0.245266       3        1  13.514286\n",
       "1     -0.066946  0.113664       4        1  13.348451\n",
       "7     -0.264221  0.153866       5        1  13.046183\n",
       "9      0.252270  0.135880       6        1   8.644870\n",
       "0      0.161209 -0.074485       7        1   3.902303\n",
       "3     -0.230721 -0.262877       8        1   2.143332\n",
       "5      0.335549 -0.151771       9        1   1.822201\n",
       "4      0.021224 -0.392247      10        1   1.756316, topic_info=           Term           Freq          Total Category  logprob  loglift\n",
       "15         game  106126.000000  106126.000000  Default  10.0000  10.0000\n",
       "27    minecraft   20814.000000   20814.000000  Default   9.0000   9.0000\n",
       "173       world    9568.000000    9568.000000  Default   8.0000   8.0000\n",
       "32         play   30148.000000   30148.000000  Default   7.0000   7.0000\n",
       "16          get   23164.000000   23164.000000  Default   6.0000   6.0000\n",
       "...         ...            ...            ...      ...      ...      ...\n",
       "1768       step     312.814609     431.726789  Topic10  -4.4607   3.7198\n",
       "764      pretty     774.273887    3033.985184  Topic10  -3.5544   2.6762\n",
       "15         game     954.909407  106126.723313  Topic10  -3.3447  -0.6688\n",
       "243        good     497.693500   13622.773103  Topic10  -3.9964   0.7324\n",
       "1424       yeah     275.061525     633.178028  Topic10  -4.5893   3.2082\n",
       "\n",
       "[281 rows x 6 columns], token_table=       Topic      Freq       Term\n",
       "term                             \n",
       "12262      3  0.944153         ad\n",
       "9503       3  0.986363   adictive\n",
       "554        1  0.989025  aesthetic\n",
       "5942       7  0.972351      agian\n",
       "75         1  0.178350      alone\n",
       "...      ...       ...        ...\n",
       "405        6  0.950147     zombie\n",
       "405        7  0.002320     zombie\n",
       "405        9  0.010441     zombie\n",
       "2411       1  0.003636       zone\n",
       "2411       8  0.992565       zone\n",
       "\n",
       "[1127 rows x 3 columns], R=10, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 9, 7, 2, 8, 10, 1, 4, 6, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=10)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model\n",
    "\n",
    "we need to save the corpora.Dictionary and the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the LDA multicore model (and the corpora.Dictionary object) automatically\n",
    "\n",
    "# lda_save_folder = Path(f'lda_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "# if not lda_save_folder.exists():\n",
    "#     lda_save_folder.mkdir()\n",
    "\n",
    "# lda_model.save(str(lda_save_folder.joinpath('lda_model')))     # no need to add file extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n",
    "gensim provide functions to calculate, so we don't need to install octis (as the evaluation backend of octis also relies on gensim)\n",
    "\n",
    "octis seems awesome for simple development, but it installs many packages ;("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = lemmatized words (?) (list of list of str)\n",
    "\n",
    "# create a result object from the LDAMulticore model for octis evaluation\n",
    "# referencing from https://github.com/MIND-Lab/OCTIS/blob/master/octis/models/LDA.py\n",
    "# and guideline in README: https://github.com/MIND-Lab/OCTIS/tree/master\n",
    "result_lda_online = {}\n",
    "result_lda_online['topic-word-matrix'] = lda_model.get_topics()\n",
    "\n",
    "top_words = 10\n",
    "topics_output = []\n",
    "for topic in result_lda_online[\"topic-word-matrix\"]:\n",
    "    top_k = np.argsort(topic)[-top_words:]\n",
    "    top_k_words = list(reversed([id2word[i] for i in top_k]))\n",
    "    topics_output.append(top_k_words)\n",
    "result_lda_online[\"topics\"] = topics_output\n",
    "\n",
    "def _get_topic_document_matrix(lda_model, corpus, num_topics=10):\n",
    "    \"\"\"\n",
    "    Return the topic representation of the\n",
    "    corpus\n",
    "    \"\"\"\n",
    "\n",
    "    id_corpus = corpus\n",
    "\n",
    "    doc_topic_tuples = []\n",
    "    for document in id_corpus:\n",
    "        doc_topic_tuples.append(\n",
    "            lda_model.get_document_topics(document, minimum_probability=0))\n",
    "\n",
    "    topic_document = np.zeros((num_topics, len(doc_topic_tuples)))\n",
    "\n",
    "    for ndoc in range(len(doc_topic_tuples)):\n",
    "        document = doc_topic_tuples[ndoc]\n",
    "        for topic_tuple in document:\n",
    "            topic_document[topic_tuple[0]][ndoc] = topic_tuple[1]\n",
    "    return topic_document\n",
    "\n",
    "result_lda_online['topic-document-matrix'] = _get_topic_document_matrix(lda_model, corpus, num_topics=N_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics(num_topics=N_TOPICS, num_words=10, formatted=True, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup: get the model's topics in their native ordering...\n",
    "all_topics = lda_model.print_topics()\n",
    "# ...then create a empty list per topic to collect the docs:\n",
    "docs_per_topic = [[] for _ in all_topics]\n",
    "\n",
    "# now, for every doc...\n",
    "for doc_id, doc_bow in enumerate(corpus):\n",
    "    # ...get its topics...\n",
    "    doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "    # ...& for each of its topics...\n",
    "    for topic_id, score in doc_topics:\n",
    "        # ...add the doc_id & its score to the topic's doc list\n",
    "        docs_per_topic[topic_id].append((doc_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're interested in the top docs per topic, you can further sort each list's pairs by their score\n",
    "\n",
    "for doc_list in docs_per_topic:\n",
    "    doc_list.sort(key=lambda id_and_score: id_and_score[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_per_topic[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top 10 documents for each topic, also the name of the game\n",
    "for topic_id, docs in enumerate(docs_per_topic):\n",
    "    print(f'Topic {topic_id + 1}:')\n",
    "    for doc_id, score in docs[:10]:\n",
    "        print(f'Game: {dataset.iloc[doc_id][\"app_name\"]}')\n",
    "        print(f'Doc ID: {doc_id}')\n",
    "        print(f'Score: {score}')\n",
    "        print(f'Doc: {dataset.iloc[doc_id][\"review_text\"]}')\n",
    "        print()\n",
    "    print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[1655473]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1655473]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lda_online['topic-document-matrix'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_topics().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(result_lda_online['topic-document-matrix'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference test\n",
    "\n",
    "inference_test = [\"well its been fun guys, but that's it, no more updates, that one was the last one, there is no longer going to be anymore content for this game anymore, there is no way to replay it as there won't be any updates, nope, that was it, the last update, nothing more, this game has no new ways to experience it as there is no more content updates, nothing new to freshen up the experience, its such a shame that this game has no replay-ability, once you beat the game there is like no point to playing again, as they said guys 1.2 will be they final update. nothing more after 1.2, there is no chance they will make another final update right? several years and final updates later: alright, thats it, no more updates we wont be getting anymore, thats it, nothing more, no more updates, for real this time... oh god, redigit made another tweet.\",\n",
    "                  \"keeps forcing me to play it\",\n",
    "'''I will leave the cat here, so that everybody who passes by can pet it and give it a thumbs up and awards\n",
    " \n",
    " | _ _ l\n",
    "  ` x\n",
    "  /  |\n",
    " /  \n",
    "  |||\n",
    "| |||\n",
    "| (__)__)\n",
    "''']\n",
    "\n",
    "inference_test = cleaning_strlist(inference_test)\n",
    "\n",
    "inference_test = list(map(lambda x: lemmatization(x), inference_test))\n",
    "\n",
    "corpus_test = [id2word.doc2bow(text) for text in inference_test]\n",
    "\n",
    "test_output = lda_model[corpus_test]\n",
    "\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference\n",
    "\n",
    "corpus_test = [id2word.doc2bow(text) for text in inference_test]\n",
    "\n",
    "output_test = lda_model[corpus_test]\n",
    "\n",
    "for i in range(len(output_test)):\n",
    "    # print(sorted(test_output[i], key=lambda x: x[1], reverse=True))\n",
    "    print(sorted(output_test[i], key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model (both corpora Dictionary and the LDA model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del id2word\n",
    "# del lda_model\n",
    "\n",
    "# model_datetime = datetime(2024, 1, 15, 0, 21, 57)\n",
    "# lda_save_folder = Path(f'lda_model_{model_datetime.strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "\n",
    "# # id2word_load = gensim.corpora.Dictionary.load('lda_model.id2word')\n",
    "# id2word_l = gensim.corpora.Dictionary.load(str(lda_save_folder.joinpath('lda_model.id2word')))\n",
    "\n",
    "# lda_model_l = gensim.models.ldamulticore.LdaMulticore.load(str(lda_save_folder.joinpath('lda_model')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_test2 = [id2word_l.doc2bow(text) for text in inference_test]\n",
    "\n",
    "# output_test2 = lda_model_l[corpus_test2]\n",
    "\n",
    "# for i in range(len(output_test2)):\n",
    "#     print(sorted(output_test2[i], key=lambda x: x[1], reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-test-tm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
