{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo ipynb for LDA\n",
    "\n",
    "Creating training pipeline on different situations (15/01/2024)\n",
    "\n",
    "- For all games (using 0.1 and 0.5 of the whole dataset for pipeline developments)\n",
    "- For top 11 genres\n",
    "\n",
    "- (if possible): per game, focus on large and small (indie) games\n",
    "\n",
    "Do the same thing for all three model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gensim\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "\n",
    "import pyLDAvis\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nltk stopwords\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# download spacy stopwords\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training conditions\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class TRAINING_CONDS(Enum):\n",
    "    ALL_GAMES = 1\n",
    "    ALL_GAMES_LARGE = 2\n",
    "    BY_GENRE = 3\n",
    "    ALL_GAMES_TINY = 4\n",
    "    BY_GAME = 5\n",
    "\n",
    "# training condition\n",
    "training_cond = TRAINING_CONDS.BY_GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 81776 entries, 63365 to 145140\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   index         81776 non-null  int64 \n",
      " 1   app_id        81776 non-null  int64 \n",
      " 2   app_name      81776 non-null  object\n",
      " 3   review_text   81776 non-null  object\n",
      " 4   review_score  81776 non-null  int64 \n",
      " 5   review_votes  81776 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "\n",
    "GENRES = ['Action', 'Indie', 'Adventure', 'RPG', 'Strategy', 'Simulation', 'Free to Play', 'Causal', 'Massively Multiplayer', 'Racing', 'Sports']\n",
    "GAMES = ['Terraria', 'PAYDAY 2', 'Dota 2', 'Rocket League', 'Undertale', 'Left 4 Dead 2', 'Warframe', 'Portal 2', 'Grand Theft Auto V', 'Fallout: New Vegas']\n",
    "\n",
    "training_genre_id = 9\n",
    "training_game_id = 0\n",
    "\n",
    "if training_cond == TRAINING_CONDS.ALL_GAMES:\n",
    "    dataset_path = Path('../dataset_cleaned_heartless_sampled_for_demo.pkl')\n",
    "elif training_cond == TRAINING_CONDS.ALL_GAMES_LARGE:\n",
    "    dataset_path = Path('../dataset_cleaned_heartless_sampled_for_demo_large.pkl')\n",
    "elif training_cond == TRAINING_CONDS.ALL_GAMES_TINY:\n",
    "    dataset_path = Path('../dataset_cleaned_heartless_sampled_for_demo_tiny.pkl')\n",
    "elif training_cond == TRAINING_CONDS.BY_GENRE:\n",
    "    dataset_path = Path(f'../../dataset/topic_modelling/top_11_genres/{training_genre_id:02}_{GENRES[training_genre_id]}.pkl')\n",
    "elif training_cond == TRAINING_CONDS.BY_GAME:\n",
    "    dataset_path = Path(f'../../dataset/topic_modelling/top_10_games/{training_game_id:02}_{GAMES[training_game_id]}.pkl')\n",
    "\n",
    "# dataset_path = Path('../../dataset/topic_modelling/top_10_games/00_Terraria.pkl')\n",
    "\n",
    "# dataset_path = Path('../dataset_cleaned_heartless_sampled_for_demo.pkl')\n",
    "\n",
    "\n",
    "dataset = pd.read_pickle(dataset_path)\n",
    "\n",
    "dataset.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../sa')\n",
    "\n",
    "%autoreload 2\n",
    "import str_cleaning_functions\n",
    "\n",
    "def cleaning(df, review):\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links2(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.clean(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.deEmojify(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_non_letters(x))\n",
    "    df[review] = df[review].apply(lambda x: x.lower())\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_stopword(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))\n",
    "\n",
    "def cleaning_strlist(str_list):\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.remove_links(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.remove_links2(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.clean(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.deEmojify(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.remove_non_letters(x), str_list))\n",
    "    str_list = list(map(lambda x: x.lower(), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.unify_whitespaces(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.remove_stopword(x), str_list))\n",
    "    str_list = list(map(lambda x: str_cleaning_functions.unify_whitespaces(x), str_list))\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply data preprocessing\n",
    "\n",
    "cleaning(dataset, 'review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['review_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['werewolf riding unicorn shooting rainbows gun build teleporters find hair dresser spider cavern get sword shoots cats take lord moon using yoyo summon sharknado minion shoots sharks enemies find sky temples air wyverns spawn buy music box wizard go record music like playing base whenever want go build castle made entirely white marble would seem thing minecraft game dimension trust get used start learning game terraria simply one satisfying sandbox experiences may sound rude compared minecraft imagination',\n",
       "       'copies game go around giving people look sad', 'introduction',\n",
       "       ...,\n",
       "       'game game start newb get pro hours entertainment even get bored get mods newb',\n",
       "       'far one greatest games played yet',\n",
       "       'game awesome eye cthulhu possible'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# t = nltk.word_tokenize(X[0])\n",
    "# tt = nltk.pos_tag(t)\n",
    "# tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do lemmatization, but not stemming (as part of speech is important in topic modelling)\n",
    "# use nltk wordnet for lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# from https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word\n",
    "\n",
    "# from: https://www.cnblogs.com/jclian91/p/9898511.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None     # if none -> created as noun by wordnet\n",
    "    \n",
    "def lemmatization(text):\n",
    "   # use nltk to get PoS tag\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "    # then we only need adj, adv, verb, noun\n",
    "    # convert from nltk Penn Treebank tag to wordnet tag\n",
    "    wn_tagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1])), tagged))\n",
    "\n",
    "    # lemmatize by the PoS\n",
    "    lemmatized = list(map(lambda x: lemma.lemmatize(x[0], pos=x[1] if x[1] else wordnet.NOUN), wn_tagged))\n",
    "    # lemma.lemmatize(wn_tagged[0], pos=wordnet.NOUN)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the data\n",
    "\n",
    "X_lemmatized = list(map(lambda x: lemmatization(x), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['werewolf',\n",
       " 'rid',\n",
       " 'unicorn',\n",
       " 'shooting',\n",
       " 'rainbow',\n",
       " 'gun',\n",
       " 'build',\n",
       " 'teleporters',\n",
       " 'find',\n",
       " 'hair',\n",
       " 'dresser',\n",
       " 'spider',\n",
       " 'cavern',\n",
       " 'get',\n",
       " 'sword',\n",
       " 'shoot',\n",
       " 'cat',\n",
       " 'take',\n",
       " 'lord',\n",
       " 'moon',\n",
       " 'use',\n",
       " 'yoyo',\n",
       " 'summon',\n",
       " 'sharknado',\n",
       " 'minion',\n",
       " 'shoot',\n",
       " 'sharks',\n",
       " 'enemy',\n",
       " 'find',\n",
       " 'sky',\n",
       " 'temple',\n",
       " 'air',\n",
       " 'wyverns',\n",
       " 'spawn',\n",
       " 'buy',\n",
       " 'music',\n",
       " 'box',\n",
       " 'wizard',\n",
       " 'go',\n",
       " 'record',\n",
       " 'music',\n",
       " 'like',\n",
       " 'play',\n",
       " 'base',\n",
       " 'whenever',\n",
       " 'want',\n",
       " 'go',\n",
       " 'build',\n",
       " 'castle',\n",
       " 'make',\n",
       " 'entirely',\n",
       " 'white',\n",
       " 'marble',\n",
       " 'would',\n",
       " 'seem',\n",
       " 'thing',\n",
       " 'minecraft',\n",
       " 'game',\n",
       " 'dimension',\n",
       " 'trust',\n",
       " 'get',\n",
       " 'use',\n",
       " 'start',\n",
       " 'learning',\n",
       " 'game',\n",
       " 'terrarium',\n",
       " 'simply',\n",
       " 'one',\n",
       " 'satisfy',\n",
       " 'sandbox',\n",
       " 'experience',\n",
       " 'may',\n",
       " 'sound',\n",
       " 'rude',\n",
       " 'compare',\n",
       " 'minecraft',\n",
       " 'imagination']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lemmatized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter empty list of strings in X_lemmatized, as they are not useful for topic modelling\n",
    "\n",
    "X_lemmatized = list(filter(lambda x: len(x) > 0, X_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lematized data, as separate pickle file\n",
    "\n",
    "X_lemmatized_file = dataset_path.parent.joinpath('cleaned_lemmatized', dataset_path.stem + '_cleaned_lemmatized.pkl')\n",
    "\n",
    "if not X_lemmatized_file.parent.exists():\n",
    "    X_lemmatized_file.parent.mkdir()\n",
    "\n",
    "with open(X_lemmatized_file, \"wb\") as f:\n",
    "    pickle.dump(X_lemmatized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the lematized data, as separate pickle file\n",
    "# for convenient hyperparameter selection\n",
    "\n",
    "# X_lemmatized_file = dataset_path.parent.joinpath('cleaned_lemmatized', dataset_path.stem + '_cleaned_lemmatized.pkl')\n",
    "\n",
    "# with open(X_lemmatized_file, \"rb\") as f:\n",
    "#     X_lemmatized = pickle.load(f)\n",
    "\n",
    "# X_lemmatized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim to build a dictionary and train our LDAModel\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(X_lemmatized)\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in X_lemmatized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_LdaMulticore_params(corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, \n",
    "        passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, \n",
    "        eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, \n",
    "        minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=np.float32):\n",
    "    \n",
    "    hyperparameters = dict()\n",
    "    hyperparameters['corpus'] = corpus\n",
    "    hyperparameters[\"num_topics\"] = num_topics\n",
    "    hyperparameters['id2word'] = id2word\n",
    "    hyperparameters[\"workers\"] = workers\n",
    "    hyperparameters[\"chunksize\"] = chunksize\n",
    "    hyperparameters[\"passes\"] = passes\n",
    "    hyperparameters[\"alpha\"] = alpha\n",
    "    hyperparameters[\"eta\"] = eta\n",
    "    hyperparameters[\"decay\"] = decay\n",
    "    hyperparameters[\"offset\"] = offset\n",
    "    hyperparameters[\"eval_every\"] = eval_every\n",
    "    hyperparameters[\"iterations\"] = iterations\n",
    "    hyperparameters[\"gamma_threshold\"] = gamma_threshold\n",
    "    hyperparameters['minimum_probability'] = minimum_probability\n",
    "    hyperparameters[\"random_state\"] = random_state\n",
    "    hyperparameters['minimum_phi_value'] = minimum_phi_value\n",
    "    hyperparameters['per_word_topics'] = per_word_topics\n",
    "    hyperparameters['dtype'] = dtype\n",
    "\n",
    "    if \"alpha\" in hyperparameters:\n",
    "        if isinstance(hyperparameters[\"alpha\"], float):\n",
    "            hyperparameters[\"alpha\"] = [\n",
    "                hyperparameters[\"alpha\"]\n",
    "            ] * hyperparameters[\"num_topics\"]\n",
    "\n",
    "    return hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class METRICS(Enum):\n",
    "    # coherence metrics\n",
    "    UMASS = 'u_mass'\n",
    "    C_V = 'c_v'\n",
    "    C_UCI = 'c_uci'\n",
    "    C_NPMI = 'c_npmi'\n",
    "\n",
    "    # diversity metrics\n",
    "    TOPIC_DIVERSITY = 'topic_diversity'\n",
    "    INVERTED_RBO = 'inverted_rbo'\n",
    "\n",
    "    # similarity metrics\n",
    "    PAIRWISE_JACCARD_SIMILARITY = 'pairwise_jaccard_similarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def _init_config_dict(config_path:Path, model_name:str, hyperparameters:dict, search_space_dict:dict, metrics:list[METRICS], monitor:METRICS):\n",
    "    # init dict for config.json\n",
    "\n",
    "    _hyperparameters = hyperparameters\n",
    "    _search_space_dict = search_space_dict\n",
    "\n",
    "    if not config_path.exists():\n",
    "        config = {}\n",
    "        config['model'] = model_name\n",
    "        config.update(hyperparameters)\n",
    "\n",
    "        config.pop('corpus', '')\n",
    "        config.pop('id2word', '')\n",
    "        \n",
    "        # remove hyperparameters that are in the search space\n",
    "        for key in search_space_dict.keys():\n",
    "            config.pop(key, '')\n",
    "\n",
    "        config['dtype'] = str(config['dtype'])      # datatype is not json serializable, so convert to str\n",
    "\n",
    "        # store the search space\n",
    "        config['search_space'] = search_space_dict\n",
    "\n",
    "        # store the metrics types\n",
    "        config['metrics'] = list(map(lambda x: x.value, metrics))\n",
    "\n",
    "        # store the monitor metric\n",
    "        config['monitor'] = monitor.value\n",
    "\n",
    "        config['gensim_version'] = str(gensim.__version__)\n",
    "\n",
    "        # save the file\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "\n",
    "        print('Created config.json at:', config_path)\n",
    "    else:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # check whether hyperparameters in _hyperparameters are same in _config\n",
    "        # for every key in _hyperparameters, check whether the value is the same in _config\n",
    "        for key in config.keys() & _hyperparameters.keys():\n",
    "            if key == 'dtype':      # edge case for dtype\n",
    "                # convert the _hyperparameters[key] to str for comparison only\n",
    "                if str(_hyperparameters[key]) != config[key]:\n",
    "                    raise Exception(f'hyperparameters {key} is different in config.json. Please modify the hyperparameters passed in.')\n",
    "\n",
    "            else:\n",
    "                if _hyperparameters[key] != config[key]:\n",
    "                    raise Exception(f'hyperparameters {key} is different in config.json. Please modify the hyperparameters passed in.')\n",
    "            \n",
    "        # check search space\n",
    "        if 'search_space' not in config.keys():\n",
    "            raise Exception('search_space is not found in config.json. Please modify the hyperparameters passed in.')\n",
    "        \n",
    "        # check whether search space in _search_space_dict is same in _config\n",
    "        # for every key in _search_space_dict, check whether the value is the same in _config\n",
    "        for key in config['search_space'].keys() & _search_space_dict.keys():\n",
    "            if _search_space_dict[key] != config['search_space'][key]:\n",
    "                raise Exception(f'search_space {key} is different in config.json. Please modify the hyperparameters passed in.')\n",
    "            \n",
    "        # check the metrics and monitor\n",
    "        if 'metrics' not in config.keys():\n",
    "            raise Exception('metrics is not found in config.json. Please modify the hyperparameters passed in.')\n",
    "        \n",
    "        # check whether metrics in _metrics is same in _config\n",
    "        # for every key in _metrics, check whether the value is the same in _config\n",
    "        if config['metrics'] != list(map(lambda x: x.value, metrics)):\n",
    "            raise Exception(f'metrics is different in config.json. Please modify the hyperparameters passed in.')\n",
    "        \n",
    "        # check whether monitor in _monitor is same in _config\n",
    "        # for every key in _monitor, check whether the value is the same in _config\n",
    "        if config['monitor'] != monitor.value:\n",
    "            raise Exception(f'monitor is different in config.json. Please modify the hyperparameters passed in.')\n",
    "\n",
    "\n",
    "        print('Loaded existing config.json from:', config_path)\n",
    "        print('Hyperparameters and search space are checked to be consistent with config.json')\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_result_dict(result_path:Path, monitor_type:str):\n",
    "    # init dict for result.json\n",
    "\n",
    "    if not result_path.exists():\n",
    "        result = {}\n",
    "        result['best_metric'] = -float('inf')\n",
    "        result['best_model_checkpoint'] = \"\"\n",
    "        result['best_hyperparameters'] = dict()\n",
    "        result[\"monitor_type\"] = monitor_type\n",
    "        result[\"log_history\"] = list()\n",
    "    else:\n",
    "        with open(result_path, 'r') as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        # check whether metric_type in result.json is same as metric_type passed in\n",
    "        if 'monitor_type' not in result.keys():\n",
    "            raise Exception('metric_type is not found in result.json. Please modify the metric_type passed in.')\n",
    "        elif result['monitor_type'] != monitor_type:\n",
    "            raise Exception(f'metric_type is different in result.json. Please modify the metric_type passed in.')\n",
    "\n",
    "        print('Loaded existing result.json from:', result_path)\n",
    "        print('metric_type is checked to be consistent with result.json')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topic_diversity(topics, topk=10):\n",
    "    if topic is None:\n",
    "        return 0\n",
    "\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than ' + str(topk))\n",
    "    else:\n",
    "        unique_words = set()\n",
    "        for topic in topics:\n",
    "            unique_words = unique_words.union(set(topic[:topk]))\n",
    "        td = len(unique_words) / (topk * len(topics))\n",
    "        return td\n",
    "    \n",
    "# TODO: include other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from gensim.models import CoherenceModel\n",
    "from copy import deepcopy\n",
    "\n",
    "def grid_search(X, hyperparameters:dict, search_space:dict, save_folder:Path, metrics:list[METRICS]=[METRICS.C_NPMI], monitor:METRICS=METRICS.C_NPMI, save_each_models=True, run_from_checkpoints=False):\n",
    "    \"\"\"\n",
    "    Perform grid search for LDA model hyperparameter selection\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : List of input texts\n",
    "    hyperparameters : dict of hyperparameters\n",
    "    search_space : dict of search space for hyperparameters\n",
    "    save_each_models : save each model or not\n",
    "    save_path : folder to save the model\n",
    "    run_from_checkpoints : whether to run from checkpoints or not\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    best_model : best model\n",
    "    best_model_path : path to the best model\n",
    "    best_hyperparameters : best hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    config_json_path = save_folder.joinpath('config.json')\n",
    "    result_json_path = save_folder.joinpath('result.json')\n",
    "\n",
    "    if monitor not in metrics:\n",
    "        raise Exception('monitor is not in metrics. Please modify the metrics passed in.')\n",
    "\n",
    "    if run_from_checkpoints:\n",
    "        if not save_folder.exists():\n",
    "            print('Save folder:' + str(save_folder.resolve()) + ' does not exist. Function terminates.')\n",
    "            raise Exception('No checkpoints found. Function terminates.')\n",
    "        \n",
    "        # check for existing configs\n",
    "        if not config_json_path.exists():\n",
    "            raise Exception('No config.json found. Function terminates.')\n",
    "        \n",
    "        # check for existing results\n",
    "        if not result_json_path.exists():\n",
    "            print('no result.json is found. Assuming no existing checkpoints.')\n",
    "    else:\n",
    "        if save_folder.exists():\n",
    "            raise Exception('Checkpoints found. Please delete the checkpoints or set run_from_checkpoints=True. Function terminates.')\n",
    "        \n",
    "    if not save_folder.exists():\n",
    "        save_folder.mkdir()\n",
    "\n",
    "    # init / load existing json files\n",
    "    # also doing consistency checks for hyperparameters and search space\n",
    "    config = _init_config_dict(config_json_path, 'lda_multicore', hyperparameters, search_space, metrics, monitor)\n",
    "\n",
    "    result = _init_result_dict(result_json_path, monitor.value)\n",
    "    \n",
    "    print(f'Grid Search folder: {save_folder}')\n",
    "\n",
    "\n",
    "    # init\n",
    "    best_model = None\n",
    "    best_model_path = None\n",
    "    best_metric_score = -float('inf')\n",
    "\n",
    "    # get the search space keys\n",
    "    keys = list(search_space.keys())\n",
    "\n",
    "    # iterate through the search space by using combination func\n",
    "    for values in product(*search_space.values()):\n",
    "\n",
    "        # create a dict of search space hyperparams\n",
    "        search_space_dict = dict(zip(keys, values))\n",
    "\n",
    "        # create the model folder by using search_space keys and current hyperparams values\n",
    "        model_path = save_folder.joinpath(\n",
    "            'lda_multicore_' + '_'.join([f'{key}_{value}' for key, value in search_space_dict.items()])\n",
    "        )\n",
    "\n",
    "        # check whether the current search space is already trained\n",
    "        # by comparing the folder name\n",
    "\n",
    "        if model_path.exists():\n",
    "            print(f'Skipping current search space: {search_space_dict}')\n",
    "            continue\n",
    "\n",
    "\n",
    "        ##########\n",
    "        # Training starts\n",
    "        ##########\n",
    "\n",
    "        print(f'Training with current search space: {search_space_dict}')\n",
    "\n",
    "        # update existing hyperparams dict\n",
    "        hyperparameters.update(search_space_dict)\n",
    "\n",
    "        # train the model\n",
    "        model = gensim.models.ldamulticore.LdaMulticore(**hyperparameters)\n",
    "\n",
    "        ##########\n",
    "        # Training ends\n",
    "        ##########\n",
    "\n",
    "        ##########\n",
    "        # Evaluation starts\n",
    "        ##########\n",
    "\n",
    "        # TODO: reference octis to prepare three variables for evaluation\n",
    "\n",
    "        print('Computing evaluation metrics')\n",
    "\n",
    "        metrics_score = dict()\n",
    "\n",
    "        for metric in metrics:\n",
    "            if metric == METRICS.UMASS:\n",
    "                # compute the coherence\n",
    "                coherencemodel = CoherenceModel(model=model, texts=X, dictionary=id2word, coherence=metric.value)\n",
    "                score = coherencemodel.get_coherence()\n",
    "\n",
    "                metrics_score[metric.value] = score\n",
    "\n",
    "                print(f'Evaluation metric ({metric.value}): {score}')\n",
    "\n",
    "            elif metric == METRICS.C_V:\n",
    "                # compute the coherence\n",
    "                coherencemodel = CoherenceModel(model=model, texts=X, dictionary=id2word, coherence=metric.value)\n",
    "                score = coherencemodel.get_coherence()\n",
    "\n",
    "                metrics_score[metric.value] = score\n",
    "\n",
    "                print(f'Evaluation metric ({metric.value}): {score}')\n",
    "\n",
    "            elif metric == METRICS.C_UCI:\n",
    "                # compute the coherence\n",
    "                coherencemodel = CoherenceModel(model=model, texts=X, dictionary=id2word, coherence=metric.value)\n",
    "                score = coherencemodel.get_coherence()\n",
    "                \n",
    "                metrics_score[metric.value] = score\n",
    "\n",
    "                print(f'Evaluation metric ({metric.value}): {score}')\n",
    "\n",
    "            elif metric == METRICS.C_NPMI:\n",
    "                # compute the coherence\n",
    "                coherencemodel = CoherenceModel(model=model, texts=X, dictionary=id2word, coherence=metric.value)\n",
    "                score = coherencemodel.get_coherence()\n",
    "                \n",
    "                metrics_score[metric.value] = score\n",
    "\n",
    "                print(f'Evaluation metric ({metric.value}): {score}')\n",
    "\n",
    "            # elif metric == METRICS.TOPIC_DIVERSITY:\n",
    "            #     # compute the coherence\n",
    "            #     monitor_score = compute_topic_diversity(, topk=10)\n",
    "\n",
    "            #     print(f'Evaluation metric ({metric.value}): {monitor_score}')\n",
    "\n",
    "            # elif metric == METRICS.INVERTED_RBO:\n",
    "            #     # compute the coherence\n",
    "            #     monitor_score = model.inverted_rbo(X=X, topn=10)\n",
    "\n",
    "            #     print(f'Evaluation metric ({metric.value}): {monitor_score}')\n",
    "\n",
    "            # elif metric == METRICS.PAIRWISE_JACCARD_SIMILARITY:\n",
    "            #     # compute the coherence\n",
    "            #     monitor_score = model.pairwise_jaccard_similarity(X=X, topn=10)\n",
    "\n",
    "            #     print(f'Evaluation metric ({metric.value}): {monitor_score}')\n",
    "\n",
    "            else:\n",
    "                raise Exception(f'Unknown metric: {metric.value}')\n",
    "            \n",
    "        # get the monitor score\n",
    "        monitor_score = metrics_score[monitor.value]\n",
    "\n",
    "        ##########\n",
    "        # Evaluation ends\n",
    "        ##########\n",
    "\n",
    "        ##########\n",
    "        # Save models\n",
    "        ##########\n",
    "\n",
    "        if not model_path.exists():\n",
    "            model_path.mkdir(parents=True)\n",
    "\n",
    "        # save the model\n",
    "        if save_each_models:\n",
    "            model.save(str(model_path.joinpath('lda_multicore')))\n",
    "\n",
    "        ##########\n",
    "        # Save models ends\n",
    "        ##########\n",
    "            \n",
    "        model_hyperparameters = deepcopy(hyperparameters)\n",
    "        model_hyperparameters.pop('corpus', '')     # pop as it is not json serializable\n",
    "        model_hyperparameters.pop('id2word', '')    # pop as it is not json serializable\n",
    "        model_hyperparameters['dtype'] = str(model_hyperparameters['dtype'])\n",
    "            \n",
    "        if monitor_score > best_metric_score:\n",
    "            best_metric_score = monitor_score\n",
    "            best_model = model\n",
    "            best_model_path = model_path\n",
    "            best_hyperparameters = model_hyperparameters\n",
    "            \n",
    "        ###########\n",
    "        # Update result dict and json file\n",
    "        ###########\n",
    "            \n",
    "        model_log_history = dict()\n",
    "        model_log_history.update(metrics_score)         # add the metrics score values to the log history\n",
    "        model_log_history['hyperparameters'] = model_hyperparameters\n",
    "\n",
    "        result['best_metric'] = best_metric_score\n",
    "        result['best_model_checkpoint'] = str(best_model_path)      # relative path\n",
    "        result['best_hyperparameters'] = model_hyperparameters\n",
    "        result[\"log_history\"].append(model_log_history)\n",
    "\n",
    "        # print(result)\n",
    "\n",
    "        # save result\n",
    "        with open(result_json_path, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        print(\"Saved result.json at:\", result_json_path)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    print('Grid Search ends')\n",
    "    return best_model, best_model_path, best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created config.json at: lda_multicore_grid_search_20240118_160314/config.json\n",
      "Grid Search folder: lda_multicore_grid_search_20240118_160314\n",
      "Training with current search space: {'num_topics': 10}\n",
      "Computing evaluation metrics\n",
      "Evaluation metric (c_npmi): 0.008000086560907694\n",
      "Evaluation metric (c_v): 0.5306057116838987\n",
      "Evaluation metric (u_mass): -3.1910666342541076\n",
      "Saved result.json at: lda_multicore_grid_search_20240118_160314/result.json\n",
      "\n",
      "\n",
      "\n",
      "Grid Search ends\n"
     ]
    }
   ],
   "source": [
    "# grid search code\n",
    "\n",
    "hyperparameters = _init_LdaMulticore_params(\n",
    "    corpus=corpus, num_topics=10, id2word=id2word, \n",
    "    workers=3, chunksize=2000, random_state=42, passes=10)\n",
    "\n",
    "# create search_space dict\n",
    "search_space = dict()\n",
    "\n",
    "search_space['num_topics'] = [10]\n",
    "# search_space['decay'] = [0.7, 0.8, 0.9]\n",
    "# search_space['offset'] = [16, 64, 128]\n",
    "\n",
    "\n",
    "training_datetime = datetime.now()\n",
    "# training_datetime = datetime(2024, 1, 18, 15, 51, 58)\n",
    "\n",
    "training_folder = Path(f'lda_multicore_grid_search_{training_datetime.strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "\n",
    "best_model, best_model_path, best_hyperparameters = grid_search(\n",
    "    X_lemmatized, hyperparameters, search_space, \n",
    "    training_folder,\n",
    "    metrics=[METRICS.C_NPMI, METRICS.C_V, METRICS.UMASS],\n",
    "    monitor=METRICS.C_NPMI,\n",
    "    run_from_checkpoints=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_TOPICS = 20\n",
    "\n",
    "# # Online LDA, how to effective train LDA models\n",
    "# # https://papers.nips.cc/paper_files/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html\n",
    "\n",
    "# lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "#                                              id2word=id2word,\n",
    "#                                              num_topics=N_TOPICS,         # later can use grid search to find the best number of topics\n",
    "#                                              random_state=42,\n",
    "#                                              chunksize=2048,                # chunk size affects memory consumption, and updating speed (like DL batch_size). https://groups.google.com/g/gensim/c/FE7_FYSconA\n",
    "#                                              passes=10,                     # no. of passes over the whole corpus. If larger chunksize, then the passes should be larger too.\n",
    "#                                             #  alpha='auto',\n",
    "#                                              workers=3)     # workers = no. of cores (physical cores, but not logical threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the best model from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from training folder\n",
    "\n",
    "training_folder = Path(f'lda_multicore_grid_search_{training_datetime.strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "training_result_json_path = training_folder.joinpath('result.json')\n",
    "with open(training_result_json_path, 'r') as f:\n",
    "    training_result = json.load(f)\n",
    "\n",
    "best_model_checkpoint_path = Path(training_result['best_model_checkpoint'])\n",
    "\n",
    "best_model = gensim.models.ldamulticore.LdaMulticore.load(str(best_model_checkpoint_path.joinpath('lda_multicore')))\n",
    "\n",
    "lda_model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelcheng/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/sklearn/manifold/_mds.py:298: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el4604312013216480178881179\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el4604312013216480178881179_data = {\"mdsDat\": {\"x\": [-0.07575043723053833, 0.07577084813961839, -0.09300427360298408, 0.028419492320921502, -0.24733064318136722, -0.2899100954014118, 0.0300108738600076, -0.20033954492108852, 0.3254900829278541, 0.44664369708898843], \"y\": [0.021130806156669974, 0.17180298810022726, 0.2074224386790717, 0.3491413575803308, 0.22447463352080688, -0.11125005109626825, -0.2731527810943686, -0.3388488285083116, -0.2720434879786517, 0.021322924640493318], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [28.269223462334846, 15.823430046383432, 13.109806558717487, 12.133336032092414, 11.864811078491758, 9.082660943897476, 3.5949734128369735, 2.8104456040804258, 1.93482537945674, 1.3764874817084494]}, \"tinfo\": {\"Term\": [\"game\", \"minecraft\", \"play\", \"world\", \"best\", \"fun\", \"like\", \"get\", \"cool\", \"good\", \"variety\", \"progression\", \"generate\", \"environment\", \"customization\", \"biomes\", \"structure\", \"equipment\", \"housing\", \"vast\", \"system\", \"resource\", \"material\", \"limit\", \"certain\", \"craft\", \"unique\", \"enemy\", \"combat\", \"explore\", \"weapon\", \"build\", \"building\", \"item\", \"world\", \"boss\", \"different\", \"fight\", \"many\", \"game\", \"terrarium\", \"devs\", \"award\", \"penny\", \"otherworld\", \"dlc\", \"dev\", \"nominate\", \"dollar\", \"cent\", \"positive\", \"update\", \"developer\", \"release\", \"ago\", \"year\", \"patch\", \"sale\", \"come\", \"back\", \"review\", \"content\", \"since\", \"new\", \"buy\", \"still\", \"game\", \"time\", \"terrarium\", \"hour\", \"play\", \"boring\", \"bored\", \"tickle\", \"pickle\", \"freind\", \"timesink\", \"blah\", \"adicting\", \"finaly\", \"killing\", \"bore\", \"cancer\", \"tired\", \"controller\", \"youre\", \"good\", \"get\", \"playing\", \"really\", \"stop\", \"start\", \"never\", \"play\", \"fun\", \"game\", \"time\", \"friend\", \"hour\", \"funnest\", \"adictive\", \"itch\", \"especialy\", \"bla\", \"sibling\", \"thx\", \"dec\", \"mutiplayer\", \"gb\", \"best\", \"ever\", \"co\", \"friend\", \"friends\", \"great\", \"play\", \"one\", \"ive\", \"highly\", \"favorite\", \"fun\", \"game\", \"hour\", \"recommend\", \"soooo\", \"chinese\", \"meme\", \"recommed\", \"verry\", \"dank\", \"iz\", \"mutch\", \"luv\", \"steroid\", \"mincraft\", \"ripoff\", \"minecraft\", \"rip\", \"zelda\", \"like\", \"awesome\", \"love\", \"people\", \"starbound\", \"well\", \"game\", \"much\", \"terrarium\", \"say\", \"good\", \"fun\", \"unicorn\", \"eyeball\", \"worm\", \"demon\", \"rainbow\", \"slime\", \"cat\", \"shoot\", \"hole\", \"rid\", \"zombie\", \"bunny\", \"giant\", \"tree\", \"eye\", \"flesh\", \"fly\", \"wall\", \"sword\", \"kill\", \"die\", \"house\", \"get\", \"go\", \"would\", \"make\", \"fight\", \"build\", \"expert\", \"mage\", \"summoner\", \"zone\", \"n\", \"ranger\", \"warrior\", \"l\", \"v\", \"ing\", \"holy\", \"mode\", \"melee\", \"class\", \"normal\", \"let\", \"magic\", \"use\", \"dont\", \"hard\", \"weapon\", \"make\", \"click\", \"fix\", \"crash\", \"delete\", \"plz\", \"window\", \"glitch\", \"gud\", \"mac\", \"file\", \"please\", \"lag\", \"save\", \"computer\", \"help\", \"wont\", \"cant\", \"open\", \"world\", \"work\", \"need\", \"ok\", \"ign\", \"yo\", \"alright\", \"k\", \"h\", \"le\", \"gr\", \"cough\", \"cool\", \"dis\", \"nice\", \"r\", \"u\", \"guess\", \"life\", \"step\", \"pretty\", \"game\", \"awsome\", \"cup\", \"ti\", \"sugar\", \"pour\", \"crust\", \"egg\", \"nuff\", \"degree\", \"pie\", \"simulator\", \"badge\", \"w\", \"c\", \"minute\", \"dig\", \"f\"], \"Freq\": [112445.0, 21877.0, 31842.0, 10031.0, 10144.0, 21612.0, 20270.0, 24176.0, 3854.0, 14443.0, 1297.1946827254592, 1208.252499898613, 735.5438978650182, 355.9124006055345, 298.91368795494475, 305.4423421441327, 492.4692679620401, 669.088201695812, 159.21674983498076, 398.8934834249607, 1471.6641807300423, 651.5566513859084, 1017.9082916142361, 439.2535888793135, 648.6854535511565, 4482.972669946385, 1490.887260908019, 2669.808061846143, 1774.6569666776218, 4161.095803591204, 4524.316552520888, 6190.293352260348, 2844.659148068323, 6609.908890446171, 7180.050146570589, 7706.276843020479, 3605.684386355827, 4930.562947369535, 4685.309849011443, 16299.301941813095, 6001.310373217651, 755.053559962167, 524.3476192371661, 293.6619920038129, 286.06928679044205, 250.29571942350125, 185.35435293673456, 180.6738705424774, 609.1159991193738, 152.28788693243928, 232.62216018857174, 6940.331454551184, 1055.868051866234, 1176.3235920939255, 753.2276757908643, 2960.877813799207, 573.0554290009558, 1763.2162131318578, 3561.125958139812, 2518.9102455365614, 1997.569381536625, 4066.71370936702, 1691.1312787066054, 4348.807884824278, 4211.085757204982, 3514.607584302254, 19273.159832129986, 4162.152228701649, 4243.712570542624, 3658.449330517086, 3958.513537632592, 1089.3031970582394, 441.3243416135453, 173.10375100996148, 171.15110484801974, 61.22293346626114, 48.907183734439776, 46.95770245044365, 44.1639041236599, 42.68791085512862, 42.61534450410971, 1624.3843036789328, 79.85409659258227, 126.7957395304006, 214.98825930014291, 75.7635026759025, 8722.642502979654, 13818.27741129801, 1216.7289174545458, 5439.010565414257, 1304.7268544644958, 2853.961326229298, 2095.6409780992926, 9027.859149342212, 6708.469876286214, 19555.336295183177, 4327.076269752233, 3285.031587047942, 2850.649211850459, 76.47934111023933, 54.112038657099625, 53.672306095056115, 36.55223951129258, 34.458888492187974, 34.15414285456612, 33.834213192249535, 33.866831787256565, 31.150108264976872, 30.538944828612706, 9664.765090220073, 5636.655210686636, 429.71159823080166, 5903.709145525204, 1100.7800922033127, 7754.98740067802, 15251.796789370059, 6323.558841406591, 649.1305646996789, 1406.7280365770891, 1283.9182933734858, 8846.8864390138, 31874.932088989382, 5686.115262747184, 2605.2316069161266, 143.19302101009296, 119.03389719313536, 85.63120729179529, 59.246380969410254, 57.21054674981181, 51.97838359138103, 48.7801731052108, 46.30780310369177, 42.7782862903628, 42.56668781273414, 180.2368130016325, 330.37648438384724, 17795.505561710957, 533.529591987641, 179.66786892565108, 11916.060775866834, 4138.576979312235, 5292.2841061221825, 2550.450497745597, 580.7397650351496, 4122.786675662682, 23435.641896693203, 4224.348300282428, 5109.158658769738, 2484.353352520335, 3790.639095249552, 3756.7812780866434, 644.3443957682779, 409.6046460205291, 402.3970419098856, 395.91732793274485, 364.69641398035026, 1094.978108106905, 333.2717250845741, 881.5872645192948, 405.2185464019377, 284.61980576624387, 881.0779300428385, 659.6862964365619, 1236.2284378695417, 618.6153914394121, 1236.3536795547031, 985.4832671565472, 1055.7240306005779, 1335.4387828049921, 1274.6281146792335, 3883.8928069336407, 1918.7447297046738, 1471.4992644892538, 3064.7485363151086, 1954.9676977956997, 1721.1716503185623, 1728.6177006058015, 1548.8895723032438, 1311.1856550266764, 1746.7838605242382, 345.497530085547, 298.93380068106245, 245.73833098278598, 213.36418025832754, 212.54945500726018, 181.30551126199057, 136.56274478120798, 264.11391321903164, 92.68215068946057, 120.27128028593863, 2524.452374248287, 421.8050972554653, 830.2207492715539, 522.8537508907376, 742.7777168502759, 477.5768970011091, 826.2089379972801, 552.1415640396165, 615.0405469444008, 499.68928578691725, 481.29377724584054, 810.666185398826, 800.8997079676146, 400.88887643619995, 289.26919004990043, 257.6283754207325, 254.7435526464492, 244.83770267125345, 231.65688442519055, 698.817798154604, 189.960278139199, 1080.8659770208567, 358.6390246078623, 416.5590024984687, 708.4040059971745, 989.2706891742301, 429.97493099917756, 692.9406784834948, 759.5009476207548, 2008.8906783146726, 836.3053003609674, 830.6927911479667, 953.9449461247754, 413.6650747319827, 399.55737951426295, 359.2656465125387, 253.29008242452514, 180.33733136349002, 163.46391071891682, 152.30792300314306, 144.83371996177374, 3826.968690560007, 194.92493601938165, 1762.5576631810063, 252.62476073614317, 1366.2073457704957, 489.12797745464866, 1109.1373080719472, 341.37913567056063, 670.035688914569, 1180.7572241220553, 1101.6541431403027, 224.2160305313937, 176.31082402177498, 164.2434138278481, 162.54121683565813, 157.86265421644725, 149.9443998646014, 137.36112704211038, 130.27259991279766, 121.18799999064281, 229.48057767189647, 217.90792909226784, 168.20839835428274, 264.8742963858813, 250.78162539514256, 261.73679385090514, 188.07144833637082], \"Total\": [112445.0, 21877.0, 31842.0, 10031.0, 10144.0, 21612.0, 20270.0, 24176.0, 3854.0, 14443.0, 1298.1869264964625, 1209.2667304766483, 736.5348843198602, 356.90314791640003, 299.90592621605214, 306.4622395905636, 494.1413943541128, 672.2267194627311, 160.20770916218436, 401.4260275087295, 1484.3885118350167, 657.597483951395, 1042.4375291669185, 442.90731079548135, 664.749904029612, 5037.272003616082, 1593.9310102232992, 2958.843839770703, 1928.5561357342924, 4908.241042870188, 5505.5395687865675, 8032.860758153117, 3329.8875481805358, 8787.234362001722, 10031.867703707952, 11040.140410709739, 4463.7287840860145, 6835.663647847058, 8289.277594575415, 112445.7013027557, 17824.433691042672, 756.0556573323058, 525.4888724807982, 294.663742707524, 287.0710398545756, 251.2975165565579, 186.35624668682703, 181.67975525922793, 612.9900978301641, 153.28963199444053, 234.17748782134314, 7076.954015749123, 1074.7726246802256, 1199.7649462035529, 772.7986651713967, 3196.4079310313873, 595.60786103086, 1962.449164601782, 4910.010360295465, 3300.144422355715, 2634.071804563215, 6366.2642523576815, 2269.1052861509866, 8341.325477644783, 8361.75159316621, 6805.589235538255, 112445.7013027557, 13219.952154689086, 17824.433691042672, 13440.269156739938, 31842.00108492132, 1090.7888151713935, 442.39026248126237, 174.10345176296602, 172.1508029609467, 62.22843365190274, 49.90703394222782, 47.95750099705528, 45.16370864125365, 43.687749314035344, 43.615310278787014, 1680.449327248649, 82.23021234336889, 134.6022940131159, 233.51936200396298, 79.08356444007262, 14443.329151133687, 24176.49750844711, 1712.5532926336687, 9401.960161445035, 1865.4909821473775, 5259.830043988621, 3713.9803971449933, 31842.00108492132, 21612.474228452385, 112445.7013027557, 13219.952154689086, 10409.0306710002, 13440.269156739938, 77.4846265719645, 55.11680880204836, 54.67728873154885, 37.557199464680956, 35.46372377560488, 35.15921511750208, 34.83928988450579, 34.873052257325384, 32.15495288617082, 31.543794698722827, 10144.158669425626, 6489.043906699265, 541.9691409228194, 10409.0306710002, 1638.027936189899, 14402.696864896827, 31842.00108492132, 12231.992130234421, 921.9951949376743, 2301.7430857898407, 2077.562753168626, 21612.474228452385, 112445.7013027557, 13440.269156739938, 6356.9270974284655, 144.18776292191407, 120.02720506858793, 86.62450194494396, 60.2653770407979, 58.20372659576539, 52.97163944149001, 49.773508318621055, 47.30107404648224, 43.77149609669766, 43.55985518374764, 187.52430736182262, 354.3557999382761, 21877.092793916436, 626.6658929277165, 200.86901912555754, 20270.64636736495, 6356.959638501867, 9851.259953944884, 4759.881149385958, 849.8834468743237, 9966.1954600338, 112445.7013027557, 11922.912346713265, 17824.433691042672, 6828.329111643504, 14443.329151133687, 21612.474228452385, 645.3322440554259, 410.58936423081354, 403.3817752566771, 396.90204891494653, 365.6812347707926, 1098.057191320798, 334.25645435406904, 884.3746091976486, 406.6172585496791, 285.6048236660793, 884.3451634022922, 663.2187068687565, 1259.6671470320878, 624.9728886138223, 1274.6347697941335, 1024.7199139629845, 1104.2950264907656, 1448.2742206997843, 1401.0640525202252, 5102.144745349285, 2315.51377717753, 2510.8512666484485, 24176.49750844711, 7793.643417650886, 9263.719564824407, 11029.2604967887, 6835.663647847058, 8032.860758153117, 1750.4491696566708, 346.48671823249526, 299.9228625234361, 246.74693126622108, 214.35445602585108, 213.53845143359723, 182.30307799300323, 137.55211164842214, 266.7434196596323, 93.6712876652535, 121.79059620377907, 3102.9024386239494, 469.91771683000286, 1047.5186224280615, 651.8253578241768, 2034.0746586479177, 1003.80090455925, 3453.118828218233, 2122.5927567306007, 3382.78501627332, 5505.5395687865675, 11029.2604967887, 811.6542440680618, 802.3049963684388, 401.87699948855254, 290.25729355853986, 258.6198881368909, 255.75775999470412, 245.82581119413197, 232.64496046283978, 702.2273195709224, 190.95041144445372, 1211.7939799183578, 378.6525604549708, 532.6136201515378, 1166.6391031824169, 2011.7982374086605, 568.0100822474092, 1298.078052361054, 1573.7322969035397, 10031.867703707952, 2279.9932123646986, 3771.209109288296, 954.9233578834157, 414.6434587418707, 400.53624750912553, 360.2440086874248, 254.2686175387723, 181.31584479688615, 164.4422743024588, 153.28628155521469, 145.83220421084047, 3854.689831279685, 197.76446903112483, 2373.9911037253146, 272.40782887567514, 2015.6556640338654, 594.6945050736913, 2340.7084005435663, 467.096213321054, 3301.5256206849162, 112445.7013027557, 1102.6191028027927, 225.1811107181719, 177.2757703087436, 165.20833659491495, 163.50623558206132, 158.82757176981778, 150.90938551572256, 138.32609748376962, 131.23777388635318, 122.15293343738094, 232.03553676172913, 222.1194245524916, 170.7407395323419, 293.6674418455158, 1023.6860930974769, 1836.4222126704246, 418.06455671222284], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.8676, -5.9386, -6.435, -7.1609, -7.3354, -7.3138, -6.8361, -6.5297, -7.9653, -7.0469, -5.7414, -6.5562, -6.1101, -6.9505, -6.5606, -4.6275, -5.7284, -5.1458, -5.5542, -4.702, -4.6184, -4.3048, -5.0824, -4.2393, -4.1565, -4.0858, -4.8453, -4.5324, -4.5834, -3.3367, -4.3358, -5.8285, -6.1931, -6.7729, -6.7991, -6.9327, -7.233, -7.2586, -6.0433, -7.4295, -7.0059, -3.6102, -5.4932, -5.3851, -5.8309, -4.4621, -6.1043, -4.9804, -4.2775, -4.6237, -4.8556, -4.1447, -5.0221, -4.0776, -4.1098, -4.2906, -2.5888, -4.1215, -4.1021, -4.2505, -4.1717, -5.2739, -6.1774, -7.1133, -7.1246, -8.1526, -8.3772, -8.4179, -8.4793, -8.5132, -8.5149, -4.8743, -7.887, -7.4246, -6.8966, -7.9395, -3.1935, -2.7334, -5.1632, -3.6658, -5.0934, -4.3107, -4.6195, -3.1591, -3.456, -2.3862, -3.8945, -4.17, -4.3119, -7.8527, -8.1987, -8.2069, -8.591, -8.65, -8.6589, -8.6683, -8.6673, -8.7509, -8.7708, -3.0135, -3.5527, -6.1266, -3.5064, -5.186, -3.2337, -2.5573, -3.4377, -5.7141, -4.9407, -5.0321, -3.1019, -1.8202, -3.544, -4.3245, -7.2032, -7.388, -7.7173, -8.0857, -8.1206, -8.2166, -8.2801, -8.3321, -8.4113, -8.4163, -6.9731, -6.3671, -2.3807, -5.8879, -6.9763, -2.7817, -3.8393, -3.5934, -4.3234, -5.8031, -3.8431, -2.1054, -3.8188, -3.6286, -4.3496, -3.9271, -3.9361, -5.4319, -5.885, -5.9027, -5.919, -6.0011, -4.9017, -6.0912, -5.1184, -5.8957, -6.249, -5.119, -5.4084, -4.7803, -5.4727, -4.7802, -5.007, -4.9382, -4.7032, -4.7498, -3.6356, -4.3407, -4.6061, -3.8724, -4.322, -4.4494, -4.4451, -4.5549, -4.7215, -3.5078, -5.1284, -5.2731, -5.4691, -5.6103, -5.6142, -5.7732, -6.0566, -5.397, -6.4442, -6.1836, -3.1396, -4.9288, -4.2516, -4.714, -4.3629, -4.8046, -4.2565, -4.6595, -4.5517, -4.7594, -4.7969, -4.0293, -4.0414, -4.7335, -5.0598, -5.1756, -5.1869, -5.2265, -5.2819, -4.1778, -5.4803, -3.7416, -4.8448, -4.6951, -4.1641, -3.8302, -4.6634, -4.1862, -4.0945, -3.1218, -3.9982, -4.0049, -3.4932, -4.3288, -4.3635, -4.4698, -4.8193, -5.159, -5.2572, -5.3279, -5.3782, -2.104, -5.0812, -2.8793, -4.8219, -3.134, -4.1612, -3.3425, -4.5208, -3.8465, -3.2799, -3.0088, -4.6007, -4.8411, -4.912, -4.9224, -4.9516, -5.0031, -5.0907, -5.1437, -5.216, -4.5775, -4.6293, -4.8881, -4.4341, -4.4888, -4.446, -4.7765], \"loglift\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2626, 1.2626, 1.2621, 1.2606, 1.2601, 1.2601, 1.26, 1.2587, 1.2572, 1.2571, 1.2548, 1.2542, 1.2396, 1.2551, 1.2389, 1.1468, 1.1966, 1.1606, 1.1802, 1.0983, 1.0671, 1.0028, 1.1059, 0.9787, 0.9289, 0.9039, 1.0499, 0.9367, 0.6929, -0.668, 0.1748, 1.8424, 1.8415, 1.8403, 1.8402, 1.8397, 1.8383, 1.8381, 1.8373, 1.8371, 1.837, 1.8242, 1.8259, 1.8239, 1.818, 1.7671, 1.8051, 1.7366, 1.5225, 1.5735, 1.5671, 1.3955, 1.5497, 1.1924, 1.1577, 1.1829, 0.0799, 0.688, 0.4085, 0.5425, -0.2412, 2.0304, 2.0294, 2.0261, 2.026, 2.0155, 2.0116, 2.0107, 2.0094, 2.0087, 2.0086, 1.9979, 2.0025, 1.9721, 1.9491, 1.9889, 1.5275, 1.4724, 1.69, 1.4845, 1.6743, 1.4204, 1.4596, 0.7713, 0.8619, 0.2826, 0.915, 0.8785, 0.4811, 2.0962, 2.0908, 2.0907, 2.0821, 2.0805, 2.0802, 2.0799, 2.0799, 2.0775, 2.0768, 2.0608, 1.9684, 1.8771, 1.5421, 1.7117, 1.4901, 1.3731, 1.4494, 1.7583, 1.6168, 1.6279, 1.216, 0.8486, 1.249, 1.2172, 2.1247, 2.1233, 2.1201, 2.1145, 2.1144, 2.1127, 2.1114, 2.1104, 2.1086, 2.1085, 2.092, 2.0615, 1.9251, 1.9707, 2.0201, 1.6003, 1.7024, 1.5102, 1.5076, 1.7508, 1.2489, 0.5634, 1.094, 0.8821, 1.1205, 0.7939, 0.3819, 2.3973, 2.3964, 2.3964, 2.3963, 2.3961, 2.396, 2.3959, 2.3956, 2.3954, 2.3953, 2.3951, 2.3935, 2.38, 2.3886, 2.3683, 2.3598, 2.3538, 2.3177, 2.3042, 2.126, 2.2108, 1.8645, 0.3334, 1.0159, 0.7157, 0.5456, 0.9142, 0.5862, 3.3235, 3.3228, 3.3223, 3.3215, 3.321, 3.321, 3.3201, 3.3184, 3.3157, 3.315, 3.3131, 3.1193, 3.2176, 3.0931, 3.1052, 2.3182, 2.5828, 1.8954, 1.979, 1.6209, 0.9261, 0.1938, 3.5706, 3.5701, 3.5694, 3.5684, 3.568, 3.5679, 3.5678, 3.5676, 3.567, 3.5666, 3.4575, 3.5175, 3.3261, 3.073, 2.862, 3.2934, 2.9441, 2.8433, 1.9636, 2.5689, 2.0589, 3.9441, 3.9428, 3.9427, 3.9424, 3.9413, 3.9397, 3.9392, 3.9388, 3.9383, 3.9379, 3.9307, 3.6473, 3.8698, 3.5562, 3.7497, 3.1983, 3.6316, 2.3503, -0.6112, 4.2848, 4.2813, 4.2802, 4.2798, 4.2797, 4.2795, 4.2792, 4.2786, 4.2783, 4.2777, 4.2746, 4.2665, 4.2707, 4.1824, 2.8791, 2.3374, 3.4868]}, \"token.table\": {\"Topic\": [3, 4, 2, 7, 9, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 6, 7, 8, 2, 10, 1, 2, 4, 6, 7, 1, 4, 3, 1, 2, 3, 4, 5, 3, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 4, 5, 6, 1, 4, 5, 6, 6, 9, 1, 2, 3, 4, 5, 6, 7, 9, 7, 10, 3, 6, 2, 3, 5, 8, 6, 2, 1, 6, 7, 5, 1, 7, 8, 1, 4, 1, 5, 1, 2, 3, 4, 6, 7, 8, 10, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 1, 3, 4, 1, 9, 10, 9, 1, 2, 4, 5, 6, 7, 8, 10, 10, 1, 5, 4, 10, 8, 6, 2, 1, 2, 2, 1, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 6, 7, 10, 5, 9, 2, 2, 3, 4, 2, 3, 5, 6, 7, 8, 10, 1, 6, 7, 1, 1, 6, 4, 1, 2, 4, 6, 7, 3, 7, 1, 2, 3, 4, 5, 6, 1, 6, 7, 8, 6, 7, 10, 1, 2, 4, 5, 6, 1, 3, 4, 5, 6, 8, 3, 2, 8, 1, 6, 7, 1, 6, 7, 3, 1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 4, 6, 1, 2, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 6, 7, 9, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 6, 7, 8, 1, 2, 4, 5, 6, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 7, 1, 9, 7, 4, 1, 2, 3, 4, 5, 6, 7, 8, 3, 4, 8, 5, 9, 1, 3, 6, 8, 3, 7, 3, 4, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 3, 4, 5, 6, 8, 5, 5, 8, 7, 1, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 6, 7, 1, 7, 5, 1, 5, 1, 2, 5, 6, 1, 2, 3, 4, 6, 7, 8, 10, 1, 3, 7, 1, 2, 3, 4, 5, 6, 7, 5, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 8, 1, 3, 4, 5, 9, 2, 1, 3, 7, 8, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 4, 5, 6, 8, 2, 1, 2, 2, 1, 2, 3, 4, 5, 6, 7, 8, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 2, 3, 7, 8, 8, 1, 2, 10, 1, 2, 3, 5, 6, 9, 1, 7, 9, 6, 7, 1, 2, 3, 4, 5, 6, 7, 5, 1, 2, 3, 4, 5, 6, 1, 2, 6, 1, 7, 1, 2, 3, 6, 7, 10, 6, 1, 2, 5, 6, 1, 2, 5, 6, 1, 2, 3, 4, 1, 2, 3, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 6, 9, 4, 6, 10, 1, 2, 3, 4, 6, 6, 7, 5, 1, 2, 5, 1, 2, 3, 4, 6, 7, 8, 1, 6, 7, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 6, 10, 7, 1, 6, 7, 1, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 4, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 1, 2, 3, 1, 6, 7, 2, 3, 5, 6, 7, 8, 9, 6, 1, 2, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 10, 7, 9, 1, 1, 4, 5, 5, 10, 1, 6, 7, 8, 7, 1, 3, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 8, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 7, 9, 3, 5, 1, 5, 6, 1, 6, 7], \"Freq\": [0.9742335455554089, 0.9797374190138733, 0.974380564998769, 0.02329196569718173, 0.0012939980942878737, 0.996546760924748, 0.997166690754517, 0.0248546489178647, 0.06339508553100934, 0.06512547248098725, 0.18672448269307215, 0.6510974169053291, 0.008337318940802717, 0.00015730790454344747, 0.00015730790454344747, 0.9994385161646312, 0.008787494208904702, 0.7632999280079636, 0.07757236267171047, 0.12393397004972494, 0.00636335787541375, 0.019696107709613987, 0.013506247848625393, 0.9814540103334451, 0.017842780845446736, 0.018237096444241138, 0.9527650655869763, 0.008182048674983863, 0.0029573669909580226, 0.9952286467901651, 0.9587261680452248, 0.9800343850878704, 0.028563788994809505, 0.0005950789373918647, 0.9664081943243883, 0.003570473624351188, 0.0005950789373918647, 0.9968573845331389, 0.0009167677428401867, 0.9983600719529633, 0.6979983689813057, 0.0003623142325363642, 0.05697391306634327, 0.0018115711626818212, 0.1711934748734321, 0.06530714041467965, 0.006159341953118192, 9.057855813409105e-05, 0.7705847501112643, 0.0017428411149527786, 0.0008714205574763893, 0.06348921204470837, 0.1632046215502209, 0.8543832062901102, 0.0006006208831564922, 0.13363814650231953, 0.011111486338395106, 0.9951468394431259, 0.004523394724741481, 0.0029898041961006947, 0.5036026187912009, 0.06960264168522418, 0.22997573876406543, 0.14111875805595278, 0.03599724252105236, 0.00023918433568805556, 0.016503719162475835, 0.09534594582238178, 0.9023812729618276, 0.972878431420605, 0.012160980392757563, 0.043140703209751115, 0.39134780768845656, 0.030814788006965084, 0.53386620222067, 0.9962410468438162, 0.9915869587677835, 0.9763070232366511, 0.022564877270492705, 0.0015043251513661804, 0.9914418979597088, 0.2062015847501879, 0.7923486821419258, 0.9991939374766493, 0.20480870887039537, 0.7934031064348649, 0.920377668614854, 0.07933396242144938, 0.1160893680814351, 0.7252530521719129, 0.025661860312738285, 0.0008146622321504218, 0.1126270535947958, 0.004073311160752108, 0.0008146622321504218, 0.014460254620669986, 0.1508607070686199, 0.1268601400349758, 0.10114524678464287, 0.0025714893250332934, 0.010285957300133174, 0.6068714807078572, 0.19948904878236376, 0.6388361900770656, 0.022462152736911822, 0.12801856280128068, 0.01130961536403952, 0.038540701390950456, 0.920694533228261, 0.038540701390950456, 0.000518848490420833, 0.9928165864202639, 0.006485606130260412, 0.994293412656389, 0.8899658380134745, 0.00019852015124101596, 0.0007940806049640638, 0.07206281490048878, 0.03374842571097271, 0.0031763224198562553, 0.9978177415237283, 0.9947894955479321, 0.9947548410503662, 0.9969792987171601, 0.9816573651158514, 0.9749648453228813, 0.9905684632579561, 0.9956683480951486, 0.9977272757411745, 0.9927222901784118, 0.016747728390789165, 0.9825333989262975, 0.9986037306617999, 0.01857039263761772, 0.1010574855163383, 0.8287577551532187, 0.051392481950616485, 0.8078447805467103, 0.020834599165514157, 0.00044805589603256254, 0.0008961117920651251, 0.12276731551292212, 0.00022402794801628127, 0.047269897031435346, 0.4998850447714279, 0.3283558627420164, 0.028860465547805754, 0.1426687164816058, 0.010113040071344835, 0.9860214069561214, 0.9948367314793345, 0.9934907629922766, 0.0032626954449664256, 0.0016313477224832128, 0.004240097386303424, 0.4927935406748202, 0.22048506408777804, 0.007066828977172373, 0.26005930635994334, 0.015075901817967731, 0.9939739631659436, 0.902379491648641, 0.07401539650601212, 0.02331991944709971, 0.9974694873898631, 0.9951999535732379, 0.0029751867072443585, 0.9851639772767149, 0.006934765837158563, 0.08445003730584205, 0.8686950005347294, 0.03251634648089904, 0.007397083559635801, 0.0017138458242625882, 0.9980295516622472, 0.8477578757148356, 0.00040747795035560476, 0.033820669879515194, 0.026078588822758705, 0.06193664845405192, 0.029745890375959146, 0.021967076894130452, 0.9696895371837586, 0.000784538460504659, 0.007060846144541931, 0.9985645896310109, 0.5477622925055411, 0.4496913143713613, 0.129959973333275, 0.23922261758013952, 0.6180318731849077, 0.01203333086419213, 0.0004813332345676852, 0.7213637554494148, 0.0005851662992897301, 0.0005851662992897301, 0.05090946803820652, 0.22660564939994798, 0.9950227316230205, 0.9842576162692274, 0.0012464087903308716, 0.998373441055028, 0.028300416147711904, 0.9612382726033182, 0.008782887769979557, 0.035316649142153975, 0.9562661921567845, 0.008149995955881686, 0.9802592869559529, 0.07522314274483369, 0.00028821127488442026, 0.31559134599844013, 0.567199788972539, 0.004803521248073671, 0.03631462063543695, 0.00028821127488442026, 0.00028821127488442026, 0.14651764765272993, 0.1727687261905107, 0.6721497086068985, 0.008546862779742578, 0.10558717044047614, 0.0007403132020366424, 0.3103763099538624, 0.409346931151136, 0.1738347937532291, 9.25391502545803e-05, 0.9808397273414535, 0.14494996083590225, 0.17139828180810748, 0.17390615891441613, 0.2834701516439281, 0.208420595260458, 0.0059584314227900175, 0.00034683406789374734, 0.0007292408606996739, 0.010502847030320912, 0.0003201545242096129, 0.982760644243451, 0.999273782774927, 0.0911215530384534, 0.1393088473143491, 0.5715468088449156, 0.019481730132143237, 0.026058365144904966, 0.12677601455418053, 0.019523092616500228, 0.00616301016919181, 4.136248435699201e-05, 0.018258791661107054, 0.9812115866577531, 0.9966406652331564, 0.21735662118739782, 0.26059698797615405, 0.1937476375401244, 0.007955201011581266, 0.017578428041719895, 0.25084545125228025, 0.01719349896051435, 0.03374544945235279, 0.0007698581624110902, 0.00025661938747036344, 0.06487423987886151, 0.05033465570110173, 0.6039466322980885, 0.01253173683892629, 0.26247411246613017, 0.001730902878304736, 0.003669514102006041, 0.0003461805756609472, 0.0001384722302643789, 0.9916086322783467, 0.12414341680396175, 0.06221057128431193, 0.09942582374903425, 0.5384408262386596, 0.1755226832664515, 0.00013886288233105342, 0.00013886288233105342, 0.9972277049906577, 0.07230603214447337, 0.016815356312668225, 0.053809140200538313, 0.015133820681401402, 0.018496891943935046, 0.8222709236894762, 0.9927428030442668, 0.2533415501952657, 0.001182457643851882, 0.5353576982539395, 0.0002956144109629705, 0.0008868432328889115, 0.026900911397630314, 0.18180286274222685, 0.3027142526898897, 0.013917896675397229, 0.11482264757202713, 0.02684165787398037, 0.049706773840704384, 0.4915999932845664, 0.14380405964656898, 0.005213440228878634, 0.6112758668360198, 0.23981825052841715, 0.9960226514844758, 0.008210814555228943, 0.985297746627473, 0.08787026407188876, 0.27216716848007544, 0.21212372808548252, 0.4230570038211342, 7.44032718644274e-05, 0.003496953777628088, 0.0005208229030509918, 7.44032718644274e-05, 7.44032718644274e-05, 0.0005208229030509918, 0.3803490922322769, 0.5858570834279364, 0.033056517963642916, 0.9924616039483983, 0.9984481637698491, 0.9928335813248086, 0.9876129788571968, 0.7522275755593082, 0.02219128248624283, 0.07021549381544526, 0.005007263740485561, 0.11391525009604653, 0.008648910097202334, 0.02742614912402319, 0.0004552057945895965, 0.289589361708169, 0.7039082237775345, 0.005423021754834626, 0.9844594374648156, 0.9950107191715122, 0.16522463435958235, 0.07271451879881975, 0.7612484933008515, 0.0005879880226319656, 0.9858923328791203, 0.9959861637759999, 0.04225509522707351, 0.007922830355076283, 0.9480986991574619, 0.9912292972802965, 0.20599047248271413, 0.13273849062131937, 0.041788043343748686, 0.003441368275367539, 0.0009832480786764396, 0.10373267230036438, 0.36527666122829733, 0.1460123396834513, 0.0017088843698220197, 0.22087330479949605, 0.05724762638903766, 0.1388468550480391, 0.00042722109245550493, 0.1038147254666877, 0.0034177687396440394, 0.473788191533155, 0.1970336775461785, 0.04656980260480534, 0.10991262733422276, 0.0007399862702034746, 0.5878450930496403, 0.04439917621220848, 0.002170626392596859, 0.009274494586550216, 0.001973296720542599, 4.933241801356497e-05, 0.9911780395124578, 0.006773426238126135, 0.002639256310517767, 0.1378503872955049, 0.11693935652755644, 0.18484945159434052, 0.5371901690484624, 0.0006090591485810231, 0.019794422328883253, 0.9823744636236945, 0.002848080591940011, 0.9954041668830339, 0.995709162417309, 0.2859132709449154, 0.024905337190323642, 0.21318968634917038, 0.47619004707898804, 0.3617649615911906, 0.18541587630425685, 0.08015043259313596, 0.06600623860611197, 0.06927028337234828, 0.15676481668951595, 0.04361126479332398, 0.03372846258444183, 0.0002720037305196922, 0.002992041035716614, 0.5651879728416757, 0.11207249237351477, 0.030641994685546556, 0.1611720665350008, 0.12244734096783368, 0.006755715363742548, 0.0015682910665830916, 0.0001206377743525455, 0.976557320239182, 0.006715030689267459, 0.01630793167393526, 0.10001751012295348, 0.8980295589763058, 0.9927907008880597, 0.0319958520813154, 0.9598755624394618, 0.1836624289090789, 0.002514045194126267, 0.813453604994019, 0.00036567930096382064, 0.029305858702471748, 0.29110486311121936, 0.25203038484125706, 0.022467825005228342, 0.0986630576316549, 0.0117223434809887, 0.04981995979420197, 0.24519235114401364, 0.14019132364113593, 0.046085883403867677, 0.8134319560235106, 0.20129310106532797, 0.18577675785820894, 0.16128609722859405, 0.09024640697762204, 0.35427585787497723, 0.0008387212544388665, 0.006290409408291499, 0.9724937737100073, 0.9640816489372764, 0.9936812322404545, 0.23520308057576658, 0.08538375642096599, 0.22380090192327734, 0.008485342253015254, 0.09572526729182834, 0.0814062522398651, 0.03977504181100901, 0.22035373163298988, 0.0029168363994739937, 0.006894340580574894, 0.08346839962814635, 0.23263450735070468, 0.5643540826470799, 0.07081351323291125, 0.002423276118236507, 0.010770116081051143, 0.03527213016544249, 0.00026925290202627853, 0.33280082493361934, 0.5213799667277775, 0.10058353462511045, 0.0015585053040839522, 0.03248884133898085, 0.0013187352573018056, 0.009830571918068007, 0.22957120569861236, 0.026958820485708607, 0.000421231570089197, 0.000421231570089197, 0.7426312580672543, 0.9962584974960031, 0.1534153263595093, 0.0429562913806626, 0.8023621568602336, 0.001534153263595093, 0.9904132516720121, 0.9990330555056666, 0.1948169991141362, 0.20495435030597542, 0.00793002472264843, 0.5170049107838007, 0.0048234170993428594, 0.05665471270923054, 0.008257036051417439, 0.001880315140421793, 0.003760630280843586, 0.4930953005964547, 0.002541728353589973, 0.0006354320883974932, 0.0006354320883974932, 0.020333826828719783, 0.4829283871820948, 0.9962690773157816, 0.03693705446050203, 0.9620423729939847, 0.9977474571475092, 0.20819847573879918, 0.1283645496230134, 0.10882624665257111, 0.005462321260553763, 0.5357276620927729, 0.011134731800359593, 0.0014706249547644745, 0.0006302678377562034, 0.9933151461326163, 0.9905615575087932, 0.045945604866297146, 0.12433263818569405, 0.28352489455429297, 0.47898999686996857, 0.0500282628516824, 0.005527290810983115, 0.0019785188698405467, 0.00964135385779441, 6.281012285208085e-05, 0.011678468685341045, 0.1261274618016833, 0.7106348195030027, 0.1506522460408995, 0.0005839234342670523, 0.10480329338536273, 0.0008252227825619112, 0.002475668347685734, 0.892065827949426, 0.9976030917755143, 0.004270265298784451, 0.9949718146167771, 0.9969038759882204, 0.17446479784715596, 0.03543816206270355, 0.5179423686087442, 0.06845320193308549, 0.0006057805480804027, 0.20293648360693486, 0.9989524805034957, 0.06974836251373472, 0.9287545113670991, 0.9981370803147183, 0.9974784333688742, 0.10827529393014769, 0.018081335921537434, 0.5784963886896595, 0.04094890782230536, 0.2539895892978317, 0.0001063607995384555, 0.0001063607995384555, 0.9790032502419875, 0.11971192815910117, 0.03555176841518642, 0.06937314102255403, 0.40978918903345407, 0.3647988980301651, 0.0006292348392068393, 0.011668952359627243, 0.9801919982086884, 0.006667972776929853, 0.9914879784549042, 0.007603435417598959, 0.0778262762787491, 0.7585214634387352, 0.07554843404620035, 0.029991589395225265, 0.03530655460450569, 0.022398781953396084, 0.997882305843733, 0.009574479906619198, 0.0031914933022063993, 0.8521287116891086, 0.1340427186926688, 0.036686291016725055, 0.0028220223859019276, 0.931267387347636, 0.028220223859019275, 0.03057404037886257, 0.8983672197989118, 0.05299500332336179, 0.017834856887669833, 0.10138681767964564, 0.007510134642936714, 0.0018775336607341785, 0.10138681767964564, 0.005632600982202535, 0.7829315365261524, 0.08655118848800666, 0.30768874283130626, 0.1126190591324486, 0.013473281456677855, 0.3637785993303021, 0.053600228403740166, 0.00644374330536767, 0.05433247196116831, 0.00131803840337066, 0.9973149283426364, 0.002261485098282622, 0.9670295507556701, 0.008619369377259418, 0.9869177936962034, 0.0929881928739908, 0.7452276499996134, 0.12868508208154175, 0.029967758840906984, 0.0026442140153741457, 0.9972158177689081, 0.001821398753915814, 0.9917623874741901, 0.06706801998513198, 0.24826933713794472, 0.6836231510765208, 0.24506495252126603, 0.091828062116192, 0.5426030833946418, 0.0017110819027861864, 0.10722779924126769, 0.010076371205296432, 0.0013308414799448117, 0.18197535663081063, 0.08349457539531312, 0.004281773097195544, 0.7300423130718403, 0.9871474507574459, 0.07258739587461004, 0.5164872398770329, 0.2218764529770469, 0.12519121717645293, 0.053485449591817925, 0.006465274126483485, 0.0033795751115709128, 0.0005877521933166805, 0.01340129769548516, 0.26856200581752265, 0.6995477397043254, 0.0010721038156388128, 0.0005360519078194064, 0.016081557234582194, 0.0005360519078194064, 0.9956664339831076, 0.0020237122641933083, 0.9926859829242288, 0.9969230004152685, 0.0021412297279368624, 0.9100226343731666, 0.08779041884541136, 0.9916541311548538, 0.0006736780782301996, 0.003368390391150998, 0.004715746547611397, 0.33667268784060655, 0.23810013117739282, 0.02143125591653253, 0.05009976841220824, 0.28662902219257774, 0.02305823607773526, 0.014474513158286368, 0.02951005395836678, 0.9759096730361588, 0.9928034705108221, 0.9936620914071921, 0.13593090042785128, 0.314827160590271, 0.32730829502020725, 0.18668751377625872, 0.000453859433815864, 0.030106009109785645, 0.0012859350624782812, 0.000226929716907932, 0.003177016036711048, 0.9818255289769815, 0.007429293886347569, 0.04457576331808542, 0.9435203235661412, 0.008000347040797086, 0.9904429636506792, 0.0016000694081594173, 0.00992232967012563, 0.2381359120830151, 0.049115531867121864, 0.004465048351556533, 0.0014883494505188444, 0.018852426373238695, 0.6776951164695805, 0.9979355687435455, 0.9354231710387019, 0.006901176982847566, 0.05771893476563419, 0.008195616344394245, 0.9806478867257942, 0.010315172295530688, 0.0007065186503788142, 0.33534901566000086, 0.01998193616626948, 0.20010895494046682, 0.00984617144424873, 0.0017375596666321287, 0.1187332438865288, 0.23920404743968973, 0.06979197994305718, 0.005212678999896386, 0.9897151365040872, 0.007497841943212781, 0.9990857044758062, 0.9939564768039942, 0.004982237978967389, 0.979318736682868, 0.005856833013251526, 0.9839479462262564, 0.013119062487226683, 0.9217867589709275, 0.06352388151709763, 0.0006904769730119308, 0.992852133889625, 0.821717825015487, 0.00018163523983543036, 0.029424908853339718, 0.05776000626766685, 0.09081761991771517, 0.21833808184137504, 0.2066987355667429, 0.07013709522386082, 0.051373666315617655, 0.4136984887095539, 0.022576318205105413, 0.016355288299698588, 0.00010033919202269073, 0.00010033919202269073, 0.0007023743441588351, 0.9970371964677834, 0.24119290181952557, 0.7570288159298978, 0.24736915747878324, 0.20087779100227432, 0.15219343554102444, 0.0026315867816891834, 0.00043859779694819726, 0.0008771955938963945, 0.028508856801632818, 0.3666677582486929, 0.7157191673636353, 0.0031898347291972606, 0.0007974586822993152, 0.00408697574678399, 0.07296746943038733, 0.002990470058622432, 0.20026181159241552, 0.9965745223472284, 0.04933223602053873, 0.1401165040583354, 0.08031331203343724, 0.23208820009662642, 0.2419114681007162, 0.18577850807734608, 0.0073404640030560915, 0.051491196021437584, 0.011658384004853792, 0.9263523504787988, 0.0006257023643895973, 0.06695015298968691, 0.005631321279506376, 0.9986611760796673, 0.9610087827742102, 0.025289704809847637, 0.08961063322935185, 0.8961063322935185, 0.014935105538225309, 0.00226156039832401, 0.9962173554617266, 0.9969728852861995], \"Term\": [\"adicting\", \"adictive\", \"ago\", \"ago\", \"ago\", \"alright\", \"award\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awsome\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"badge\", \"badge\", \"best\", \"best\", \"best\", \"best\", \"best\", \"biomes\", \"bla\", \"blah\", \"bore\", \"bore\", \"bore\", \"bore\", \"bore\", \"bored\", \"boring\", \"boring\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"build\", \"build\", \"build\", \"build\", \"build\", \"building\", \"building\", \"building\", \"building\", \"bunny\", \"bunny\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"c\", \"c\", \"cancer\", \"cancer\", \"cant\", \"cant\", \"cant\", \"cant\", \"cat\", \"cent\", \"certain\", \"certain\", \"certain\", \"chinese\", \"class\", \"class\", \"click\", \"co\", \"co\", \"combat\", \"combat\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"content\", \"content\", \"content\", \"content\", \"content\", \"controller\", \"controller\", \"controller\", \"cool\", \"cool\", \"cool\", \"cough\", \"craft\", \"craft\", \"craft\", \"craft\", \"craft\", \"craft\", \"crash\", \"crust\", \"cup\", \"customization\", \"dank\", \"dec\", \"degree\", \"delete\", \"demon\", \"dev\", \"developer\", \"developer\", \"devs\", \"die\", \"die\", \"die\", \"die\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dig\", \"dig\", \"dig\", \"dig\", \"dis\", \"dis\", \"dlc\", \"dollar\", \"dollar\", \"dollar\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"dont\", \"egg\", \"enemy\", \"enemy\", \"enemy\", \"environment\", \"equipment\", \"equipment\", \"especialy\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"expert\", \"expert\", \"explore\", \"explore\", \"explore\", \"explore\", \"explore\", \"explore\", \"eye\", \"eye\", \"eye\", \"eye\", \"eyeball\", \"f\", \"f\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"file\", \"finaly\", \"fix\", \"fix\", \"flesh\", \"flesh\", \"flesh\", \"fly\", \"fly\", \"fly\", \"freind\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friends\", \"friends\", \"friends\", \"friends\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"funnest\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gb\", \"generate\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"giant\", \"giant\", \"glitch\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gr\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"gud\", \"guess\", \"guess\", \"guess\", \"guess\", \"guess\", \"guess\", \"h\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"highly\", \"highly\", \"highly\", \"highly\", \"hole\", \"holy\", \"holy\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"house\", \"house\", \"house\", \"housing\", \"ign\", \"ing\", \"itch\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"ive\", \"ive\", \"ive\", \"iz\", \"k\", \"kill\", \"kill\", \"kill\", \"kill\", \"killing\", \"l\", \"lag\", \"lag\", \"lag\", \"le\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"limit\", \"limit\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"luv\", \"mac\", \"mac\", \"mage\", \"magic\", \"magic\", \"magic\", \"magic\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"material\", \"material\", \"material\", \"melee\", \"melee\", \"meme\", \"mincraft\", \"mincraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"minecraft\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"mode\", \"mode\", \"mode\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"mutch\", \"mutiplayer\", \"n\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nominate\", \"normal\", \"normal\", \"normal\", \"normal\", \"nuff\", \"ok\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"otherworld\", \"patch\", \"patch\", \"penny\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"pickle\", \"pie\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"please\", \"please\", \"please\", \"please\", \"plz\", \"positive\", \"positive\", \"pour\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"progression\", \"r\", \"r\", \"rainbow\", \"ranger\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"recommed\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"release\", \"release\", \"release\", \"resource\", \"resource\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"rid\", \"rip\", \"rip\", \"rip\", \"rip\", \"ripoff\", \"ripoff\", \"ripoff\", \"ripoff\", \"sale\", \"sale\", \"sale\", \"sale\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"shoot\", \"shoot\", \"sibling\", \"simulator\", \"simulator\", \"since\", \"since\", \"since\", \"since\", \"since\", \"slime\", \"slime\", \"soooo\", \"starbound\", \"starbound\", \"starbound\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"step\", \"step\", \"step\", \"step\", \"steroid\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"structure\", \"structure\", \"sugar\", \"summoner\", \"sword\", \"sword\", \"sword\", \"system\", \"system\", \"system\", \"system\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"terrarium\", \"thx\", \"ti\", \"tickle\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timesink\", \"tired\", \"tired\", \"tired\", \"tree\", \"tree\", \"tree\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"unicorn\", \"unique\", \"unique\", \"unique\", \"update\", \"update\", \"update\", \"update\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"v\", \"v\", \"variety\", \"vast\", \"vast\", \"verry\", \"w\", \"w\", \"wall\", \"wall\", \"wall\", \"wall\", \"warrior\", \"weapon\", \"weapon\", \"weapon\", \"weapon\", \"weapon\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"window\", \"wont\", \"wont\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worm\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"year\", \"year\", \"year\", \"year\", \"yo\", \"youre\", \"youre\", \"zelda\", \"zelda\", \"zelda\", \"zombie\", \"zombie\", \"zone\"]}, \"R\": 10, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 9, 2, 7, 8, 10, 1, 6, 5, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el4604312013216480178881179\", ldavis_el4604312013216480178881179_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el4604312013216480178881179\", ldavis_el4604312013216480178881179_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el4604312013216480178881179\", ldavis_el4604312013216480178881179_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.075750  0.021131       1        1  28.269223\n",
       "8      0.075771  0.171803       2        1  15.823430\n",
       "1     -0.093004  0.207422       3        1  13.109807\n",
       "6      0.028419  0.349141       4        1  12.133336\n",
       "7     -0.247331  0.224475       5        1  11.864811\n",
       "9     -0.289910 -0.111250       6        1   9.082661\n",
       "0      0.030011 -0.273153       7        1   3.594973\n",
       "5     -0.200340 -0.338849       8        1   2.810446\n",
       "4      0.325490 -0.272043       9        1   1.934825\n",
       "3      0.446644  0.021323      10        1   1.376487, topic_info=           Term           Freq          Total Category  logprob  loglift\n",
       "15         game  112445.000000  112445.000000  Default  10.0000  10.0000\n",
       "27    minecraft   21877.000000   21877.000000  Default   9.0000   9.0000\n",
       "32         play   31842.000000   31842.000000  Default   8.0000   8.0000\n",
       "173       world   10031.000000   10031.000000  Default   7.0000   7.0000\n",
       "541        best   10144.000000   10144.000000  Default   6.0000   6.0000\n",
       "...         ...            ...            ...      ...      ...      ...\n",
       "1609          w     168.208398     170.740740  Topic10  -4.8881   4.2707\n",
       "1925          c     264.874296     293.667442  Topic10  -4.4341   4.1824\n",
       "726      minute     250.781625    1023.686093  Topic10  -4.4888   2.8791\n",
       "471         dig     261.736794    1836.422213  Topic10  -4.4460   2.3374\n",
       "1860          f     188.071448     418.064557  Topic10  -4.7765   3.4868\n",
       "\n",
       "[258 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "5755      3  0.974234  adicting\n",
       "9503      4  0.979737  adictive\n",
       "1200      2  0.974381       ago\n",
       "1200      7  0.023292       ago\n",
       "1200      9  0.001294       ago\n",
       "...     ...       ...       ...\n",
       "2369      5  0.896106     zelda\n",
       "2369      6  0.014935     zelda\n",
       "405       1  0.002262    zombie\n",
       "405       6  0.996217    zombie\n",
       "2411      7  0.996973      zone\n",
       "\n",
       "[767 rows x 3 columns], R=10, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 9, 2, 7, 8, 10, 1, 6, 5, 4])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=10)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model\n",
    "\n",
    "we need to save the corpora.Dictionary and the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the LDA multicore model (and the corpora.Dictionary object) automatically\n",
    "\n",
    "# lda_save_folder = Path(f'lda_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "# if not lda_save_folder.exists():\n",
    "#     lda_save_folder.mkdir()\n",
    "\n",
    "# lda_model.save(str(lda_save_folder.joinpath('lda_model')))     # no need to add file extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n",
    "gensim provide functions to calculate, so we don't need to install octis (as the evaluation backend of octis also relies on gensim)\n",
    "\n",
    "octis seems awesome for simple development, but it installs many packages ;("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = lemmatized words (?) (list of list of str)\n",
    "\n",
    "# create a result object from the LDAMulticore model for octis evaluation\n",
    "# referencing from https://github.com/MIND-Lab/OCTIS/blob/master/octis/models/LDA.py\n",
    "# and guideline in README: https://github.com/MIND-Lab/OCTIS/tree/master\n",
    "result_lda_online = {}\n",
    "result_lda_online['topic-word-matrix'] = lda_model.get_topics()\n",
    "\n",
    "top_words = 10\n",
    "topics_output = []\n",
    "for topic in result_lda_online[\"topic-word-matrix\"]:\n",
    "    top_k = np.argsort(topic)[-top_words:]\n",
    "    top_k_words = list(reversed([id2word[i] for i in top_k]))\n",
    "    topics_output.append(top_k_words)\n",
    "result_lda_online[\"topics\"] = topics_output\n",
    "\n",
    "def _get_topic_document_matrix(lda_model, corpus, num_topics=10):\n",
    "    \"\"\"\n",
    "    Return the topic representation of the\n",
    "    corpus\n",
    "    \"\"\"\n",
    "\n",
    "    id_corpus = corpus\n",
    "\n",
    "    doc_topic_tuples = []\n",
    "    for document in id_corpus:\n",
    "        doc_topic_tuples.append(\n",
    "            lda_model.get_document_topics(document, minimum_probability=0))\n",
    "\n",
    "    topic_document = np.zeros((num_topics, len(doc_topic_tuples)))\n",
    "\n",
    "    for ndoc in range(len(doc_topic_tuples)):\n",
    "        document = doc_topic_tuples[ndoc]\n",
    "        for topic_tuple in document:\n",
    "            topic_document[topic_tuple[0]][ndoc] = topic_tuple[1]\n",
    "    return topic_document\n",
    "\n",
    "result_lda_online['topic-document-matrix'] = _get_topic_document_matrix(lda_model, corpus, num_topics=N_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics(num_topics=N_TOPICS, num_words=10, formatted=True, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup: get the model's topics in their native ordering...\n",
    "all_topics = lda_model.print_topics()\n",
    "# ...then create a empty list per topic to collect the docs:\n",
    "docs_per_topic = [[] for _ in all_topics]\n",
    "\n",
    "# now, for every doc...\n",
    "for doc_id, doc_bow in enumerate(corpus):\n",
    "    # ...get its topics...\n",
    "    doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "    # ...& for each of its topics...\n",
    "    for topic_id, score in doc_topics:\n",
    "        # ...add the doc_id & its score to the topic's doc list\n",
    "        docs_per_topic[topic_id].append((doc_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're interested in the top docs per topic, you can further sort each list's pairs by their score\n",
    "\n",
    "for doc_list in docs_per_topic:\n",
    "    doc_list.sort(key=lambda id_and_score: id_and_score[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_per_topic[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top 10 documents for each topic, also the name of the game\n",
    "for topic_id, docs in enumerate(docs_per_topic):\n",
    "    print(f'Topic {topic_id + 1}:')\n",
    "    for doc_id, score in docs[:10]:\n",
    "        print(f'Game: {dataset.iloc[doc_id][\"app_name\"]}')\n",
    "        print(f'Doc ID: {doc_id}')\n",
    "        print(f'Score: {score}')\n",
    "        print(f'Doc: {dataset.iloc[doc_id][\"review_text\"]}')\n",
    "        print()\n",
    "    print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[1655473]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1655473]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lda_online['topic-document-matrix'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_topics().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(result_lda_online['topic-document-matrix'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference test\n",
    "\n",
    "inference_test = [\"well its been fun guys, but that's it, no more updates, that one was the last one, there is no longer going to be anymore content for this game anymore, there is no way to replay it as there won't be any updates, nope, that was it, the last update, nothing more, this game has no new ways to experience it as there is no more content updates, nothing new to freshen up the experience, its such a shame that this game has no replay-ability, once you beat the game there is like no point to playing again, as they said guys 1.2 will be they final update. nothing more after 1.2, there is no chance they will make another final update right? several years and final updates later: alright, thats it, no more updates we wont be getting anymore, thats it, nothing more, no more updates, for real this time... oh god, redigit made another tweet.\",\n",
    "                  \"keeps forcing me to play it\",\n",
    "'''I will leave the cat here, so that everybody who passes by can pet it and give it a thumbs up and awards\n",
    " \n",
    " | _ _ l\n",
    "  ` x\n",
    "  /  |\n",
    " /  \n",
    "  |||\n",
    "| |||\n",
    "| (__)__)\n",
    "''']\n",
    "\n",
    "inference_test = cleaning_strlist(inference_test)\n",
    "\n",
    "inference_test = list(map(lambda x: lemmatization(x), inference_test))\n",
    "\n",
    "corpus_test = [id2word.doc2bow(text) for text in inference_test]\n",
    "\n",
    "test_output = lda_model[corpus_test]\n",
    "\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference\n",
    "\n",
    "corpus_test = [id2word.doc2bow(text) for text in inference_test]\n",
    "\n",
    "output_test = lda_model[corpus_test]\n",
    "\n",
    "for i in range(len(output_test)):\n",
    "    # print(sorted(test_output[i], key=lambda x: x[1], reverse=True))\n",
    "    print(sorted(output_test[i], key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model (both corpora Dictionary and the LDA model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del id2word\n",
    "# del lda_model\n",
    "\n",
    "# model_datetime = datetime(2024, 1, 15, 0, 21, 57)\n",
    "# lda_save_folder = Path(f'lda_model_{model_datetime.strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "\n",
    "# # id2word_load = gensim.corpora.Dictionary.load('lda_model.id2word')\n",
    "# id2word_l = gensim.corpora.Dictionary.load(str(lda_save_folder.joinpath('lda_model.id2word')))\n",
    "\n",
    "# lda_model_l = gensim.models.ldamulticore.LdaMulticore.load(str(lda_save_folder.joinpath('lda_model')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_test2 = [id2word_l.doc2bow(text) for text in inference_test]\n",
    "\n",
    "# output_test2 = lda_model_l[corpus_test2]\n",
    "\n",
    "# for i in range(len(output_test2)):\n",
    "#     print(sorted(output_test2[i], key=lambda x: x[1], reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-test-tm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
