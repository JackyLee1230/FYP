{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not using Octis as package compatability is poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 75499 entries, 63365 to 145140\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   index         75499 non-null  int64 \n",
      " 1   app_id        75499 non-null  int64 \n",
      " 2   app_name      75499 non-null  object\n",
      " 3   review_text   75499 non-null  object\n",
      " 4   review_score  75499 non-null  int64 \n",
      " 5   review_votes  75499 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "\n",
    "dataset_path = Path('../../dataset/topic_modelling/top_10_games/00_Terraria.pkl')\n",
    "\n",
    "# dataset_path = Path('../dataset_cleaned_heartless_sampled_for_demo.pkl')\n",
    "\n",
    "dataset = pd.read_pickle(dataset_path)\n",
    "\n",
    "dataset.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "import re\n",
    "\n",
    "def clean(raw):\n",
    "    \"\"\" Remove hyperlinks and markup \"\"\"\n",
    "    result = re.sub(\"<[a][^>]*>(.+?)</[a]>\", 'Link.', raw)\n",
    "    result = re.sub('&gt;', \"\", result)\n",
    "    result = re.sub('&#x27;', \"'\", result)\n",
    "    result = re.sub('&quot;', '\"', result)\n",
    "    result = re.sub('&#x2F;', ' ', result)\n",
    "    result = re.sub('<p>', ' ', result)\n",
    "    result = re.sub('</i>', '', result)\n",
    "    result = re.sub('&#62;', '', result)\n",
    "    result = re.sub('<i>', ' ', result)\n",
    "    result = re.sub(\"\\n\", '', result)\n",
    "    return result\n",
    "\n",
    "def deEmojify(x):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'', x)\n",
    "    \n",
    "def remove_num(texts):\n",
    "   output = re.sub(r'\\d+', '', texts)\n",
    "   return output\n",
    "\n",
    "def unify_whitespaces(x):\n",
    "    cleaned_string = re.sub(' +', ' ', x)\n",
    "    return cleaned_string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words(\"english\"))\n",
    "def remove_stopword(text):\n",
    "   text=[word.lower() for word in text.split() if word.lower() not in stop]\n",
    "   return \" \".join(text)\n",
    "\n",
    "# only keep alphabets\n",
    "def remove_non_alphabets(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    return text\n",
    "\n",
    "def cleaning(df, review):\n",
    "    df[review] = df[review].apply(lambda x: clean(x))\n",
    "    df[review] = df[review].apply(lambda x: deEmojify(x))\n",
    "    df[review] = df[review].apply(lambda x: remove_num(x))\n",
    "    df[review] = df[review].apply(lambda x: unify_whitespaces(x))\n",
    "    df[review] = df[review].apply(lambda x: remove_non_alphabets(x))\n",
    "    df[review] = df[review].apply(lambda x: remove_stopword(x))\n",
    "    df[review] = df[review].apply(lambda x: x.lower())\n",
    "\n",
    "def cleaning_strlist(str_list):\n",
    "    str_list = list(map(lambda x: clean(x), str_list))\n",
    "    str_list = list(map(lambda x: deEmojify(x), str_list))\n",
    "    str_list = list(map(lambda x: remove_num(x), str_list))\n",
    "    str_list = list(map(lambda x: unify_whitespaces(x), str_list))\n",
    "    str_list = list(map(lambda x: remove_non_alphabets(x), str_list))\n",
    "    str_list = list(map(lambda x: remove_stopword(x), str_list))\n",
    "    str_list = list(map(lambda x: x.lower(), str_list))\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply data preprocessing\n",
    "\n",
    "cleaning(dataset, 'review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['review_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['werewolf riding unicorn shooting rainbows gun build teleporters find hair dresser spider cavern get sword shoots cats take lord moon using yoyo summon sharknado minion shoots sharks enemies find sky temples air wyverns spawn buy music box wizard go record music like playing base whenever want go build castle made entirely white marble would seem thing minecraft game dimension trust get used start learning game terraria simply one satisfying sandbox experiences may sound rude compared minecraft imagination',\n",
       "       'copies game go around giving people look sad', 'introduction',\n",
       "       ...,\n",
       "       'game game start newb get pro hours entertainment even get bored get mods newb',\n",
       "       'far one greatest games played yet',\n",
       "       'game awesome eye cthulhu possible'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do lemmatization, but not stemming (as part of speech is important in topic modelling)\n",
    "# use nltk wordnet for lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# from https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word\n",
    "\n",
    "# from: https://www.cnblogs.com/jclian91/p/9898511.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None     # if none -> created as noun by wordnet\n",
    "    \n",
    "def lemmatization(text):\n",
    "   # use nltk to get PoS tag\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "    # then we only need adj, adv, verb, noun\n",
    "    # convert from nltk Penn Treebank tag to wordnet tag\n",
    "    wn_tagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1])), tagged))\n",
    "\n",
    "    # lemmatize by the PoS\n",
    "    lemmatized = list(map(lambda x: lemma.lemmatize(x[0], pos=x[1] if x[1] else wordnet.NOUN), wn_tagged))\n",
    "    # lemma.lemmatize(wn_tagged[0], pos=wordnet.NOUN)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the data\n",
    "\n",
    "X_lemmatized = list(map(lambda x: lemmatization(x), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lematized data, as separate pickle file\n",
    "\n",
    "import pickle\n",
    "\n",
    "X_lemmatized_file = Path('00_Terraria_lemmatized.pkl')\n",
    "\n",
    "with open(X_lemmatized_file, \"wb\") as f:\n",
    "    pickle.dump(X_lemmatized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['werewolf',\n",
       " 'rid',\n",
       " 'unicorn',\n",
       " 'shooting',\n",
       " 'rainbow',\n",
       " 'gun',\n",
       " 'build',\n",
       " 'teleporters',\n",
       " 'find',\n",
       " 'hair',\n",
       " 'dresser',\n",
       " 'spider',\n",
       " 'cavern',\n",
       " 'get',\n",
       " 'sword',\n",
       " 'shoot',\n",
       " 'cat',\n",
       " 'take',\n",
       " 'lord',\n",
       " 'moon',\n",
       " 'use',\n",
       " 'yoyo',\n",
       " 'summon',\n",
       " 'sharknado',\n",
       " 'minion',\n",
       " 'shoot',\n",
       " 'sharks',\n",
       " 'enemy',\n",
       " 'find',\n",
       " 'sky',\n",
       " 'temple',\n",
       " 'air',\n",
       " 'wyverns',\n",
       " 'spawn',\n",
       " 'buy',\n",
       " 'music',\n",
       " 'box',\n",
       " 'wizard',\n",
       " 'go',\n",
       " 'record',\n",
       " 'music',\n",
       " 'like',\n",
       " 'play',\n",
       " 'base',\n",
       " 'whenever',\n",
       " 'want',\n",
       " 'go',\n",
       " 'build',\n",
       " 'castle',\n",
       " 'make',\n",
       " 'entirely',\n",
       " 'white',\n",
       " 'marble',\n",
       " 'would',\n",
       " 'seem',\n",
       " 'thing',\n",
       " 'minecraft',\n",
       " 'game',\n",
       " 'dimension',\n",
       " 'trust',\n",
       " 'get',\n",
       " 'use',\n",
       " 'start',\n",
       " 'learning',\n",
       " 'game',\n",
       " 'terrarium',\n",
       " 'simply',\n",
       " 'one',\n",
       " 'satisfy',\n",
       " 'sandbox',\n",
       " 'experience',\n",
       " 'may',\n",
       " 'sound',\n",
       " 'rude',\n",
       " 'compare',\n",
       " 'minecraft',\n",
       " 'imagination']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lemmatized_file = Path('00_Terraria_lemmatized.pkl')\n",
    "\n",
    "with open(X_lemmatized_file, \"rb\") as f:\n",
    "    X_lemmatized = pickle.load(f)\n",
    "\n",
    "X_lemmatized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim to build a dictionary and train our LDAModel\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(X_lemmatized)\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in X_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from lda_multicore_octis import LDAMulticoreOctis\n",
    "from octis_dataset_handler import OctisDatasetHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset in Octis\n",
    "dataset_handler = OctisDatasetHandler(corpus=corpus)\n",
    "# dataset_handler.set_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 2),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 2),\n",
       " (15, 2),\n",
       " (16, 2),\n",
       " (17, 2),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 2),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 2),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 2),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 2),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_handler.get_corpus()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the optimizer for hyperparameter optimization\n",
    "\n",
    "%autoreload 2\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Categorical, Integer\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "\n",
    "search_space = {\"num_topics\":  Categorical({10, 20, 30}),\n",
    "                \"decay\": Categorical({0.5, 0.6, 0.7}),\n",
    "                \"offset\": Categorical({1, 4, 16, 64, 256, 1024})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "npmi = Coherence(texts=X_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_runs=30\n",
    "model_runs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 20\n",
    "\n",
    "lda_model = LDAMulticoreOctis(corpus=corpus,\n",
    "                              id2word=id2word,\n",
    "                              num_topics=N_TOPICS,\n",
    "                              random_state=42,\n",
    "                              chunksize=2048,\n",
    "                              workers=3, passes=10)\n",
    "\n",
    "# lda_model.train_model(dataset_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provided transformers should be a Transformer instance. Got <skopt.space.transformers.LabelEncoder object at 0x7f4503508790>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mOptimizer()\n\u001b[0;32m----> 2\u001b[0m optimization_result \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_handler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpmi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimization_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# to keep track of other metrics\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/test_lda/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl-tm/lib/python3.9/site-packages/octis/optimization/optimizer.py:181\u001b[0m, in \u001b[0;36mOptimizer.optimize\u001b[0;34m(self, model, dataset, metric, search_space, extra_metrics, number_of_call, n_random_starts, initial_point_generator, optimization_type, model_runs, surrogate_model, kernel, acq_func, random_state, x0, y0, save_models, save_step, save_name, save_path, early_stop, early_step, plot_best_seen, plot_model, plot_name, log_scale_plot, topk)\u001b[0m\n\u001b[1;32m    178\u001b[0m     Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path_models)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Choice of the optimizer\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Perform Bayesian Optimization\u001b[39;00m\n\u001b[1;32m    184\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimization_loop(opt)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl-tm/lib/python3.9/site-packages/octis/optimization/optimizer_tool.py:124\u001b[0m, in \u001b[0;36mchoose_optimizer\u001b[0;34m(optimizer)\u001b[0m\n\u001b[1;32m    117\u001b[0m     opt \u001b[38;5;241m=\u001b[39m skopt_optimizer(\n\u001b[1;32m    118\u001b[0m         params_space_list, base_estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    119\u001b[0m         acq_func\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39macq_func,\n\u001b[1;32m    120\u001b[0m         acq_optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    121\u001b[0m         initial_point_generator\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39minitial_point_generator,\n\u001b[1;32m    122\u001b[0m         random_state\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m     opt \u001b[38;5;241m=\u001b[39m \u001b[43mskopt_optimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_space_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43macq_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43macq_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msampling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_random_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_point_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_point_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# work only for version skopt 8.0!!!\u001b[39;49;00m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43macq_optimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_points\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_restarts_optimizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_jobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43macq_func_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkappa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.96\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opt\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl-tm/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:279\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, dimensions, base_estimator, n_random_starts, n_initial_points, initial_point_generator, n_jobs, acq_func, acq_optimizer, random_state, model_queue_size, acq_func_kwargs, acq_optimizer_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_point_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspace\u001b[38;5;241m.\u001b[39mget_transformer()\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_point_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspace\u001b[38;5;241m.\u001b[39mset_transformer(transformer)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# record categorical and non-categorical indices\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl-tm/lib/python3.9/site-packages/skopt/sampler/lhs.py:81\u001b[0m, in \u001b[0;36mLhs.generate\u001b[0;34m(self, dimensions, n_samples, random_state)\u001b[0m\n\u001b[1;32m     79\u001b[0m transformer \u001b[38;5;241m=\u001b[39m space\u001b[38;5;241m.\u001b[39mget_transformer()\n\u001b[1;32m     80\u001b[0m n_dim \u001b[38;5;241m=\u001b[39m space\u001b[38;5;241m.\u001b[39mn_dims\n\u001b[0;32m---> 81\u001b[0m \u001b[43mspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormalize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m n_samples \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     83\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lhs_normalized(n_dim, n_samples, rng)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl-tm/lib/python3.9/site-packages/skopt/space/space.py:920\u001b[0m, in \u001b[0;36mSpace.set_transformer\u001b[0;34m(self, transform)\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimensions[j]\u001b[38;5;241m.\u001b[39mset_transformer(transform[j])\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimensions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl-tm/lib/python3.9/site-packages/skopt/space/space.py:647\u001b[0m, in \u001b[0;36mCategorical.set_transformer\u001b[0;34m(self, transform)\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories)\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m transform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mLabelEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m         \u001b[49m\u001b[43mNormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m Identity()\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl-tm/lib/python3.9/site-packages/skopt/space/transformers.py:292\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, transformers)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transformer, Transformer):\n\u001b[0;32m--> 292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided transformers should be a Transformer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance. Got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m transformer\n\u001b[1;32m    295\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Provided transformers should be a Transformer instance. Got <skopt.space.transformers.LabelEncoder object at 0x7f4503508790>"
     ]
    }
   ],
   "source": [
    "optimizer=Optimizer()\n",
    "optimization_result = optimizer.optimize(\n",
    "    lda_model, dataset_handler, npmi, search_space, number_of_call=optimization_runs, \n",
    "    model_runs=model_runs, save_models=True, \n",
    "    extra_metrics=None, # to keep track of other metrics\n",
    "    save_path='results/test_lda/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-test-wsl-tm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
