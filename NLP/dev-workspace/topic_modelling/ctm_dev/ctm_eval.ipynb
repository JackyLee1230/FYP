{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTM evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "# from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessingStopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"          # disable huggingface warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 725737 entries, 25636 to 4179608\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   index         725737 non-null  int64 \n",
      " 1   app_id        725737 non-null  int64 \n",
      " 2   app_name      725737 non-null  object\n",
      " 3   review_text   725737 non-null  object\n",
      " 4   review_score  725737 non-null  int64 \n",
      " 5   review_votes  725737 non-null  int64 \n",
      " 6   genre_id      725737 non-null  object\n",
      " 7   category_id   725737 non-null  object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 49.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "# TODO: load external dataset\n",
    "\n",
    "%autoreload 2\n",
    "from dataset_loader import GENRES, load_dataset\n",
    "\n",
    "genre = GENRES.INDIE\n",
    "dataset = load_dataset(genre)\n",
    "\n",
    "dataset.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "# MUST BE IDENTICAL TO THE ONE USED IN TRAINING\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../sa/')\n",
    "\n",
    "%autoreload 2\n",
    "import str_cleaning_functions\n",
    "\n",
    "# copied from lda_demo_gridsearch.ipynb\n",
    "def cleaning(df, review):\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links2(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.clean(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.deEmojify(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_non_letters(x))\n",
    "    df[review] = df[review].apply(lambda x: x.lower())\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_stopword(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))\n",
    "\n",
    "# def cleaning_strlist(str_list):\n",
    "#     str_list = list(map(lambda x: clean(x), str_list))\n",
    "#     str_list = list(map(lambda x: deEmojify(x), str_list))\n",
    "\n",
    "#     str_list = list(map(lambda x: x.lower(), str_list))\n",
    "#     str_list = list(map(lambda x: remove_num(x), str_list))\n",
    "#     str_list = list(map(lambda x: unify_whitespaces(x), str_list))\n",
    "\n",
    "#     str_list = list(map(lambda x: _deaccent(x), str_list))\n",
    "#     str_list = list(map(lambda x: remove_non_alphabets(x), str_list))\n",
    "#     str_list = list(map(lambda x: remove_stopword(x), str_list))\n",
    "#     return str_list\n",
    "\n",
    "# copied from bert_demo_gridsearch.ipynb\n",
    "def cleaning_little(df, review):\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links2(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.clean(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.deEmojify(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the dataset, as we need both untouched text and cleaned text\n",
    "# also a copy for eval with LLM\n",
    "\n",
    "dataset_preprocessed = dataset.copy()\n",
    "dataset_eval = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning(dataset_preprocessed, 'review_text')\n",
    "cleaning_little(dataset, 'review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed_temp = dataset_preprocessed['review_text'].values\n",
    "X_temp = dataset['review_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_temp.shape == X_preprocessed_temp.shape, \"X_temp and X_preprocessed_temp should have the same shape. Found: {} and {}\".format(X_temp.shape, X_preprocessed_temp.shape)\n",
    "assert len(X_temp) == len(X_preprocessed_temp), \"X_temp and X_preprocessed_temp should have the same length. Found: {} and {}\".format(len(X_temp), len(X_preprocessed_temp))\n",
    "\n",
    "# additional test so that we can use the df to keep track with the document id for LLM topic creation\n",
    "assert len(X_temp) == len(dataset), \"X_temp and dataset should have the same length. Found: {} and {}\".format(len(X_temp), len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_eval['review_text_cleaned'] = X_preprocessed_temp\n",
    "dataset_eval['review_text_cleaned_little'] = X_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove docs with 0 len\n",
    "\n",
    "# X, X_preprocessed = [], []\n",
    "\n",
    "# for i, (doc, doc_preprocessed) in enumerate(zip(list(X_temp), list(X_preprocessed_temp))):\n",
    "#     if len(doc) == 0 or len(doc_preprocessed) == 0:\n",
    "#         continue\n",
    "\n",
    "#     X.append(doc)\n",
    "#     X_preprocessed.append(doc_preprocessed)\n",
    "\n",
    "# deal with dataset_eval\n",
    "index_to_remove = []\n",
    "\n",
    "for index, row in dataset_eval.iterrows():\n",
    "    if len(row['review_text_cleaned_little']) == 0 or len(row['review_text_cleaned']) == 0:\n",
    "        index_to_remove.append(index)\n",
    "\n",
    "dataset_eval = dataset_eval.drop(index_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removed {} documents with 0 length\".format(len(index_to_remove)))\n",
    "print(len(dataset_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737139, 737139)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assert len(X) == len(X_preprocessed), \"X and X_preprocessed should have the same length. Found: {} and {}\".format(len(X), len(X_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply lemmatizing to the preprocessed dataset as well (for BoW)\n",
    "\n",
    "The function is identical in LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do lemmatization, but not stemming (as part of speech is important in topic modelling)\n",
    "# use nltk wordnet for lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# from https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word\n",
    "\n",
    "# from: https://www.cnblogs.com/jclian91/p/9898511.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None     # if none -> created as noun by wordnet\n",
    "    \n",
    "def lemmatization(text):\n",
    "   # use nltk to get PoS tag\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "    # then we only need adj, adv, verb, noun\n",
    "    # convert from nltk Penn Treebank tag to wordnet tag\n",
    "    wn_tagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1])), tagged))\n",
    "\n",
    "    # lemmatize by the PoS\n",
    "    lemmatized = list(map(lambda x: lemma.lemmatize(x[0], pos=x[1] if x[1] else wordnet.NOUN), wn_tagged))\n",
    "    # lemma.lemmatize(wn_tagged[0], pos=wordnet.NOUN)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# X_preprocessed = list(map(lambda x: lemmatization(x), X_preprocessed))\n",
    "# X_preprocessed = list(map(lambda x: ' '.join(x), X_preprocessed))\n",
    "\n",
    "def lemmatization_dataset(data):\n",
    "    return {'review_text2': ' '.join(lemmatization(data['review_text']))}\n",
    "\n",
    "temp_dataset = Dataset.from_dict({'review_text': dataset_eval['review_text_cleaned']})\n",
    "temp_dataset = temp_dataset.map(lemmatization_dataset, num_proc=4)      # speed up lemmatization\n",
    "X_preprocessed = temp_dataset['review_text2']\n",
    "\n",
    "dataset_eval['review_text_cleaned'] = X_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "\n",
    "from eval_metrics import compute_inverted_rbo, compute_topic_diversity, compute_pairwise_jaccard_similarity, \\\n",
    "                        METRICS, SEARCH_BEHAVIOUR, COHERENCE_MODEL_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctm_dataset_creation import create_ctm_dataset\n",
    "from ctm_utils import _load_ctm_model, _get_topics, _get_topic_document_metrix, _get_topic_word_metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "if platform.system() == 'Linux' or platform.system() == 'Windows':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('mps')        # m-series mac machine\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from ctm_training.ipynb\n",
    "\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "def split_X_contextual_X_bow(X_contextual, X_bow, X, sbert, split:bool=False):\n",
    "    if not split:\n",
    "        return X_contextual, X_bow, X\n",
    "    else:\n",
    "        X_contextual_new, X_bow_new, X_new = [], [], []\n",
    "        tokenizer = sbert[0].tokenizer\n",
    "\n",
    "        batch_size = 64\n",
    "        for start_index in trange(0, len(X_contextual), batch_size, desc=\"Batches\", disable=False):\n",
    "            sentence_batch = X_contextual[start_index:start_index+batch_size]\n",
    "            features = tokenizer(sentence_batch, return_attention_mask=True, return_token_type_ids=True, add_special_tokens=False, return_tensors=None, truncation=False)\n",
    "\n",
    "            # split overlapping\n",
    "            features_split = split_tokens_into_smaller_chunks(features, sbert.max_seq_length-2,  sbert.max_seq_length-2, 1)\n",
    "\n",
    "            for i, input_id_list in enumerate(features_split['input_ids']):\n",
    "                for input_id in input_id_list:\n",
    "                    X_contextual_new.append(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_id)))\n",
    "                    X_bow_new.append(X_bow[start_index+i])\n",
    "                    X_new.append(X[start_index+i])\n",
    "\n",
    "\n",
    "        assert len(X_contextual_new) == len(X_bow_new), \"X_contextual_new and X_bow_new should have the same length. Found: {} and {}\".format(len(X_contextual_new), len(X_bow_new))\n",
    "        assert len(X_contextual_new) == len(X_new), \"X_contextual_new and X_new should have the same length. Found: {} and {}\".format(len(X_contextual_new), len(X_new))\n",
    "        return X_contextual_new, X_bow_new, X_new\n",
    "    \n",
    "# ####################\n",
    "# # helper functions\n",
    "# ####################\n",
    "    \n",
    "# tokens spliting helper functions\n",
    "\n",
    "def split_tokens_into_smaller_chunks(\n",
    "    data,\n",
    "    chunk_size: int,\n",
    "    stride: int,\n",
    "    minimal_chunk_length: int,\n",
    ") -> dict:\n",
    "    \"\"\"Splits tokens into overlapping chunks with given size and stride.\"\"\"\n",
    "\n",
    "    _new_input_id_chunks = []\n",
    "    _new_token_type_ids = []\n",
    "    _new_mask_chunks = []\n",
    "\n",
    "    for input_id, token_type_id, mask_chunk in zip(data['input_ids'], data['token_type_ids'], data['attention_mask']):\n",
    "        _input_id_chunk = split_overlapping(input_id, chunk_size, stride, minimal_chunk_length)\n",
    "        _token_type_id = split_overlapping(token_type_id, chunk_size, stride, minimal_chunk_length)\n",
    "        _mask_chunk = split_overlapping(mask_chunk, chunk_size, stride, minimal_chunk_length)\n",
    "\n",
    "        _new_input_id_chunks.append(_input_id_chunk)\n",
    "        _new_token_type_ids.append(_token_type_id)\n",
    "        _new_mask_chunks.append(_mask_chunk)    \n",
    "\n",
    "    return {'input_ids':_new_input_id_chunks, 'token_type_ids':_new_token_type_ids, 'attention_mask': _new_mask_chunks}\n",
    "\n",
    "def split_overlapping(tensor:list[int], chunk_size: int, stride: int, minimal_chunk_length: int) -> list[list[int]]:\n",
    "    \"\"\"Helper function for dividing 1-dimensional tensors into overlapping chunks.\"\"\"\n",
    "    # check_split_parameters_consistency(chunk_size, stride, minimal_chunk_length)\n",
    "    result = [tensor[i : i + chunk_size] for i in range(0, len(tensor), stride)]\n",
    "    if len(result) > 1:\n",
    "        # ignore chunks with less than minimal_length number of tokens\n",
    "        result = [x for x in result if len(x) >= minimal_chunk_length]\n",
    "    return result\n",
    "\n",
    "\n",
    "def tokenize_dataset(data, tokenizer):\n",
    "    # return sbert_model[0].tokenizer(data['text'], return_attention_mask=True, return_token_type_ids=True, add_special_tokens=False, return_tensors=None, truncation=False)\n",
    "    return {'tokenized': tokenizer(data['text'], return_attention_mask=True, return_token_type_ids=True, add_special_tokens=False, return_tensors=None, truncation=False)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk to compare the results\n",
    "\n",
    "search_behaviour = SEARCH_BEHAVIOUR.GRID_SEARCH\n",
    "split_sentences = True\n",
    "sbert_model_name = 'all-MiniLM-L6-v2'\n",
    "\n",
    "\n",
    "training_datetime = datetime(2024, 1, 29, 21, 29, 10)\n",
    "training_folder = Path(f'ctm{\"[split]\" if split_sentences else \"\"}_genre_{str(genre)}_{search_behaviour.value}_{training_datetime.strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "\n",
    "training_result_json_path = training_folder.joinpath('result.json')\n",
    "with open(training_result_json_path, 'r') as f:\n",
    "    training_result = json.load(f)\n",
    "\n",
    "\n",
    "# load the embeddings\n",
    "model_name_or_path = training_result['best_hyperparameters']['sbert_params']['model_name_or_path']\n",
    "embeddings_path = training_folder.joinpath(f'embeddings_{model_name_or_path}.pkl')\n",
    "with open(embeddings_path, 'rb') as f:\n",
    "    embeddings = np.load(f)\n",
    "\n",
    "best_model_path = training_result['best_model_checkpoint']\n",
    "ctm_hyperparameters = training_result['best_hyperparameters']['ctm_params']\n",
    "sbert_params = training_result['best_hyperparameters']['sbert_params']\n",
    "\n",
    "# ctm_hyperparameters['bow_size'] = 2000\n",
    "# ctm_hyperparameters['contextual_size'] = 768\n",
    "\n",
    "best_model = _load_ctm_model(Path(best_model_path), ctm_hyperparameters)\n",
    "\n",
    "# create the dataset on the fly\n",
    "vectorizer = pickle.load(open(Path(best_model_path).joinpath('count_vectorizer.pkl'), 'rb'))\n",
    "\n",
    "training_dataset, _, _, _ = create_ctm_dataset(\n",
    "    X, X_preprocessed, \n",
    "    sbert_params, training_folder,\n",
    "    vectorizer=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether the model is trained with sentence-split or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization\n",
    "\n",
    "with the same set of training dataset, we can visualize its topic modeling performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pyLDAVis alike graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis as vis\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "lda_vis_data = best_model.get_ldavis_data_format(vocab, training_dataset, n_samples=10)\n",
    "\n",
    "ctm_pd = vis.prepare(**lda_vis_data)\n",
    "vis.display(ctm_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top N keywords for each topic\n",
    "\n",
    "topic_list = best_model.get_topic_lists(10)\n",
    "\n",
    "topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_distribution = best_model.get_doc_topic_distribution(training_dataset, n_samples=20)\n",
    "\n",
    "top_docs = best_model.get_top_documents_per_topic_id(X, doc_topic_distribution, 8, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEGIT THE BEST GAME EVER BETTER THAN MINECRAFT, SO MANY THINGS TO DO!!!!!!! BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!BEST GAME EVER!!!\n",
      "Just realized I've had this game for years and never reviewed it. Which is just horrible of me. Of all the games in my steam library this deserves a review. I've enjoyed this game a lot. I've also enjoyed reading the games development storie. The dev stopped development on this game years ago. Was 'encouraged' by his girlfriend to update it. That encouragement must have been awesome. He released not only one more major update for it, but nominated a team of developers to continue work on it for long after that. They are still updating it. Just this month (November of 2016) they released another update that in many games would have been a entire paid DLC but in this game is just par for the course of another free update. After this game was released it has seen many changes and updates. Adding content. Not nerfiing content, but adding more content. Even takign the time to fix bugs a long the way. They game is great. The amount of content I have gotten for free since I have bought this game is amazing though. Most AAA developers might have released a patch for it after release. Maybe a few pieces of paid DLC. The developers on this have released multiple patches a year for years after it was released. Most those patches equivalent to a paid DLC from a AAA title. Not sayign this is the only way to dev a game. Just saying it is a damn nice one to see. Thanks.\n",
      "Simply one of the best indie exploration-sandbox game of all time? I'm writing this review after the HUGE update that brought so much to the game than, after hundreds of hours already spent on this game, I still felt I was up for another nth playthrough, this time in goddam painful 'Expert mode' (but I'm enjoying it!) Seriously, which developer cherish more his baby than Terraria's one? The game is out since 2011, and yet 4 years after, you just happen to get a ton of new content for free for the thord time, for a game that has already been bought by so many people here. There is no profit involved here, just one man's dedication for his game and his community. If all developers could be as much Terraria is simply one of the best gaming experience I ever had. - It's retro stylish, without being ugly nor botched. - Musics are great and stick to the head as you could expect on old videogames. - There are a lot of things to do to really beat the game. - Exploring is both entertaining and rewarding. - Classic combat style and yet so much diversified thanks to a ton of different weapons. - I never felt like I was grinding for something. - You can build so much things, with so much - It's just totally amazing to watch all the crazy/beautiful things the community already made with this game. - Multiplayer is such a wonderful experience here, and it's even better since they made it easier to organize. What else could we ask for? I would buy this again.\n",
      "If these side scroller retro crafting type games appeal to you at all, this is one of the best. Absolutely recommended. Tons of content for the price. Probably the best game I've ever bought for this price! And they just keep adding more content!! Edit/update: Every time I scroll past this game in my Steam library, it makes me smile. I can't wait for a sequel/expansion/major update. What a game!\n",
      "Terraria is a truly amazing game experience, over the years countless updates have been added to the game enhancing it above and beyond what was already a great game. Terraria is a great game if you enjoy action adventure, with the potential to also build your hearts desire (in a 2D space). If you enjoy these kinds of games and have not played Terraria I highly recommend it. Great value for price and still to this day the developer over at Redigit have continued to put out countless updates adding new content for free.\n",
      "A super fun and addictive Sandbox Crafting 2D Side Scrolling Retro RPG. The amount of content and work put in to this little indie game is astounding. Literally as many hours or more of gameplay as The Witcher 3 right up until endgame, not to mention the insane replayability all thanks to the major free content updates the devs provide us every few months. One of the best price/content ratio games on steam right now, i got it for around $4-5 CND when it went on sale one time and it's one of the best choices i made on steam. Trust me, this game is well worth your hard earned money if you are a fan of the genre. Pick it up, you won't be disappointed.\n",
      "Terraria is currently the game I've invested the longest amount of time into. When I first booted up the game in one of its early versions, I was one of the players that easily classified Terraria as 'Minecraft, minus one Of course, back then I would not have imagined the plethora of free updates and content Terraria would become throughout its development cycle. Re-Logic, Terraria's developers, have given the gaming community one of the most content-rich, sandbox-exploration games. Even after they began other projects, Re-Logic continues to surprise Terraria communities with new content years after Terraria's release. At the time of writing this review, the game version of Terraria has reached If there was ever an announcement for yet another massive update in the form of I wouldn't be surprised, but excited to start from scratch to enjoy the new additions.\n",
      "Fantastic game, every time I play it the developer has added new stuff (all for free, not microtransactions). I'm completely amazed by the amount of support the developer has given this game. One of the best games I've ever played, it's a 2D sidescrolling open world adventure game. It has hours on hours of content for a fantastically low price \n",
      "I've played through this game multiple times with multiple groups of varying size. It's fun even when playing alone, but with a group of people this is pretty much the best game involving crafting that you can pick up. It has tons of content, the combat system is much better than most games of this type, it runs on pretty much anything, it's regularly on sale, the soundtrack is great, and it still gets gigantic free content updates years after release. I own it on Steamand DRM-free on GoG and the Humble Store and I've bought multiple copies for friends. over the years. If you've spent more than 10$ on computer games in your life and don't have this one in your library, you've made a mistake. The only reason not to get this game if you have the chance is if you really despise the genre or have nobody to play it with. The developers deserve praise perhaps more than any other developer for their continued free support and the many community suggestions they've implemented. TL;DR: This game is absolutely amazing, and well worth the asking price. If you don't pick it up full-price, get it on one of the many sales. A blast to play with a group of people.\n",
      "Much more than a 2D Minecraft, Terraria blends one of the deepest crafting systems of all time, a charming graphical style, Metroidvania elements and epic boss fights together into a game that's more than the sum of its parts. And best of all, the game's only continued to improve, greatly deepening in new features, graphics, weapons, bosses, and even adding entirely new ways to play the game, years after the original release. Almost anyone else would sell the content they give out for free as expansion packs. Not this team. Terraria continues to grow, with the update approaching. While much smaller in scope than the expansion pack-sized it shows the continued dedication that exists to improving an already incredible game. Must buy.\n"
     ]
    }
   ],
   "source": [
    "for tt in [t[0] for t in top_docs]:\n",
    "    print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boss',\n",
       " 'buy',\n",
       " 'check',\n",
       " 'content',\n",
       " 'course',\n",
       " 'explore',\n",
       " 'felt',\n",
       " 'friend',\n",
       " 'fun',\n",
       " 'game',\n",
       " 'get',\n",
       " 'great',\n",
       " 'hour',\n",
       " 'item',\n",
       " 'like',\n",
       " 'list',\n",
       " 'love',\n",
       " 'mention',\n",
       " 'minecraft',\n",
       " 'new',\n",
       " 'number',\n",
       " 'one',\n",
       " 'play',\n",
       " 'recommend',\n",
       " 'say',\n",
       " 'special',\n",
       " 'spoil',\n",
       " 'still',\n",
       " 'stuff',\n",
       " 'terrarium',\n",
       " 'time',\n",
       " 'update',\n",
       " 'well',\n",
       " 'world']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# within the topic lists (the words)\n",
    "# find out common words between topics\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "topic_list = best_model.get_topic_lists(k=10)\n",
    "\n",
    "common_words = set()\n",
    "for topic1, topic2 in combinations(topic_list, 2):\n",
    "    common_words.update(set(topic1).intersection(set(topic2)))\n",
    "\n",
    "common_words = list(common_words)\n",
    "common_words.sort()\n",
    "common_words"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
