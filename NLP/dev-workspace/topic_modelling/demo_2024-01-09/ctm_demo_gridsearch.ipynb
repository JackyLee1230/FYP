{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo ipynb for CTM (hyperparameters grid/random search)\n",
    "\n",
    "Combined TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelcheng/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "# from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessingStopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 75499 entries, 57735 to 133233\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   index         75499 non-null  int64 \n",
      " 1   app_id        75499 non-null  int64 \n",
      " 2   app_name      75499 non-null  object\n",
      " 3   review_text   75499 non-null  object\n",
      " 4   review_score  75499 non-null  int64 \n",
      " 5   review_votes  75499 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path('../../dataset/topic_modelling/top_10_games/00_Terraria.pkl')\n",
    "\n",
    "dataset = pd.read_pickle(dataset_path)\n",
    "\n",
    "dataset.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../sa/')\n",
    "\n",
    "%autoreload 2\n",
    "import str_cleaning_functions\n",
    "\n",
    "# copied from lda_demo_gridsearch.ipynb\n",
    "def cleaning(df, review):\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links2(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.clean(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.deEmojify(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_non_letters(x))\n",
    "    df[review] = df[review].apply(lambda x: x.lower())\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_stopword(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))\n",
    "\n",
    "# def cleaning_strlist(str_list):\n",
    "#     str_list = list(map(lambda x: clean(x), str_list))\n",
    "#     str_list = list(map(lambda x: deEmojify(x), str_list))\n",
    "\n",
    "#     str_list = list(map(lambda x: x.lower(), str_list))\n",
    "#     str_list = list(map(lambda x: remove_num(x), str_list))\n",
    "#     str_list = list(map(lambda x: unify_whitespaces(x), str_list))\n",
    "\n",
    "#     str_list = list(map(lambda x: _deaccent(x), str_list))\n",
    "#     str_list = list(map(lambda x: remove_non_alphabets(x), str_list))\n",
    "#     str_list = list(map(lambda x: remove_stopword(x), str_list))\n",
    "#     return str_list\n",
    "\n",
    "# copied from bert_demo_gridsearch.ipynb\n",
    "def cleaning_little(df, review):\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.remove_links2(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.clean(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.deEmojify(x))\n",
    "    df[review] = df[review].apply(lambda x: str_cleaning_functions.unify_whitespaces(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the dataset, as we need both untouched text and cleaned text\n",
    "\n",
    "dataset_preprocessed = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning(dataset_preprocessed, 'review_text')\n",
    "\n",
    "\n",
    "cleaning_little(dataset, 'review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed = dataset_preprocessed['review_text'].values\n",
    "X = dataset['review_text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply lemmatizing to the preprocessed dataset as well (for BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do lemmatization, but not stemming (as part of speech is important in topic modelling)\n",
    "# use nltk wordnet for lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# from https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word\n",
    "\n",
    "# from: https://www.cnblogs.com/jclian91/p/9898511.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None     # if none -> created as noun by wordnet\n",
    "    \n",
    "def lemmatization(text):\n",
    "   # use nltk to get PoS tag\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "    # then we only need adj, adv, verb, noun\n",
    "    # convert from nltk Penn Treebank tag to wordnet tag\n",
    "    wn_tagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1])), tagged))\n",
    "\n",
    "    # lemmatize by the PoS\n",
    "    lemmatized = list(map(lambda x: lemma.lemmatize(x[0], pos=x[1] if x[1] else wordnet.NOUN), wn_tagged))\n",
    "    # lemma.lemmatize(wn_tagged[0], pos=wordnet.NOUN)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed = list(map(lambda x: lemmatization(x), X_preprocessed))\n",
    "X_preprocessed = list(map(lambda x: ' '.join(x), X_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce the BoW to 2000 as recommended by CTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referencing to preprocessing in CTM\n",
    "# to keep first 2000 words in the preprpcessed documents\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "VOCABULARY_SIZE = 2000\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=VOCABULARY_SIZE)\n",
    "vectorizer.fit_transform(X_preprocessed)\n",
    "temp_vocabulary = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
    "                            for doc in X_preprocessed]\n",
    "\n",
    "X_preprocessed = preprocessed_docs_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'werewolf rid unicorn rainbow gun build find hair spider cavern get sword shoot cat take lord moon use yoyo summon minion shoot enemy find sky temple air spawn buy music box wizard go record music like play base whenever want go build castle make entirely white would seem thing minecraft game dimension trust get use start game terrarium simply one satisfy sandbox experience may sound compare minecraft imagination'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# copy from: https://github.com/MilaNLProc/contextualized-topic-models/blob/master/contextualized_topic_models/utils/data_preparation.py#L44\n",
    "# call bert_embeddings_from_list() to produce embeddings by ourself\n",
    "\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "\n",
    "if platform.system() == 'Linux' or platform.system() == 'Windows':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('mps')        # m-series mac machine\n",
    "\n",
    "print(device)\n",
    "\n",
    "def bert_embeddings_from_list(\n",
    "    texts, sbert_model_to_load, batch_size=200, max_seq_length=128, device='cpu'):      # 128 is the default valule in TopicModelDataPreparation()\n",
    "    \"\"\"\n",
    "    Creates SBERT Embeddings from a list\n",
    "    \"\"\"\n",
    "\n",
    "    model = SentenceTransformer(sbert_model_to_load, device=device)\n",
    "\n",
    "    if max_seq_length is not None:\n",
    "        model.max_seq_length = max_seq_length\n",
    "\n",
    "    check_max_local_length(max_seq_length, texts)\n",
    "\n",
    "    return np.array(model.encode(texts, show_progress_bar=True))\n",
    "\n",
    "\n",
    "def check_max_local_length(max_seq_length, texts):\n",
    "    max_local_length = np.max([len(t.split()) for t in texts])\n",
    "    if max_local_length > max_seq_length:\n",
    "        warnings.simplefilter(\"always\", DeprecationWarning)\n",
    "        warnings.warn(\n",
    "            f\"the longest document in your collection has {max_local_length} words, the model instead \"\n",
    "            f\"truncates to {max_seq_length} tokens.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from eval_metrics import compute_inverted_rbo, compute_topic_diversity, compute_pairwise_jaccard_similarity, \\\n",
    "                        METRICS, SEARCH_BEHAVIOUR, COHERENCE_MODEL_METRICS"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
