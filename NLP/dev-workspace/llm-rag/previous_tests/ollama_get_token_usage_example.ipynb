{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A self-contained example to retrieve token usage of Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model='llama2:7b-chat-q4_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for detecting token usage\n",
    "\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.outputs.llm_result import LLMResult\n",
    "from collections import deque\n",
    "\n",
    "class TokenUsageCallbackHandler(BaseCallbackHandler):\n",
    "\n",
    "    def __init__(self, deque: deque = None):\n",
    "        super().__init__()\n",
    "        self.deque = deque\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        print('Response in callback')\n",
    "        print(response)\n",
    "        print()\n",
    "\n",
    "        generation = response.generations[0][0]\n",
    "        gen_info = generation.generation_info\n",
    "\n",
    "        # get token usage\n",
    "        # ref: https://github.com/orgs/langfuse/discussions/1179\n",
    "        token_usage = gen_info.get('prompt_eval_count', 0) + gen_info.get('eval_count', 0)\n",
    "        # get time costed (local machine)\n",
    "        # instead of getting total duration, we get the prompt_eval_duration and eval_duration to exclude the load duration (e.g. to load the model to the gpu, etc.)\n",
    "        time_costed = gen_info.get('prompt_eval_duration', 1e-10) + gen_info.get('eval_duration', 1e-10)     # in ns, a small value to indicate a inf time when it fails\n",
    "\n",
    "\n",
    "        # create an object to store the token usage and time costed\n",
    "        token_usage_obj = {\n",
    "            'token_usage': token_usage,\n",
    "            'time_costed': time_costed\n",
    "        }\n",
    "\n",
    "        # append the object to the deque\n",
    "        self.deque.append(token_usage_obj)\n",
    "\n",
    "\n",
    "\n",
    "common_deque = deque()\n",
    "chain_config = {\n",
    "    \"callbacks\": [TokenUsageCallbackHandler(common_deque)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response in callback\n",
      "generations=[[GenerationChunk(text='I\\'m just an AI assistant and do not have feelings or emotions, so I cannot answer the question \"How are you?\" as I do not have a physical body or personal experiences. However, I\\'m here to help you with any questions or tasks you may have! Is there anything specific you would like to know or discuss?', generation_info={'model': 'llama2:7b-chat-q4_0', 'created_at': '2024-03-25T09:49:19.476477Z', 'response': '', 'done': True, 'context': [518, 25580, 29962, 3532, 14816, 29903, 29958, 5299, 829, 14816, 29903, 6778, 13, 13, 10994, 29892, 920, 526, 366, 29973, 518, 29914, 25580, 29962, 13, 29902, 29915, 29885, 925, 385, 319, 29902, 20255, 322, 437, 451, 505, 21737, 470, 23023, 1080, 29892, 577, 306, 2609, 1234, 278, 1139, 376, 5328, 526, 366, 3026, 408, 306, 437, 451, 505, 263, 9128, 3573, 470, 7333, 27482, 29889, 2398, 29892, 306, 29915, 29885, 1244, 304, 1371, 366, 411, 738, 5155, 470, 9595, 366, 1122, 505, 29991, 1317, 727, 3099, 2702, 366, 723, 763, 304, 1073, 470, 5353, 29973], 'total_duration': 1295241083, 'load_duration': 4191291, 'prompt_eval_duration': 174427000, 'eval_count': 70, 'eval_duration': 1111013000})]] llm_output=None run=None\n",
      "\n",
      "I'm just an AI assistant and do not have feelings or emotions, so I cannot answer the question \"How are you?\" as I do not have a physical body or personal experiences. However, I'm here to help you with any questions or tasks you may have! Is there anything specific you would like to know or discuss?\n",
      "{'token_usage': 72, 'time_costed': 1310683000}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Hello, how are you?\", config=chain_config)\n",
    "\n",
    "# get the token usage object from the deque\n",
    "token_usage_obj = common_deque.popleft()\n",
    "\n",
    "print(response)\n",
    "print(token_usage_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response in callback\n",
      "generations=[[GenerationChunk(text='\\nOf course! Large language models are a class of artificial intelligence (AI) models that are trained on vast amounts of text data to generate language outputs that are coherent and natural-sounding. These models have become increasingly popular in recent years due to their ability to generate text that is often indistinguishable from human-generated text.\\n\\nThe basic idea behind large language models is to use a type of neural network called a transformer to learn the patterns and structures of language. The transformer is trained on a massive dataset of text, which can come from any source, such as books, articles, or even social media posts. The model learns to predict the next word in a sequence of text given the previous words, based on the patterns it has observed in the training data.\\n\\nLarge language models have many applications, such as:\\n\\n1. Language Translation: Large language models can be trained to translate text from one language to another. This is done by training the model on a large dataset of text in the source language and the corresponding translations in the target language.\\n2. Text Summarization: Large language models can be used to summarize long pieces of text, such as articles or documents, into shorter summaries that capture the main points.\\n3. Chatbots: Large language models can be used to power chatbots and other conversational AI systems, allowing them to understand and respond to user input in a more natural and human-like way.\\n4. Content Generation: Large language models can be used to generate content, such as articles, blog posts, or even entire books, based on a given prompt or topic.\\n5. Language Understanding: Large language models can be used to understand the meaning of text, allowing them to perform tasks such as sentiment analysis, question answering, and text classification.\\n\\nSome examples of large language models include:\\n\\n1. BERT (Bidirectional Encoder Representations from Transformers): A popular large language model developed by Google that has achieved state-of-the-art results in a wide range of natural language processing tasks.\\n2. RoBERTa (Robustly Optimized BERT Pretraining Approach): A variant of BERT that was specifically designed for text classification tasks and has achieved better results than BERT in some cases.\\n3. Transformer: A type of neural network architecture that is particularly well-suited to processing sequential data, such as text. Transformers are used in many large language models and have achieved state-of-the-art results in a wide range of natural language processing tasks.\\n4. LLaMA (LLaMA Large Language Model): A large language model developed by Meta AI that has achieved state-of-the-art results in a wide range of natural language processing tasks, including text generation and language translation.\\n\\nOverall, large language models have the potential to revolutionize many areas of natural language processing, from language translation to content generation and beyond.', generation_info={'model': 'llama2:7b-chat-q4_0', 'created_at': '2024-03-25T09:50:36.953917Z', 'response': '', 'done': True, 'context': [518, 25580, 29962, 3532, 14816, 29903, 29958, 5299, 829, 14816, 29903, 6778, 13, 13, 29950, 7889, 29901, 5199, 13, 29950, 7889, 29901, 12027, 7420, 8218, 479, 17088, 3382, 1379, 304, 592, 29889, 518, 29914, 25580, 29962, 13, 13, 2776, 3236, 29991, 8218, 479, 4086, 4733, 526, 263, 770, 310, 23116, 21082, 313, 23869, 29897, 4733, 393, 526, 16370, 373, 13426, 26999, 310, 1426, 848, 304, 5706, 4086, 14391, 393, 526, 16165, 261, 296, 322, 5613, 29899, 29879, 12449, 29889, 4525, 4733, 505, 4953, 10231, 368, 5972, 297, 7786, 2440, 2861, 304, 1009, 11509, 304, 5706, 1426, 393, 338, 4049, 1399, 391, 6202, 728, 519, 515, 5199, 29899, 13525, 1426, 29889, 13, 13, 1576, 6996, 2969, 5742, 2919, 4086, 4733, 338, 304, 671, 263, 1134, 310, 19677, 3564, 2000, 263, 4327, 261, 304, 5110, 278, 15038, 322, 12286, 310, 4086, 29889, 450, 4327, 261, 338, 16370, 373, 263, 20364, 8783, 310, 1426, 29892, 607, 508, 2041, 515, 738, 2752, 29892, 1316, 408, 8277, 29892, 7456, 29892, 470, 1584, 5264, 5745, 11803, 29889, 450, 1904, 24298, 1983, 304, 8500, 278, 2446, 1734, 297, 263, 5665, 310, 1426, 2183, 278, 3517, 3838, 29892, 2729, 373, 278, 15038, 372, 756, 8900, 297, 278, 6694, 848, 29889, 13, 13, 24105, 479, 4086, 4733, 505, 1784, 8324, 29892, 1316, 408, 29901, 13, 13, 29896, 29889, 17088, 4103, 18411, 29901, 8218, 479, 4086, 4733, 508, 367, 16370, 304, 14240, 1426, 515, 697, 4086, 304, 1790, 29889, 910, 338, 2309, 491, 6694, 278, 1904, 373, 263, 2919, 8783, 310, 1426, 297, 278, 2752, 4086, 322, 278, 6590, 5578, 800, 297, 278, 3646, 4086, 29889, 13, 29906, 29889, 3992, 6991, 3034, 2133, 29901, 8218, 479, 4086, 4733, 508, 367, 1304, 304, 19138, 675, 1472, 12785, 310, 1426, 29892, 1316, 408, 7456, 470, 10701, 29892, 964, 20511, 19138, 583, 393, 10446, 278, 1667, 3291, 29889, 13, 29941, 29889, 678, 271, 29890, 1862, 29901, 8218, 479, 4086, 4733, 508, 367, 1304, 304, 3081, 13563, 29890, 1862, 322, 916, 9678, 1288, 319, 29902, 6757, 29892, 14372, 963, 304, 2274, 322, 10049, 304, 1404, 1881, 297, 263, 901, 5613, 322, 5199, 29899, 4561, 982, 29889, 13, 29946, 29889, 10576, 28203, 29901, 8218, 479, 4086, 4733, 508, 367, 1304, 304, 5706, 2793, 29892, 1316, 408, 7456, 29892, 12618, 11803, 29892, 470, 1584, 4152, 8277, 29892, 2729, 373, 263, 2183, 9508, 470, 11261, 29889, 13, 29945, 29889, 17088, 7634, 11235, 29901, 8218, 479, 4086, 4733, 508, 367, 1304, 304, 2274, 278, 6593, 310, 1426, 29892, 14372, 963, 304, 2189, 9595, 1316, 408, 19688, 7418, 29892, 1139, 22862, 29892, 322, 1426, 12965, 29889, 13, 13, 9526, 6455, 310, 2919, 4086, 4733, 3160, 29901, 13, 13, 29896, 29889, 350, 20161, 313, 29933, 333, 8684, 284, 11346, 6119, 16314, 800, 515, 4103, 689, 414, 1125, 319, 5972, 2919, 4086, 1904, 8906, 491, 5087, 393, 756, 14363, 2106, 29899, 974, 29899, 1552, 29899, 442, 2582, 297, 263, 9377, 3464, 310, 5613, 4086, 9068, 9595, 29889, 13, 29906, 29889, 1528, 13635, 29911, 29874, 313, 21860, 504, 368, 20693, 326, 1891, 350, 20161, 349, 2267, 336, 2827, 28268, 496, 1125, 319, 17305, 310, 350, 20161, 393, 471, 10816, 8688, 363, 1426, 12965, 9595, 322, 756, 14363, 2253, 2582, 1135, 350, 20161, 297, 777, 4251, 29889, 13, 29941, 29889, 4103, 24784, 29901, 319, 1134, 310, 19677, 3564, 11258, 393, 338, 10734, 1532, 29899, 2146, 1573, 304, 9068, 8617, 2556, 848, 29892, 1316, 408, 1426, 29889, 4103, 689, 414, 526, 1304, 297, 1784, 2919, 4086, 4733, 322, 505, 14363, 2106, 29899, 974, 29899, 1552, 29899, 442, 2582, 297, 263, 9377, 3464, 310, 5613, 4086, 9068, 9595, 29889, 13, 29946, 29889, 365, 5661, 1529, 313, 2208, 29874, 1529, 8218, 479, 17088, 8125, 1125, 319, 2919, 4086, 1904, 8906, 491, 20553, 319, 29902, 393, 756, 14363, 2106, 29899, 974, 29899, 1552, 29899, 442, 2582, 297, 263, 9377, 3464, 310, 5613, 4086, 9068, 9595, 29892, 3704, 1426, 12623, 322, 4086, 13962, 29889, 13, 13, 3563, 497, 29892, 2919, 4086, 4733, 505, 278, 7037, 304, 19479, 675, 1784, 10161, 310, 5613, 4086, 9068, 29892, 515, 4086, 13962, 304, 2793, 12623, 322, 8724, 29889], 'total_duration': 10952151083, 'load_duration': 1843708, 'prompt_eval_count': 23, 'prompt_eval_duration': 290857000, 'eval_count': 639, 'eval_duration': 10656564000})]] llm_output=None run=None\n",
      "\n",
      "\n",
      "Of course! Large language models are a class of artificial intelligence (AI) models that are trained on vast amounts of text data to generate language outputs that are coherent and natural-sounding. These models have become increasingly popular in recent years due to their ability to generate text that is often indistinguishable from human-generated text.\n",
      "\n",
      "The basic idea behind large language models is to use a type of neural network called a transformer to learn the patterns and structures of language. The transformer is trained on a massive dataset of text, which can come from any source, such as books, articles, or even social media posts. The model learns to predict the next word in a sequence of text given the previous words, based on the patterns it has observed in the training data.\n",
      "\n",
      "Large language models have many applications, such as:\n",
      "\n",
      "1. Language Translation: Large language models can be trained to translate text from one language to another. This is done by training the model on a large dataset of text in the source language and the corresponding translations in the target language.\n",
      "2. Text Summarization: Large language models can be used to summarize long pieces of text, such as articles or documents, into shorter summaries that capture the main points.\n",
      "3. Chatbots: Large language models can be used to power chatbots and other conversational AI systems, allowing them to understand and respond to user input in a more natural and human-like way.\n",
      "4. Content Generation: Large language models can be used to generate content, such as articles, blog posts, or even entire books, based on a given prompt or topic.\n",
      "5. Language Understanding: Large language models can be used to understand the meaning of text, allowing them to perform tasks such as sentiment analysis, question answering, and text classification.\n",
      "\n",
      "Some examples of large language models include:\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers): A popular large language model developed by Google that has achieved state-of-the-art results in a wide range of natural language processing tasks.\n",
      "2. RoBERTa (Robustly Optimized BERT Pretraining Approach): A variant of BERT that was specifically designed for text classification tasks and has achieved better results than BERT in some cases.\n",
      "3. Transformer: A type of neural network architecture that is particularly well-suited to processing sequential data, such as text. Transformers are used in many large language models and have achieved state-of-the-art results in a wide range of natural language processing tasks.\n",
      "4. LLaMA (LLaMA Large Language Model): A large language model developed by Meta AI that has achieved state-of-the-art results in a wide range of natural language processing tasks, including text generation and language translation.\n",
      "\n",
      "Overall, large language models have the potential to revolutionize many areas of natural language processing, from language translation to content generation and beyond.\n",
      "{'token_usage': 70, 'time_costed': 1285440000}\n"
     ]
    }
   ],
   "source": [
    "# create a chain and invoke\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    (\"human\", \"Explain {concept} to me.\")\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"concept\": \"Large Language Models\"}, config=chain_config)\n",
    "\n",
    "# get the token usage object from the deque\n",
    "token_usage_obj = common_deque.popleft()\n",
    "\n",
    "print(response)\n",
    "print(token_usage_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-test-tm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
