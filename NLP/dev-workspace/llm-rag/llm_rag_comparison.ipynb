{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _sample_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_ID': 123, 'review_text': 'Great time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend', 'datetime': datetime.datetime(2024, 3, 21, 13, 17, 51, 4601), 'hash': 'b40752733fe1d3059228a21bd596ee23e16e85e4b25f7138027da300'}\n",
      "\n",
      "\n",
      "\n",
      "Len of review_text: 20\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from hashlib import sha224\n",
    "\n",
    "# create an object with review_ID, review_text and the datetime of creation\n",
    "def create_review(review_text, review_ID):    \n",
    "    review_obj = {\n",
    "        \"review_ID\": review_ID,\n",
    "        \"review_text\": review_text,\n",
    "        \"datetime\": datetime.now()\n",
    "    }\n",
    "\n",
    "    # create a hash of the review_text for creating unique collection with Chromadb\n",
    "    hash = sha224(str(review_obj).encode()).hexdigest()\n",
    "    review_obj['hash'] = hash\n",
    "\n",
    "    return review_obj\n",
    "\n",
    "# the temporary review_ID\n",
    "review_ID = 123\n",
    "# select a comment to test with\n",
    "review_text = _sample_reviews.sample_fyp_433\n",
    "review_obj = create_review(review_text, review_ID)\n",
    "\n",
    "print(review_obj)\n",
    "print('\\n\\n')\n",
    "print('Len of review_text:', len(review_text.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the llm to run with (different LLMs)\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm_mistral7b = Ollama(\n",
    "    model='mistral:7b-instruct-v0.2-q4_0', temperature=0.4,\n",
    "    num_gpu = 1,        # disable/enable gpu for testing\n",
    ")\n",
    "\n",
    "llm_mixtral8x7b = Ollama(\n",
    "    model='mixtral:8x7b-instruct-v0.1-q4_0', temperature=0.4,\n",
    "    num_gpu = 1,        # disable/enable gpu for testing\n",
    ")\n",
    "\n",
    "llm_llama2_7b = Ollama(\n",
    "    model='llama2:7b-chat-q4_0', temperature=0.4, num_gpu = 1\n",
    ")\n",
    "# llm_gemma_7b = Ollama()\n",
    "# llm_gemma_2b = Ollama()\n",
    "\n",
    "# for looping through the LLM models\n",
    "Llm_models = {\n",
    "    'mistral_7b': llm_mistral7b,\n",
    "    # 'mixtral_8x7b': llm_mixtral8x7b,\n",
    "    'llama2_7b': llm_llama2_7b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelcheng/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading tokenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 270kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 657kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 794kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 373kB/s]\n"
     ]
    }
   ],
   "source": [
    "# for embedding in testing, we use sentence transformer\n",
    "# sentence transformer embed text into vector space with dimension 384 for semantic search.\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embedding_func = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def custom_len_func(text:str) -> int:\n",
    "  return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "text_spliter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "  tokenizer=tokenizer,\n",
    "  chunk_size=250,\n",
    "  chunk_overlap=40,\n",
    ")\n",
    "\n",
    "docs = text_spliter.create_documents(\n",
    "  [review_obj['review_text']], metadatas=[{\"source\":\"review_01\"}]\n",
    ")\n",
    "\n",
    "# define in-memory db and retriever for RAG\n",
    "db = Chroma.from_documents(documents=docs, embedding=embedding_func, collection_name=review_obj['hash'])\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for detecting token usage\n",
    "\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.outputs.llm_result import LLMResult\n",
    "from collections import deque\n",
    "\n",
    "class TokenUsageCallbackHandler(BaseCallbackHandler):\n",
    "\n",
    "    def __init__(self, deque: deque = None):\n",
    "        super().__init__()\n",
    "        self.deque = deque\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        # print(response)\n",
    "\n",
    "        generation = response.generations[0][0]\n",
    "        gen_info = generation.generation_info\n",
    "\n",
    "        # get token usage\n",
    "        # ref: https://github.com/orgs/langfuse/discussions/1179\n",
    "        token_usage = gen_info.get('prompt_eval_count', 0) + gen_info.get('eval_count', 0)\n",
    "        # get time costed (local machine)\n",
    "        # instead of getting total duration, we get the prompt_eval_duration and eval_duration to exclude the load duration (e.g. to load the model to the gpu, etc.)\n",
    "        time_costed = gen_info.get('prompt_eval_duration', 1e-10) + gen_info.get('eval_duration', 1e-10)     # in ns, a small value to indicate a inf time when it fails\n",
    "\n",
    "\n",
    "        # create an object to store the token usage and time costed\n",
    "        token_usage_obj = {\n",
    "            'token_usage': token_usage,\n",
    "            'time_costed': time_costed\n",
    "        }\n",
    "\n",
    "        # append the object to the deque\n",
    "        self.deque.append(token_usage_obj)\n",
    "\n",
    "\n",
    "\n",
    "common_deque = deque()\n",
    "chain_config = {\n",
    "    \"callbacks\": [TokenUsageCallbackHandler(common_deque)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_token_usage_obj(token_usage_obj_list:list[dict]) -> dict:\n",
    "    token_usage = 0\n",
    "    time_costed = 0\n",
    "    for obj in token_usage_obj_list:\n",
    "        token_usage += obj['token_usage']\n",
    "        time_costed += obj['time_costed']\n",
    "\n",
    "    return {\n",
    "        'token_usage': token_usage,\n",
    "        'time_costed': time_costed\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Spam detection\n",
    "\n",
    "A chain to first prompt reasons and a pre-mature decision of whether the review is a prompt, then a second prompt is used to determine whether the review is a prompt.\n",
    "\n",
    "Evaluation: accuracy of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "def prompt_spam_detection(llm_model_name:str, llm_model):\n",
    "\n",
    "    chat_prompt_01 = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", _prompts.SYSTEM_TEMPLATE),\n",
    "        (\"user\", _prompts.SPAM_TEMPLATE_01)\n",
    "    ])\n",
    "\n",
    "    chain_01 = chat_prompt_01 | llm_model\n",
    "\n",
    "    resp_01 = chain_01.invoke({\n",
    "        \"review\":str(review_text)\n",
    "    }, config=chain_config)\n",
    "\n",
    "    token_usage_01 = common_deque.popleft()\n",
    "\n",
    "    chat_prompt_02 = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", _prompts.SYSTEM_TEMPLATE),\n",
    "        (\"user\", _prompts.SPAM_TEMPLATE_01),\n",
    "        (\"ai\", resp_01),\n",
    "        (\"user\", _prompts.SPAM_TEMPLATE_02)\n",
    "    ])\n",
    "\n",
    "    chain_02 = chat_prompt_02 | llm_model\n",
    "\n",
    "    resp_02 = chain_02.invoke({\n",
    "        \"review\":str(review_text)\n",
    "    }, config=chain_config)\n",
    "\n",
    "    token_usage_02 = common_deque.popleft()\n",
    "\n",
    "\n",
    "    # get token usage result and time costed\n",
    "    token_usage_result = combine_token_usage_obj([token_usage_01, token_usage_02])\n",
    "\n",
    "    # show result:\n",
    "    print('-' * 10 + f'Result for {llm_model_name}' + '-' * 10)\n",
    "    print(resp_01); print(); print(resp_02)\n",
    "    print('Token usage:', token_usage_result['token_usage'])\n",
    "    print('Time costed: {:.04} s'.format(float(token_usage_result['time_costed']) / 1e9))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    return {\n",
    "        'resp_01': resp_01,\n",
    "        'resp_02': resp_02,\n",
    "        'token_usage': token_usage_result['token_usage'],\n",
    "        'time_costed': token_usage_result['time_costed']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Result for mistral_7b----------\n",
      " 1. The review contains positive sentiment towards the game, which is not unusual or suspicious in itself. However, it's quite short and lacks specific details that would typically be found in a more informative review.\n",
      "2. The review mentions playing the game on both console and PC, but it does not provide any comparison or indication of why the reviewer is trying to switch platforms. This lack of context could potentially raise suspicions if this pattern were consistent across multiple reviews.\n",
      "3. The review ends with a recommendation, which is a common feature in genuine reviews. However, the absence of any additional information (such as the game's name or genre) makes it difficult to verify the authenticity of the recommendation.\n",
      "\n",
      " NO.\n",
      "Token usage: 469\n",
      "Time costed: 3.282 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for llama2_7b----------\n",
      "Based on the provided review, here are three reasons why I believe it is not a spam for a game:\n",
      "\n",
      "1. The reviewer mentions playing the game on both console and PC, indicating that they have experience with the game on multiple platforms. Spammers often focus on one platform or another to try to manipulate users into buying their product.\n",
      "2. The reviewer uses phrases like \"Great time!\" and \"Highly recommend,\" which suggest a genuine enthusiasm for the game. Spammy language is often more generic and lacking in passion.\n",
      "3. The reviewer mentions collecting the game on PC, which suggests that they are interested in the game beyond just playing it casually. This implies that they have invested time and effort into the game, which is not typical of spammers who are only trying to sell something without providing any context or depth.\n",
      "\n",
      "NO\n",
      "Token usage: 556\n",
      "Time costed: 3.845 s\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop over the LLM models\n",
    "\n",
    "spam_detection_result = {}\n",
    "\n",
    "for llm_model_name, llm_model in Llm_models.items():\n",
    "    spam_detection_result[llm_model_name] = prompt_spam_detection(llm_model_name, llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Summarization (content extraction with RAG)\n",
    "\n",
    "Extract related content with vector storage and write a summary for the extracted aspects\n",
    "\n",
    "Then output a JSON with the aspect as key, and the summary as value\n",
    "\n",
    "Evaluation: hallucination, relevence of the summaries, ability to ahere with the output format.\n",
    "\n",
    "We test with 10 aspects. Different combinations in wrapping the aspects were tested before, and (3, 3, 4) balances token efficiency and output quality the best.\n",
    "\n",
    "To evaluate the ability to ahere with the output format, we run the same prompt 5 times and observe the output consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_ASPECTS = ['Gameplay', 'Narrative', 'Accessibility', 'Sound', 'Graphics & Art Design', 'Performance', 'Bug', 'Suggestion', 'Price', 'Overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_aspect_extraction(llm_model_name:str, llm_model, repeat_count=5):\n",
    "\n",
    "    # aspects_response = {k: '' for k in GAME_ASPECTS}\n",
    "    responses = {}\n",
    "\n",
    "    for (start, end) in [(0, 3), (3, 6), (6, 10)]:\n",
    "        aspects = GAME_ASPECTS[start:end]\n",
    "        responses[str(aspects)] = {}        # storing the result\n",
    "\n",
    "        rag_question = _prompts.QUESTION_TEMPLATE_01 + f\"{'is ' if len(aspects) <= 1 else 'are '}\" + ': ' + f'{aspects}'\n",
    "        output_format_tempate = _prompts.OUTPUT_FORMAT_TEMPATE.format(\n",
    "            aspects_list_01=str(aspects)[1:-1].replace('\\'', '\\\"'), output_json_template=str({k: '...' for k in aspects}).replace('\\'', '\\\"')\n",
    "        )\n",
    "\n",
    "        relevant_docs = retriever.get_relevant_documents(query=rag_question, k=5)\n",
    "\n",
    "        for i in range(repeat_count):\n",
    "\n",
    "            chat_prompt_01 = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", _prompts.SYSTEM_TEMPLATE),\n",
    "                (\"user\", _prompts.KEYWORD_TEMPLATE_01)\n",
    "            ])\n",
    "\n",
    "            chain_01 = chat_prompt_01 | llm_model\n",
    "\n",
    "            print('The prompt:', chat_prompt_01.format_messages(\n",
    "                aspects = str(aspects),\n",
    "                output_format = output_format_tempate,\n",
    "                summaries = str('\\n'.join([d.page_content for d in relevant_docs]))\n",
    "            ))\n",
    "\n",
    "            resp_01 = chain_01.invoke({\n",
    "                \"aspects\":str(aspects),\n",
    "                \"output_format\":output_format_tempate,\n",
    "                \"summaries\": str('\\n'.join([d.page_content for d in relevant_docs]))\n",
    "            }, config=chain_config)\n",
    "\n",
    "            token_usage_01 = common_deque.popleft()\n",
    "\n",
    "            # get token usage result and time costed\n",
    "            token_usage_result = combine_token_usage_obj([token_usage_01])\n",
    "\n",
    "            # show result:\n",
    "            print('-' * 10 + f'Result for {llm_model_name} at attempt={i+1}' + '-' * 10)\n",
    "            print(resp_01); print()\n",
    "            print('Token usage:', token_usage_result['token_usage'])\n",
    "            print('Time costed: {:.04} s'.format(float(token_usage_result['time_costed']) / 1e9))\n",
    "            print('\\n\\n')\n",
    "\n",
    "            responses[str(aspects)][f'attempt_{i}'] = {\n",
    "                'resp_01': resp_01,\n",
    "                'token_usage': token_usage_result['token_usage'],\n",
    "                'time_costed': token_usage_result['time_costed']\n",
    "            }\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=1----------\n",
      " {\"Gameplay\": \"Played many hours on console, recommending a switch to PC\", \"Narrative\": \"NA\", \"Accessibility\": \"NA\"}\n",
      "\n",
      "Token usage: 287\n",
      "Time costed: 1.094 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=2----------\n",
      " {\"Gameplay\": \"Played many hours on console, recommending the game\", \"Narrative\": \"NA\", \"Accessibility\": \"NA\"}\n",
      "\n",
      "Token usage: 32\n",
      "Time costed: 0.5953 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=3----------\n",
      " {\"Gameplay\": \"Played many hours on console, recommending the game\", \"Narrative\": \"NA\", \"Accessibility\": \"NA\"}\n",
      "\n",
      "Token usage: 32\n",
      "Time costed: 0.5549 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=4----------\n",
      " {\"Gameplay\": \"Played many hours on console, recommending the game\", \"Narrative\": \"NA\", \"Accessibility\": \"NA\"}\n",
      "\n",
      "Token usage: 32\n",
      "Time costed: 0.5568 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Result for mistral_7b at attempt=5----------\n",
      " {\"Gameplay\": \"Played many hours on console, recommending the game.\", \"Narrative\": \"NA\", \"Accessibility\": \"NA\"}\n",
      "\n",
      "Token usage: 32\n",
      "Time costed: 0.5554 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=1----------\n",
      " {\"Sound\": \"Review mentions playing the game for many hours on console, implying satisfactory sound experience.\", \"Graphics & Art Design\": \"No specific mention in review.\", \"Performance\": \"No mention of performance in review.\"}\n",
      "\n",
      "Token usage: 231\n",
      "Time costed: 1.243 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=2----------\n",
      " {\"Sound\": \"Review mentions playing the game for many hours on console without specifying anything about the sound.\",\n",
      " \"Graphics & Art Design\": \"No explicit mention of graphics or art design in the review.\",\n",
      " \"Performance\": \"NA\"}\n",
      "\n",
      "Token usage: 51\n",
      "Time costed: 0.9039 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=3----------\n",
      " {\"Sound\": \"Review mentions playing for many hours on console implying satisfactory sound experience.\", \"Graphics & Art Design\": \"No specific mention in this review.\", \"Performance\": \"Switching from console to PC suggests potential performance improvement, but no definitive statement made.\"}\n",
      "\n",
      "Token usage: 58\n",
      "Time costed: 0.9946 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=4----------\n",
      " {\"Sound\": \"Review mentions playing the game for many hours on console.\", \"Graphics & Art Design\": \"Na\", \"Performance\": \"Switching collection from console to PC\"}\n",
      "\n",
      "Or, if we assume that the user's statement about trying to get their collection to switch to PC is related to performance:\n",
      "\n",
      "{\"Sound\": \"Review mentions playing the game for many hours on console.\", \"Graphics & Art Design\": \"Na\", \"Performance\": \"User is attempting to improve game experience on PC\"}\n",
      "\n",
      "Token usage: 108\n",
      "Time costed: 1.832 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Result for mistral_7b at attempt=5----------\n",
      " {\"Sound\": \"Review mentions playing the game for many hours on console without specifying anything about the sound.\",\n",
      " \"Graphics & Art Design\": \"No explicit mention of graphics or art design in this review.\",\n",
      " \"Performance\": \"NA\"}\n",
      "\n",
      "Token usage: 51\n",
      "Time costed: 0.8704 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=1----------\n",
      " {\"Bug\": \"NA\", \"Suggestion\": \"NA\", \"Price\": \"NA\", \"Overall\": \"Great time! played many hours on console first! highly recommend\"}\n",
      "\n",
      "Token usage: 233\n",
      "Time costed: 1.177 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=2----------\n",
      " {\"Bug\": \"NA\", \"Suggestion\": \"NA\", \"Price\": \"The reviewer does not mention the price.\", \"Overall\": \"The reviewer highly recommends the game.\"}\n",
      "\n",
      "Token usage: 43\n",
      "Time costed: 0.7402 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=3----------\n",
      " {\"Bug\": \"NA\", \"Suggestion\": \"Highly recommend switching from console to PC\", \"Price\": \"NA\", \"Overall\": \"Great time, played many hours\"}\n",
      "\n",
      "Token usage: 40\n",
      "Time costed: 0.7023 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for mistral_7b at attempt=4----------\n",
      " {\"Bug\": \"NA\", \"Suggestion\": \"Highly recommend switching from console to PC\", \"Price\": \"NA\", \"Overall\": \"Great time, played many hours\"}\n",
      "\n",
      "Token usage: 40\n",
      "Time costed: 0.7071 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Result for mistral_7b at attempt=5----------\n",
      " {\"Bug\": \"NA\", \"Suggestion\": \"Highly recommend switching from console to PC version\", \"Price\": \"NA\", \"Overall\": \"Great time played many hours on console\"}\n",
      "\n",
      "Token usage: 42\n",
      "Time costed: 0.7424 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=1----------\n",
      "{\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\n",
      "\n",
      "Token usage: 281\n",
      "Time costed: 0.8586 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=2----------\n",
      "{\"Gameplay\": \"...Great time! played many hours on console first! now trying to get my collection to switch to pc...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\n",
      "\n",
      "Token usage: 41\n",
      "Time costed: 0.6955 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=3----------\n",
      "{\"Gameplay\": \"...Great time! played many hours on console first! now trying to get my collection to switch to pc...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\n",
      "\n",
      "Token usage: 41\n",
      "Time costed: 0.7567 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=4----------\n",
      "{\"Gameplay\": \"...Great time! played many hours on console first! now trying to get my collection to switch to pc.\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\n",
      "\n",
      "Token usage: 40\n",
      "Time costed: 0.6678 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Gameplay\\', \\'Narrative\\', \\'Accessibility\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Gameplay\", \"Narrative\", \"Accessibility\", the JSON should be: {\"Gameplay\": \"...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Result for llama2_7b at attempt=5----------\n",
      "{\"Gameplay\": \"...Great time! played many hours on console first! now trying to get my collection to switch to pc...\", \"Narrative\": \"...\", \"Accessibility\": \"...\"}\n",
      "\n",
      "Token usage: 41\n",
      "Time costed: 0.6909 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=1----------\n",
      "{\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\n",
      "\n",
      "Token usage: 200\n",
      "Time costed: 0.7195 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=2----------\n",
      "{\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\n",
      "\n",
      "Token usage: 19\n",
      "Time costed: 0.3277 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=3----------\n",
      "{\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\n",
      "\n",
      "Token usage: 19\n",
      "Time costed: 0.3634 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=4----------\n",
      "{\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\n",
      "\n",
      "Token usage: 19\n",
      "Time costed: 0.3279 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Sound\\', \\'Graphics & Art Design\\', \\'Performance\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Sound\", \"Graphics & Art Design\", \"Performance\", the JSON should be: {\"Sound\": \"...\", \"Graphics & Art Design\": \"...\", \"Performance\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Result for llama2_7b at attempt=5----------\n",
      "{\"Sound\": \"Great soundtrack and immersive audio experience.\", \"Graphics & Art Design\": \"Visually stunning with detailed character designs and environments.\", \"Performance\": \"Smooth gameplay with minimal lag or glitches.\"}\n",
      "\n",
      "Token usage: 51\n",
      "Time costed: 0.8594 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=1----------\n",
      "{\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"Great time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\"}\n",
      "\n",
      "Token usage: 241\n",
      "Time costed: 1.219 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=2----------\n",
      "{\"Bug\": \"NA\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"Highly recommend\"}\n",
      "\n",
      "Token usage: 28\n",
      "Time costed: 0.5248 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=3----------\n",
      "{\"Bug\": \"NA\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"Highly recommend\"}\n",
      "\n",
      "Token usage: 28\n",
      "Time costed: 0.4912 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=4----------\n",
      "{\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\n",
      "\n",
      "Token usage: 24\n",
      "Time costed: 0.4109 s\n",
      "\n",
      "\n",
      "\n",
      "The prompt: [SystemMessage(content=\"You are reading reviews of a game to understand the characteristics of the game. Use the following pieces of context to answer user's question.\"), HumanMessage(content='You are reading reviews of a game to understand the characteristics of the game. Extract the following aspect of the game from the reviews.\\nThe aspects are [\\'Bug\\', \\'Suggestion\\', \\'Price\\', \\'Overall\\']. For each aspect, output a paragraph with less than 50 words. Then create a JSON with apsects name as key and the paragraph as value.\\nOutput the JSON as a single line with no spaces between the key, value pairs. For example, if the aspects are \"Bug\", \"Suggestion\", \"Price\", \"Overall\", the JSON should be: {\"Bug\": \"...\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"...\"}\\nOnly output the JSON. Do NOT output other text.\\n\\nGreat time! played many hours on console first! now trying to get my collection to switch to pc. Highly recommend\\n\\nIf you don\\'t know the answer, output only \"NA\". Do NOT try to make up an answer. Do NOT output other text.')]\n",
      "----------Result for llama2_7b at attempt=5----------\n",
      "{\"Bug\": \"NA\", \"Suggestion\": \"...\", \"Price\": \"...\", \"Overall\": \"Highly recommend\"}\n",
      "\n",
      "Token usage: 28\n",
      "Time costed: 0.4971 s\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect_extraction_result = {}\n",
    "\n",
    "for llm_model_name, llm_model in Llm_models.items():\n",
    "    aspect_extraction_result[llm_model_name] = prompt_aspect_extraction(llm_model_name, llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: keyword generation\n",
    "\n",
    "Given the extracted summary from task 2, we asked LLM to generate not more than 5 keywords/keyphrase to describe each aspect. Also output a JSON object, at once.\n",
    "\n",
    "However, if there is no content (or NA) for an aspect, there should be no keyword, or only 'NA' for that aspect\n",
    "\n",
    "Evaluation: hallucination, relevent of the keywords to the generated summaries, ability to ahere with the output format.\n",
    "\n",
    "We have least strigent output format for that (there's no need to pass a output format for this task), as passing one will shift the attention towards the output format, ignoring the context. Also, the context itself is a good example of the expected output format.\n",
    "\n",
    "Passing 10 aspects at once.\n",
    "\n",
    "In our application, task 2 and task 3 is chained (i.e. response from task 2 will be directly applied to task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we provide 10 generated aspects response from different reviews\n",
    "# instead of using the response from different models, we believed this is more controlled and thus better evaluate the performance of the models\n",
    "# the aspects response are first generated using mixtral8x7b and then manually reviewed and corrected to ensure the quality of the response\n",
    "\n",
    "import _aspects_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_aspect_keyword_extraction(llm_model_name:str, llm_model, aspects_response:dict, repeat_count=5):\n",
    "\n",
    "    responses = {}\n",
    "\n",
    "    for i in range(repeat_count):\n",
    "\n",
    "        chat_prompt_01 = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", _prompts.SYSTEM_TEMPLATE),\n",
    "            (\"user\", _prompts.KEYWORD_TEMPLATE_02)\n",
    "        ])\n",
    "\n",
    "        chain_01 = chat_prompt_01 | llm_model\n",
    "\n",
    "        resp_01 = chain_01.invoke({\n",
    "            \"aspects\":GAME_ASPECTS,\n",
    "            \"context\": aspects_response\n",
    "        }, config=chain_config)\n",
    "\n",
    "        token_usage_01 = common_deque.popleft()\n",
    "        \n",
    "        token_usage_result = combine_token_usage_obj([token_usage_01])\n",
    "\n",
    "        # show result:\n",
    "        print('-' * 10 + f'Result for {llm_model_name} at attempt={i+1}' + '-' * 10)\n",
    "        print(resp_01); print()\n",
    "        print('Token usage:', token_usage_result['token_usage'])\n",
    "        print('Time costed: {:.04} s'.format(float(token_usage_result['time_costed']) / 1e9))\n",
    "        print('\\n\\n')\n",
    "\n",
    "        responses[f'attempt_{i}'] = {\n",
    "            'resp_01': resp_01,\n",
    "            'token_usage': token_usage_result['token_usage'],\n",
    "            'time_costed': token_usage_result['time_costed']\n",
    "        }\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Aspect response 00----------\n",
      "----------Result for mistral_7b at attempt=1----------\n",
      " {\n",
      "\"Gameplay\": [\"performance issues\", \"running 25-35 fps\", \"disappointing for $70 title\"],\n",
      "\"Narrative\": [\"NA\"],\n",
      "\"Accessibility\": [\"NA\"],\n",
      "\"Sound\": [\"sound details not mentioned\"],\n",
      "\"Graphics & Art Design\": [\"odd appearance\", \"max settings\", \"disabling up-scaling doesn't help\"],\n",
      "\"Performance\": [\"poorly optimized\", \"running 25-35 fps on both low and ultra settings\", \"not acceptable for $70 title\"],\n",
      "\"Bug\": [\"NA\"],\n",
      "\"Suggestion\": [\"run at 50-60 fps on lowest settings\", \"optimize settings to get better frames\"],\n",
      "\"Price\": [\"$70\"],\n",
      "\"Overall\": [\"time-consuming optimization process\", \"refund not possible due to hours invested\"]\n",
      "}\n",
      "\n",
      "Token usage: 426\n",
      "Time costed: 4.109 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for mistral_7b at attempt=2----------\n",
      " ```json\n",
      "{\n",
      "  \"Gameplay\": [\"performance issues\", \"25 - 35 fps on low settings\", \"disappointing for $70 title\"],\n",
      "  \"Narrative\": [\"NA\"],\n",
      "  \"Accessibility\": [\"NA\"],\n",
      "  \"Sound\": [\"sound details not mentioned\"],\n",
      "  \"Graphics & Art Design\": [\"odd appearance\", \"max settings\", \"disabling up-scaling doesn't help\"],\n",
      "  \"Performance\": [\"poorly optimized\", \"25 - 35 fps on both low and ultra settings\", \"not acceptable for $70 title\"],\n",
      "  \"Bug\": [\"NA\"],\n",
      "  \"Suggestion\": [\"run at 50 - 60 fps on lowest settings\", \"optimize settings to get better frames\"],\n",
      "  \"Price\": [\"$70\"],\n",
      "  \"Overall\": [\"time-consuming optimization process\", \"refund not possible due to hours invested\"]\n",
      "}\n",
      "```\n",
      "\n",
      "Token usage: 214\n",
      "Time costed: 3.984 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for llama2_7b at attempt=1----------\n",
      "{\n",
      "\"Gameplay\": [\"performance issues\", \"poor optimization\", \"low fps\"],\n",
      "\"Narrative\": [\"NA\"],\n",
      "\"Accessibility\": [\"NA\"],\n",
      "\"Sound\": [\"no sound details\"],\n",
      "\"Graphics & Art Design\": [\"odd appearance\", \"unacceptable framerate\"],\n",
      "\"Performance\": [\"25-35 fps\", \"poor optimization\"],\n",
      "\"Bug\": [\"NA\"],\n",
      "\"Suggestion\": [\"optimize settings for better frames\"],\n",
      "\"Price\": [\"$70\"],\n",
      "\"Overall\": [\"time-consuming optimization\", \"refund not possible\"]\n",
      "}\n",
      "\n",
      "Token usage: 553\n",
      "Time costed: 3.213 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for llama2_7b at attempt=2----------\n",
      "Here is the JSON output based on the reviews provided:\n",
      "\n",
      "{\n",
      "\"Gameplay\": [\"performance issues\", \"poor optimization\", \"low fps\"],\n",
      "\"Narrative\": [\"NA\"],\n",
      "\"Accessibility\": [\"NA\"],\n",
      "\"Sound\": [\"no mention of sound details\"],\n",
      "\"Graphics & Art Design\": [\"odd appearance\", \"disabling up-scaling doesn't help\"],\n",
      "\"Performance\": [\"poorly optimized\", \"low fps\", \"not acceptable for $70 title\"],\n",
      "\"Bug\": [\"NA\"],\n",
      "\"Suggestion\": [\"should run at 50 - 60 fps on lowest settings\"],\n",
      "\"Price\": [\"$70\"],\n",
      "\"Overall\": [\"time-consuming optimization\", \"refund not possible due to hours invested\"]\n",
      "}\n",
      "\n",
      "Token usage: 175\n",
      "Time costed: 3.039 s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------Aspect response 01----------\n",
      "----------Result for mistral_7b at attempt=1----------\n",
      " {\n",
      "\"Gameplay\": [\"good graphics\", \"stealth gameplay\", \"variety of weapons\", \"exploring Paris\", \"coop missions\", \"customization\"],\n",
      "\"Narrative\": [\"story isn't great\", \"latest games of the same series did not have very good stories\"],\n",
      "\"Accessibility\": [\"requires a good PC\", \"may have some bugs\", \"not easy to run on lower-end systems\"],\n",
      "\"Sound\": [\"NA\"],\n",
      "\"Graphics & Art Design\": [\"graphics are really good\", \"Paris is huge\", \"lot of things you can do\"],\n",
      "\"Performance\": [\"need a pretty good PC\", \"some bugs affect gameplay\"],\n",
      "\"Bug\": [\"yes, there are some bugs\", \"reviewer has experienced a few of them\"],\n",
      "\"Suggestion\": [\"NA\"],\n",
      "\"Price\": [\"NA\"],\n",
      "\"Overall\": [\"reviewer thinks the game isn't bad\", \"praises graphics, stealth mechanics, and customization\", \"acknowledges need for good PC, some bugs, and so-so story\"]\n",
      "}\n",
      "\n",
      "Token usage: 663\n",
      "Time costed: 5.007 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for mistral_7b at attempt=2----------\n",
      " ```json\n",
      "{\n",
      "\"Gameplay\": [\"good graphics\", \"stealth gameplay\", \"variety of weapons\", \"exploring Paris\", \"coop missions\", \"customization\"],\n",
      "\"Narrative\": [\"story isn't great\", \"latest games had poor stories\"],\n",
      "\"Accessibility\": [\"requires good PC\", \"may have bugs\", \"not easy to run on lower-end systems\"],\n",
      "\"Sound\": [\"NA\"],\n",
      "\"Graphics & Art Design\": [\"graphics are really good\", \"huge Paris\", \"things to do\"],\n",
      "\"Performance\": [\"need pretty good PC\", \"some bugs affect gameplay\"],\n",
      "\"Bug\": [\"yes\", \"experienced a few\"],\n",
      "\"Suggestion\": [\"NA\"],\n",
      "\"Price\": [\"NA\"],\n",
      "\"Overall\": [\"reviewer thinks game isn't bad\", \"praises graphics, stealth mechanics, customization\", \"acknowledges need for good PC, bugs, so-so story\"]\n",
      "}\n",
      "```\n",
      "\n",
      "Token usage: 216\n",
      "Time costed: 3.895 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for llama2_7b at attempt=1----------\n",
      "{\n",
      "\"Gameplay\": [\"good\", \"graphics\", \"stealth\", \"weapons\", \"coop\"],\n",
      "\"Narrative\": [\"bad\", \"story\"],\n",
      "\"Accessibility\": [\"PC\", \"bugs\", \"lower-end\"],\n",
      "\"Sound\": [],\n",
      "\"Graphics & Art Design\": [\"really good\", \"huge\", \"things to do\"],\n",
      "\"Performance\": [\"pretty good\", \"PC\", \"bugs\"],\n",
      "\"Bug\": [\"yes\", \"few\"],\n",
      "\"Suggestion\": [],\n",
      "\"Price\": [],\n",
      "\"Overall\": [\"not bad\", \"praising\", \"graphics\", \"stealth\", \"customization\"]\n",
      "}\n",
      "\n",
      "Token usage: 597\n",
      "Time costed: 3.399 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for llama2_7b at attempt=2----------\n",
      "{\n",
      "\"Gameplay\": [\"good\", \"graphics\", \"stealth\", \"weapons\", \"coop\"],\n",
      "\"Narrative\": [\"story\", \"great\", \"same\", \"series\"],\n",
      "\"Accessibility\": [\"PC\", \"bugs\", \"lower-end\", \"run\"],\n",
      "\"Sound\": [],\n",
      "\"Graphics & Art Design\": [\"good\", \"huge\", \"things\", \"do\"],\n",
      "\"Performance\": [\"pretty\", \"need\", \"run\", \"gameplay\"],\n",
      "\"Bug\": [\"some\", \"experienced\"],\n",
      "\"Suggestion\": [],\n",
      "\"Price\": [],\n",
      "\"Overall\": [\"praising\", \"graphics\", \"mechanics\", \"customization\"]\n",
      "}\n",
      "\n",
      "Token usage: 159\n",
      "Time costed: 2.71 s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------Aspect response 02----------\n",
      "----------Result for mistral_7b at attempt=1----------\n",
      " {\n",
      "\"Gameplay\": [\"Not mentioned\"],\n",
      "\"Narrative\": [\"Not mentioned\"],\n",
      "\"Accessibility\": [\"Social Club app\", \"issues\", \"permanently broken\"],\n",
      "\"Sound\": [\"No information\"],\n",
      "\"Graphics & Art Design\": [\"Not mentioned\"],\n",
      "\"Performance\": [\"Social Club app\", \"issues\", \"crashes\", \"error messages\", \"permanently broken\"],\n",
      "\"Bug\": [\"Social Club app\", \"attempting to fix\", \"permanently broken\", \"crashes\", \"error messages\"],\n",
      "\"Suggestion\": [\"NA\"],\n",
      "\"Price\": [\"NA\"],\n",
      "\"Overall\": [\"enjoyed the game\", \"negative experience\"]\n",
      "}\n",
      "\n",
      "Token usage: 520\n",
      "Time costed: 3.388 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for mistral_7b at attempt=2----------\n",
      " '''\n",
      "{\n",
      "\"Gameplay\": [\"Not mentioned in the review\"],\n",
      "\"Narrative\": [\"Not mentioned in the review\"],\n",
      "\"Accessibility\": [\"Issues with Social Club app\", \"Permanently broken\"],\n",
      "\"Sound\": [\"No information provided\"],\n",
      "\"Graphics & Art Design\": [\"Not mentioned in the review\"],\n",
      "\"Performance\": [\"Negatively impacted\", \"Crashes\", \"Permanently broken\"],\n",
      "\"Bug\": [\"Game became permanently broken\", \"Attempting to fix Social Club app issue\", \"Error messages\"],\n",
      "\"Suggestion\": [\"NA\"],\n",
      "\"Price\": [\"NA\"],\n",
      "\"Overall\": [\"Enjoyed the game while it worked\", \"Negative experience\", \"Unresolved bug\"]\n",
      "}\n",
      "'''\n",
      "\n",
      "Token usage: 163\n",
      "Time costed: 2.813 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for llama2_7b at attempt=1----------\n",
      "{\n",
      "\"Gameplay\": [],\n",
      "\"Narrative\": [],\n",
      "\"Accessibility\": [\"Social Club app issues\"],\n",
      "\"Sound\": [],\n",
      "\"Graphics & Art Design\": [],\n",
      "\"Performance\": [\"crashes\", \"permanently broken\"],\n",
      "\"Bug\": [\"game became permanently broken\"],\n",
      "\"Suggestion\": [],\n",
      "\"Price\": [],\n",
      "\"Overall\": [\"enjoyed the game while it worked\", \"ultimately left with a negative experience\"]\n",
      "}\n",
      "\n",
      "Token usage: 489\n",
      "Time costed: 2.498 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for llama2_7b at attempt=2----------\n",
      "{\n",
      "\"Gameplay\": [],\n",
      "\"Narrative\": [],\n",
      "\"Accessibility\": [\"Social Club app issues\"],\n",
      "\"Sound\": [],\n",
      "\"Graphics & Art Design\": [],\n",
      "\"Performance\": [\"crashes\", \"permanently broken\"],\n",
      "\"Bug\": [\"game became permanently broken\"],\n",
      "\"Suggestion\": [],\n",
      "\"Price\": [],\n",
      "\"Overall\": [\"enjoyed the game while it worked\", \"ultimately left with a negative experience\"]\n",
      "}\n",
      "\n",
      "Token usage: 105\n",
      "Time costed: 1.767 s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------Aspect response 03----------\n",
      "----------Result for mistral_7b at attempt=1----------\n",
      " {\n",
      "\"Gameplay\": [\"inconsistent mana distribution\", \"allow specific card buying for coins\"],\n",
      "\"Narrative\": [\"NA\"],\n",
      "\"Accessibility\": [\"NA\"],\n",
      "\"Sound\": [\"NA\"],\n",
      "\"Graphics & Art Design\": [\"NA\"],\n",
      "\"Performance\": [\"NA\"],\n",
      "\"Bug\": [\"potential bug in random algorithm\", \"inconsistent mana distribution\"],\n",
      "\"Suggestion\": [\"improve random algorithm for shuffling deck\", \"implement specific card buying\"],\n",
      "\"Price\": [\"NA\"],\n",
      "\"Overall\": [\"good despite mentioned issues\"]\n",
      "}\n",
      "\n",
      "Token usage: 460\n",
      "Time costed: 2.976 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for mistral_7b at attempt=2----------\n",
      " '''\n",
      "{\n",
      "\"Gameplay\": [\"inconsistent mana distribution\", \"allowing specific card buying\"],\n",
      "\"Narrative\": [],\n",
      "\"Accessibility\": [],\n",
      "\"Sound\": [],\n",
      "\"Graphics & Art Design\": [],\n",
      "\"Performance\": [],\n",
      "\"Bug\": [\"potential bug\", \"inconsistent mana distribution\"],\n",
      "\"Suggestion\": [\"improve random algorithm\", \"implement specific card buying\"],\n",
      "\"Price\": [],\n",
      "\"Overall\": [\"good\", \"mentioned issues\"]\n",
      "}\n",
      "'''\n",
      "\n",
      "Token usage: 114\n",
      "Time costed: 2.011 s\n",
      "\n",
      "\n",
      "\n",
      "----------Result for llama2_7b at attempt=1----------\n",
      "{\n",
      "\"Gameplay\": [\"inconsistent mana distribution\", \"specific card buying\", \"coins\"],\n",
      "\"Narrative\": [\"NA\"],\n",
      "\"Accessibility\": [\"NA\"],\n",
      "\"Sound\": [\"NA\"],\n",
      "\"Graphics & Art Design\": [\"NA\"],\n",
      "\"Performance\": [\"potential bug\", \"inconsistent mana distribution\"],\n",
      "\"Bug\": [\"potential bug\"],\n",
      "\"Suggestion\": [\"improve random algorithm\", \"specific card buying\"],\n",
      "\"Price\": [\"NA\"],\n",
      "\"Overall\": [\"good despite issues\"]\n",
      "}\n",
      "\n",
      "Token usage: 475\n",
      "Time costed: 2.846 s\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAspect response \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m llm_model_name, llm_model \u001b[38;5;129;01min\u001b[39;00m Llm_models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 9\u001b[0m     aspect_keyword_extraction_result_per_aspect_response[llm_model_name] \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_aspect_keyword_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspects_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m aspect_keyword_extraction_result[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maspects_response_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m aspect_keyword_extraction_result_per_aspect_response\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 14\u001b[0m, in \u001b[0;36mprompt_aspect_keyword_extraction\u001b[0;34m(llm_model_name, llm_model, aspects_response, repeat_count)\u001b[0m\n\u001b[1;32m      7\u001b[0m chat_prompt_01 \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[1;32m      8\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, _prompts\u001b[38;5;241m.\u001b[39mSYSTEM_TEMPLATE),\n\u001b[1;32m      9\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, _prompts\u001b[38;5;241m.\u001b[39mKEYWORD_TEMPLATE_02)\n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     12\u001b[0m chain_01 \u001b[38;5;241m=\u001b[39m chat_prompt_01 \u001b[38;5;241m|\u001b[39m llm_model\n\u001b[0;32m---> 14\u001b[0m resp_01 \u001b[38;5;241m=\u001b[39m \u001b[43mchain_01\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maspects\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mGAME_ASPECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maspects_response\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m token_usage_01 \u001b[38;5;241m=\u001b[39m common_deque\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m     21\u001b[0m token_usage_result \u001b[38;5;241m=\u001b[39m combine_token_usage_obj([token_usage_01])\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/langchain_core/runnables/base.py:2053\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2053\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2056\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/langchain_core/language_models/llms.py:230\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    227\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    228\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    241\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/langchain_core/language_models/llms.py:525\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    519\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    523\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    524\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/langchain_core/language_models/llms.py:698\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         )\n\u001b[1;32m    685\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    686\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    687\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     ]\n\u001b[0;32m--> 698\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/langchain_core/language_models/llms.py:562\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    561\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    563\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/langchain_core/language_models/llms.py:549\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    541\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    546\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 549\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    553\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    557\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    558\u001b[0m         )\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/langchain_community/llms/ollama.py:400\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 400\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/langchain_community/llms/ollama.py:309\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    302\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    308\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    311\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/langchain_community/llms/ollama.py:154\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    153\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    155\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    156\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    157\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    159\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    572\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/urllib3/response.py:931\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/urllib3/response.py:1074\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1074\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1075\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   1076\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m )\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/site-packages/urllib3/response.py:1025\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_left\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-tm/lib/python3.9/http/client.py:610\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    608\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mbytes\u001b[39m(b[\u001b[38;5;241m0\u001b[39m:total_bytes]))\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    611\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested, compensating for partial reads.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    Normally, we have a blocking socket, but a read() can be interrupted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     s \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aspect_keyword_extraction_result = {}\n",
    "\n",
    "for i, aspects_response in enumerate(_aspects_responses.ALL_ASPECTS_RESPONSES):\n",
    "\n",
    "    aspect_keyword_extraction_result_per_aspect_response = {}\n",
    "    print('-'* 10 + f'Aspect response {i:02}' + '-'* 10)\n",
    "\n",
    "    for llm_model_name, llm_model in Llm_models.items():\n",
    "        aspect_keyword_extraction_result_per_aspect_response[llm_model_name] = prompt_aspect_keyword_extraction(llm_model_name, llm_model, aspects_response, repeat_count=2)\n",
    "\n",
    "    aspect_keyword_extraction_result[f'aspects_response_{i:02}'] = aspect_keyword_extraction_result_per_aspect_response\n",
    "\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-test-tm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
