{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT fine-tuning script\n",
    "\n",
    "On 1M imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-sampled dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "dataset_folder_path = Path('../../dataset/sa/sampled_120k_2023-12-16/')\n",
    "dataset_valid_folder_path = Path('../../dataset/sa/sampled_valid_2023-12-16/')\n",
    "\n",
    "dataset_val_bal = pd.read_pickle(dataset_valid_folder_path / 'validation_balanced.pkl')\n",
    "dataset_val_imbal = pd.read_pickle(dataset_valid_folder_path / 'validation_imbalanced.pkl')\n",
    "\n",
    "dataset_traintest = pd.read_pickle(dataset_folder_path / 'dataset_imbal_sampled_120k.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 301344 entries, 2447379 to 2758277\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   index         301344 non-null  int64 \n",
      " 1   app_id        301344 non-null  int64 \n",
      " 2   app_name      301344 non-null  object\n",
      " 3   review_text   301344 non-null  object\n",
      " 4   review_score  301344 non-null  int64 \n",
      " 5   review_votes  301344 non-null  int64 \n",
      " 6   num_of_words  301344 non-null  int64 \n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 18.4+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset_val_bal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_67963_row0_col0, #T_67963_row0_col1, #T_67963_row1_col1 {\n",
       "  background-color: #fcfbfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_67963_row1_col0 {\n",
       "  background-color: #3f007d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_67963\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_67963_level0_col0\" class=\"col_heading level0 col0\" >review_score</th>\n",
       "      <th id=\"T_67963_level0_col1\" class=\"col_heading level0 col1\" >review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_67963_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_67963_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_67963_row0_col1\" class=\"data row0 col1\" >150672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67963_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_67963_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_67963_row1_col1\" class=\"data row1 col1\" >150672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2f72602880>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = dataset_val_bal.groupby('review_score').count()['review_text'].reset_index().sort_values(by='review_score',ascending=True)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 925305 entries, 1752495 to 4683282\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   index         925305 non-null  int64 \n",
      " 1   app_id        925305 non-null  int64 \n",
      " 2   app_name      925305 non-null  object\n",
      " 3   review_text   925305 non-null  object\n",
      " 4   review_score  925305 non-null  int64 \n",
      " 5   review_votes  925305 non-null  int64 \n",
      " 6   num_of_words  925305 non-null  int64 \n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 56.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset_val_imbal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_204cd_row0_col0, #T_204cd_row0_col1 {\n",
       "  background-color: #fcfbfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_204cd_row1_col0, #T_204cd_row1_col1 {\n",
       "  background-color: #3f007d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_204cd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_204cd_level0_col0\" class=\"col_heading level0 col0\" >review_score</th>\n",
       "      <th id=\"T_204cd_level0_col1\" class=\"col_heading level0 col1\" >review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_204cd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_204cd_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_204cd_row0_col1\" class=\"data row0 col1\" >150928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_204cd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_204cd_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_204cd_row1_col1\" class=\"data row1 col1\" >774377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2fbc4118e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = dataset_val_imbal.groupby('review_score').count()['review_text'].reset_index().sort_values(by='review_score',ascending=True)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 120000 entries, 3794742 to 3855230\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   index         120000 non-null  int64 \n",
      " 1   app_id        120000 non-null  int64 \n",
      " 2   app_name      120000 non-null  object\n",
      " 3   review_text   120000 non-null  object\n",
      " 4   review_score  120000 non-null  int64 \n",
      " 5   review_votes  120000 non-null  int64 \n",
      " 6   num_of_words  120000 non-null  int64 \n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 7.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset_traintest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_42757_row0_col0, #T_42757_row0_col1 {\n",
       "  background-color: #fcfbfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_42757_row1_col0, #T_42757_row1_col1 {\n",
       "  background-color: #3f007d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_42757\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_42757_level0_col0\" class=\"col_heading level0 col0\" >review_score</th>\n",
       "      <th id=\"T_42757_level0_col1\" class=\"col_heading level0 col1\" >review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_42757_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_42757_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_42757_row0_col1\" class=\"data row0 col1\" >20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_42757_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_42757_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_42757_row1_col1\" class=\"data row1 col1\" >100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2f1d15af10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = dataset_traintest.groupby('review_score').count()['review_text'].reset_index().sort_values(by='review_score',ascending=True)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imbal_valid = dataset_val_imbal['review_text']\n",
    "y_imbal_valid = dataset_val_imbal['review_score']\n",
    "\n",
    "X_bal_valid = dataset_val_bal['review_text']\n",
    "y_bal_valid = dataset_val_bal['review_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any data augmentation plz add here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data augmentation here (if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and testing dataset from here :D\n",
    "\n",
    "Sample a fixed amount of comments from here \n",
    "\n",
    "baseline (imbalanced): positive comments : negative comments = 5:1\n",
    "\n",
    "baseline (balanced): positive comments : negative comments = 1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "\n",
    "TEST_RATIO = 0.1            # ratio of test set to the whole train-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test = dataset_traintest['review_text']\n",
    "y_train_test = dataset_traintest['review_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_test,\n",
    "    y_train_test,\n",
    "    random_state=13,\n",
    "    test_size=TEST_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "108000\n",
      "108000\n",
      "\n",
      "Test set\n",
      "12000\n",
      "12000\n",
      "\n",
      "Validation (imbalanced) set\n",
      "925305\n",
      "925305\n",
      "\n",
      "Validation (balanced) set\n",
      "301344\n",
      "301344\n",
      "\n",
      "\n",
      "Training set\n",
      "review_score\n",
      "1    89949\n",
      "0    18051\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set\n",
      "review_score\n",
      "1    10051\n",
      "0     1949\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation (imbalanced) set\n",
      "review_score\n",
      "1    774377\n",
      "0    150928\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation (balanced) set\n",
      "review_score\n",
      "0    150672\n",
      "1    150672\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "training set datatype\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "\n",
      "test set datatype\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "\n",
      "validation (imbalanced) set datatype\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "\n",
      "validation (balanced) set datatype\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print('Training set')\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print()\n",
    "print('Test set')\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "print()\n",
    "print('Validation (imbalanced) set')\n",
    "print(len(X_imbal_valid))\n",
    "print(len(y_imbal_valid))\n",
    "print()\n",
    "print('Validation (balanced) set')\n",
    "print(len(X_bal_valid))\n",
    "print(len(y_bal_valid))\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Training set')\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print()\n",
    "print('Test set')\n",
    "print(pd.Series(y_test).value_counts())\n",
    "print()\n",
    "print('Validation (imbalanced) set')\n",
    "print(pd.Series(y_imbal_valid).value_counts())\n",
    "print()\n",
    "print('Validation (balanced) set')\n",
    "print(pd.Series(y_bal_valid).value_counts())\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('training set datatype')\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print()\n",
    "print('test set datatype')\n",
    "print(type(X_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "print('validation (imbalanced) set datatype')\n",
    "print(type(X_imbal_valid))\n",
    "print(type(y_imbal_valid))\n",
    "print()\n",
    "print('validation (balanced) set datatype')\n",
    "print(type(X_bal_valid))\n",
    "print(type(y_bal_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108000\n",
      "108000\n",
      "12000\n",
      "12000\n",
      "\n",
      "\n",
      "training set\n",
      "Counter({1: 89949, 0: 18051})\n",
      "pos / neg ratio = 4.983048030580023\n",
      "\n",
      "testing set\n",
      "Counter({1: 10051, 0: 1949})\n",
      "pos / neg ratio = 5.157003591585428\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "# distribution of +ve and -ve comments in train and test set\n",
    "import collections\n",
    "\n",
    "print('training set')\n",
    "print(collections.Counter(y_train))\n",
    "print('pos / neg ratio =', float(collections.Counter(y_train)[1] / collections.Counter(y_train)[0]))\n",
    "print()\n",
    "print('testing set')\n",
    "print(collections.Counter(y_test))\n",
    "print('pos / neg ratio =', float(collections.Counter(y_test)[1] / collections.Counter(y_test)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply cleaning before further creating a subset of comments for training data\n",
    "\n",
    "data cleaning is performed on training and testing dataset first\n",
    "\n",
    "same cleaning will be performed on validation dataset after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "import str_cleaning_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    " \n",
    "# # setting path\n",
    "# sys.path.append('../')\n",
    "\n",
    "# import str_cleaning_functions\n",
    "\n",
    "# X_valid = str_cleaning_functions.cleaning_arr(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your custom cleaning function for BERT\n",
    "# modified from str_cleaning_functions.py\n",
    "\n",
    "def cleaning_arr(str_arr):\n",
    "    str_arr = str_arr.apply(lambda x: str_cleaning_functions.clean(x))\n",
    "    str_arr = str_arr.apply(lambda x: str_cleaning_functions.deEmojify(x))\n",
    "\n",
    "    return str_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = str_cleaning_functions.cleaning_pyarr(list(X_train.ravel()))\n",
    "# X_train = np.array(X_train)\n",
    "\n",
    "X_train = str_cleaning_functions.cleaning_arr(X_train)\n",
    "X_train = X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = str_cleaning_functions.cleaning_pyarr(list(X_test.ravel()))\n",
    "# X_test = np.array(X_test)\n",
    "\n",
    "X_test = str_cleaning_functions.cleaning_arr(X_test)\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set\n",
      "(108000,)\n",
      "(108000,)\n",
      "\n",
      "testing set\n",
      "(12000,)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "print('training set')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print()\n",
    "print('testing set')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging face stuff :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original shape of X_train does not match with the requirement (need a flattened 1-D array of strings)\n",
    "\n",
    "(That's the reason behind sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# create a dataset object for handling large amount of data\n",
    "from datasets import Dataset\n",
    "\n",
    "ds_train = Dataset.from_dict({\n",
    "    \"text\": [str(s) for s in list(X_train.flatten())],\n",
    "    \"label\": list(y_train)\n",
    "})\n",
    "\n",
    "ds_test = Dataset.from_dict({\n",
    "    \"text\": [str(s) for s in list(X_test.flatten())],\n",
    "    \"label\": list(y_test)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"text\"], max_length=tokenizer.model_max_length, truncation=True)\n",
    "\n",
    "# the tokenizer only accept list of strings\n",
    "# tokenized_data = tokenizer(ds_train['text'], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "# tokenized_data = dict(tokenized_data)\n",
    "\n",
    "# labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 108000\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Trainer API backend\n",
    "\n",
    "Copy from https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer\n",
    "\n",
    "TODO: experiment on the TrainingArguments class, e.g. hyperparameters  \n",
    "and also different parameters in 'Evaluate' library\n",
    "\n",
    "Also do logging about the training loss for each epoch (read: https://discuss.huggingface.co/t/using-tensorboard-summarywriter-with-huggingface-trainerapi/23015/5 later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 108000/108000 [00:09<00:00, 11171.08 examples/s]\n",
      "Map: 100%|██████████| 12000/12000 [00:01<00:00, 10625.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"text\"], padding='max_length', max_length=tokenizer.model_max_length, truncation=True)\n",
    "\n",
    "# apply tokenizer to the dataset\n",
    "ds_train = ds_train.map(tokenize_dataset, batched=True)\n",
    "ds_test = ds_test.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting 100k samples (roughly 10%) for training (and testing the program flow\n",
    "# and 10k samples for testing (reduce time)\n",
    "# n_train = 100 * 1000\n",
    "# n_test = 10 * 1000\n",
    "\n",
    "# ds_train_small = ds_train.shuffle(seed=42).select(range(n_train))\n",
    "# ds_test_small = ds_test.shuffle(seed=13).select(range(n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot both training and testing loss on the same graph\n",
    "\n",
    "https://stackoverflow.com/questions/73281901/is-there-a-way-to-plot-training-and-validation-losses-on-the-same-graph-with-hug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import TrainerCallback\n",
    "from transformers.integrations import is_tensorboard_available\n",
    "\n",
    "def custom_rewrite_logs(d, mode):\n",
    "    '''If you want to combine train and eval for other metrics besides the loss then custom_rewrite_logs should be modified accordingly.'''\n",
    "    new_d = {}\n",
    "    eval_prefix = \"eval_\"\n",
    "    eval_prefix_len = len(eval_prefix)\n",
    "    test_prefix = \"test_\"\n",
    "    test_prefix_len = len(test_prefix)\n",
    "\n",
    "    # we combine loss, accuracy, recall and f1\n",
    "\n",
    "    for k, v in d.items():\n",
    "        if mode == 'eval' and k.startswith(eval_prefix):\n",
    "            if k[eval_prefix_len:] == 'loss':\n",
    "                new_d[\"combined/\" + k[eval_prefix_len:]] = v\n",
    "\n",
    "            elif k[eval_prefix_len:] == 'accuracy':\n",
    "                new_d[\"combined/\" + k[eval_prefix_len:]] = v\n",
    "\n",
    "            elif k[eval_prefix_len:] == 'recall':\n",
    "                new_d[\"combined/\" + k[eval_prefix_len:]] = v\n",
    "\n",
    "            elif k[eval_prefix_len:] == 'f1':\n",
    "                new_d[\"combined/\" + k[eval_prefix_len:]] = v\n",
    "            \n",
    "        elif mode == 'test' and k.startswith(test_prefix):\n",
    "            if k[test_prefix_len:] == 'loss':\n",
    "                new_d[\"combined/\" + k[test_prefix_len:]] = v\n",
    "\n",
    "            elif k[test_prefix_len:] == 'accuracy':\n",
    "                new_d[\"combined/\" + k[test_prefix_len:]] = v\n",
    "\n",
    "            elif k[test_prefix_len:] == 'recall':\n",
    "                new_d[\"combined/\" + k[test_prefix_len:]] = v    \n",
    "\n",
    "            elif k[test_prefix_len:] == 'f1':\n",
    "                new_d[\"combined/\" + k[test_prefix_len:]] = v\n",
    "\n",
    "        elif mode == 'train':\n",
    "            if k == 'loss':\n",
    "                new_d[\"combined/\" + k] = v\n",
    "\n",
    "            elif k == 'train_accuracy':\n",
    "                new_d[\"combined/\" + 'accuracy'] = v\n",
    "\n",
    "            elif k == 'train_recall':\n",
    "                new_d[\"combined/\" + 'recall'] = v\n",
    "\n",
    "            elif k == 'train_f1':\n",
    "                new_d[\"combined/\" + 'f1'] = v\n",
    "                \n",
    "    return new_d\n",
    "\n",
    "\n",
    "class CombinedTensorBoardCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A [`TrainerCallback`] that sends the logs to [TensorBoard](https://www.tensorflow.org/tensorboard).\n",
    "    Args:\n",
    "        tb_writer (`SummaryWriter`, *optional*):\n",
    "            The writer to use. Will instantiate one if not set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tb_writers=None):\n",
    "        has_tensorboard = is_tensorboard_available()\n",
    "        if not has_tensorboard:\n",
    "            raise RuntimeError(\n",
    "                \"TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or\"\n",
    "                \" install tensorboardX.\"\n",
    "            )\n",
    "        if has_tensorboard:\n",
    "            try:\n",
    "                from torch.utils.tensorboard import SummaryWriter  # noqa: F401\n",
    "\n",
    "                self._SummaryWriter = SummaryWriter\n",
    "            except ImportError:\n",
    "                try:\n",
    "                    from tensorboardX import SummaryWriter\n",
    "\n",
    "                    self._SummaryWriter = SummaryWriter\n",
    "                except ImportError:\n",
    "                    self._SummaryWriter = None\n",
    "        else:\n",
    "            self._SummaryWriter = None\n",
    "        self.tb_writers = tb_writers\n",
    "\n",
    "    def _init_summary_writer(self, args, log_dir=None):\n",
    "        log_dir = log_dir or args.logging_dir\n",
    "        if self._SummaryWriter is not None:\n",
    "            self.tb_writers = dict(train=self._SummaryWriter(log_dir=os.path.join(log_dir, 'train')),\n",
    "                                   eval=self._SummaryWriter(log_dir=os.path.join(log_dir, 'eval')))\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        if not state.is_world_process_zero:\n",
    "            return\n",
    "\n",
    "        log_dir = None\n",
    "\n",
    "        if state.is_hyper_param_search:\n",
    "            trial_name = state.trial_name\n",
    "            if trial_name is not None:\n",
    "                log_dir = os.path.join(args.logging_dir, trial_name)\n",
    "\n",
    "        if self.tb_writers is None:\n",
    "            self._init_summary_writer(args, log_dir)\n",
    "\n",
    "        for k, tbw in self.tb_writers.items():\n",
    "            tbw.add_text(\"args\", args.to_json_string())\n",
    "            if \"model\" in kwargs:\n",
    "                model = kwargs[\"model\"]\n",
    "                if hasattr(model, \"config\") and model.config is not None:\n",
    "                    model_config_json = model.config.to_json_string()\n",
    "                    tbw.add_text(\"model_config\", model_config_json)\n",
    "            # Version of TensorBoard coming from tensorboardX does not have this method.\n",
    "            if hasattr(tbw, \"add_hparams\"):\n",
    "                tbw.add_hparams(args.to_sanitized_dict(), metric_dict={})\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not state.is_world_process_zero:\n",
    "            return\n",
    "\n",
    "        if self.tb_writers is None:\n",
    "            self._init_summary_writer(args)\n",
    "\n",
    "        for tbk, tbw in self.tb_writers.items():\n",
    "            logs_new = custom_rewrite_logs(logs, mode=tbk)\n",
    "            for k, v in logs_new.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    tbw.add_scalar(k, v, state.global_step)\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"Trainer is attempting to log a value of \"\n",
    "                        f'\"{v}\" of type {type(v)} for key \"{k}\" as a scalar. '\n",
    "                        \"This invocation of Tensorboard's writer.add_scalar() \"\n",
    "                        \"is incorrect so we dropped this attribute.\"\n",
    "                    )\n",
    "            tbw.flush()\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        for tbw in self.tb_writers.values():\n",
    "            tbw.close()\n",
    "        self.tb_writers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 13:12:17.689115: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-16 13:12:17.844081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-16 13:12:17.844101: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-16 13:12:17.845093: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-16 13:12:17.914082: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Specify where to save the checkpoints from your training:\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer_2023-12-16_120k_imbal\",\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  per_device_eval_batch_size=32,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  weight_decay=0.01,\n",
    "                                #   warmup_steps=10000,   # seems not required in fine-tuning, as we have less than 10k steps\n",
    "                                  num_train_epochs=3,\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  save_strategy=\"steps\",\n",
    "                                  save_steps=500,\n",
    "                                  eval_steps=500,\n",
    "                                  metric_for_best_model='eval_loss',\n",
    "                                  load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_recall = evaluate.load('recall')\n",
    "metric_f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred      # logits: an n*num_of_class array with probability, e.g. [[ 1.9851098, -1.6966375],[ 2.7240963, -2.372472 ],...], labels = true labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = metric_acc.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels)['recall']\n",
    "    f1_score = metric_f1.compute(predictions=predictions, references=labels, pos_label=1)['f1']\n",
    "    # rocauc = metric_rocauc.compute(predictions=predictions, references=labels)['roc_auc']\n",
    "    return {'accuracy': acc, \"recall\": recall, \"f1\": f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override original trainer to add evaluation on training dataset as well\n",
    "\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from transformers.trainer_utils import speed_metrics\n",
    "from transformers.debug_utils import DebugOption\n",
    "from transformers.utils import  is_torch_tpu_available\n",
    "\n",
    "if is_torch_tpu_available(check_device=False):\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.debug.metrics as met\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def evaluate(self, \n",
    "                 eval_dataset: Optional[Dataset] = None,\n",
    "                 ignore_keys: Optional[List[str]] = None,\n",
    "                 metric_key_prefix: str = \"eval\"\n",
    "                 ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "            Run evaluation and returns metrics.\n",
    "            The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n",
    "            (pass it to the init `compute_metrics` argument).\n",
    "            You can also subclass and override this method to inject custom behavior.\n",
    "            Args:\n",
    "                eval_dataset (`Dataset`, *optional*):\n",
    "                    Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n",
    "                    not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n",
    "                    method.\n",
    "                ignore_keys (`Lst[str]`, *optional*):\n",
    "                    A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
    "                    gathering predictions.\n",
    "                metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n",
    "                    An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
    "                    \"eval_bleu\" if the prefix is \"eval\" (default)\n",
    "            Returns:\n",
    "                A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n",
    "                dictionary also contains the epoch number which comes from the training state.\n",
    "            \"\"\"\n",
    "        # memory metrics - must set up as early as possible\n",
    "        self._memory_tracker.start()\n",
    "\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        train_dataloader = self.get_train_dataloader()\n",
    "        start_time = time.time()\n",
    "\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        eval_output = eval_loop(\n",
    "            eval_dataloader,\n",
    "            description=\"Evaluation\",\n",
    "            # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "            # self.args.prediction_loss_only\n",
    "            prediction_loss_only=True if self.compute_metrics is None else None,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=metric_key_prefix,\n",
    "        )\n",
    "\n",
    "        train_output = eval_loop(\n",
    "            train_dataloader,\n",
    "            description='Training Evaluation',\n",
    "            prediction_loss_only=True if self.compute_metrics is None else None,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=\"train\",\n",
    "        )\n",
    "\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in eval_output.metrics:\n",
    "            start_time += eval_output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        eval_output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=eval_output.num_samples,\n",
    "                num_steps=math.ceil(eval_output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        train_n_samples = len(self.train_dataset)\n",
    "        train_output.metrics.update(speed_metrics('train', start_time, train_n_samples))\n",
    "        self.log(train_output.metrics | eval_output.metrics)\n",
    "\n",
    "        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, train_output.metrics)\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, eval_output.metrics)\n",
    "\n",
    "        self._memory_tracker.stop_and_update_metrics(eval_output.metrics)\n",
    "        self._memory_tracker.stop_and_update_metrics(train_output.metrics)\n",
    "\n",
    "        # only works in Python >= 3.9\n",
    "        return train_output.metrics | eval_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=ds_train_small,\n",
    "#     eval_dataset=ds_test_small,\n",
    "#     # train_dataset=ds_train,\n",
    "#     # eval_dataset=ds_test,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[CombinedTensorBoardCallback]\n",
    "# )\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # train_dataset=ds_train_small,\n",
    "    # eval_dataset=ds_test_small,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CombinedTensorBoardCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='10125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5001/10125 1:39:23 < 1:53:11, 0.75 it/s, Epoch 1.48/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.264212</td>\n",
       "      <td>0.892417</td>\n",
       "      <td>0.948363</td>\n",
       "      <td>0.936576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>0.251483</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.949756</td>\n",
       "      <td>0.940585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.242411</td>\n",
       "      <td>0.903417</td>\n",
       "      <td>0.956124</td>\n",
       "      <td>0.943128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.242280</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.942586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.259800</td>\n",
       "      <td>0.232692</td>\n",
       "      <td>0.907167</td>\n",
       "      <td>0.952841</td>\n",
       "      <td>0.945037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>0.243924</td>\n",
       "      <td>0.907917</td>\n",
       "      <td>0.959009</td>\n",
       "      <td>0.945788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.257785</td>\n",
       "      <td>0.901750</td>\n",
       "      <td>0.935628</td>\n",
       "      <td>0.941012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>0.231894</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.950353</td>\n",
       "      <td>0.945368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2715' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 06:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer.train()\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# save model only saves the tokenizer with the model\n",
    "# https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/trainer#transformers.Trainer\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "trainer.save_model(\"test_trainer_save_model_{}_v1\".format(datetime.today().strftime(\"%Y-%m-%d\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.11214113980531693,\n",
       " 'train_accuracy': 0.96127,\n",
       " 'train_recall': 0.9673028879784151,\n",
       " 'train_f1': 0.9615281461394046,\n",
       " 'train_runtime': 518.6035,\n",
       " 'train_samples_per_second': 192.826,\n",
       " 'eval_loss': 0.22014589607715607,\n",
       " 'eval_accuracy': 0.9169,\n",
       " 'eval_recall': 0.9267864115579851,\n",
       " 'eval_f1': 0.9195157384987893,\n",
       " 'eval_runtime': 518.6035,\n",
       " 'eval_samples_per_second': 19.283,\n",
       " 'eval_steps_per_second': 0.604}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate(\n",
    "    ds_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.5350044,  2.991728 ],\n",
       "       [ 1.0069603, -2.805289 ],\n",
       "       [-2.4684582,  3.0222573],\n",
       "       [-2.1462834,  2.3476632],\n",
       "       [ 1.9798595, -3.9196928]], dtype=float32), label_ids=array([1, 0, 1, 1, 0]), metrics={'test_loss': 0.008759320713579655, 'test_accuracy': 1.0, 'test_recall': 1.0, 'test_f1': 1.0, 'test_runtime': 0.0466, 'test_samples_per_second': 107.387, 'test_steps_per_second': 21.477})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.predict(\n",
    "    ds_test.shuffle(seed=1).select(range(5))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final probability of each class is calculated by applying softmax to the values for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here marks the end of training :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the trained model from directory, and create a pipeline for direct inferencing\n",
    "\n",
    "https://discuss.huggingface.co/t/using-trainer-at-inference-time/9378/7\n",
    "\n",
    "https://discuss.huggingface.co/t/how-to-load-a-pipeline-saved-with-pipeline-save-pretrained/5373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer_loaded = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model_loaded = AutoModelForSequenceClassification.from_pretrained('test_trainer_save_model_24-11-2023_v5', output_hidden_states=True)\n",
    "\n",
    "# just load the model\n",
    "# trainer_test = Trainer(\n",
    "#     model = model_loaded\n",
    "# )\n",
    "\n",
    "# can input TrainingArgument class to make custom evaluation on a testing dataset\n",
    "# or without, then just predict\n",
    "# need to form a dataset object, cannot accept string I guess ??\n",
    "# if with a true label -> automatically perform evaluation\n",
    "# else just predict\n",
    "\n",
    "# load the pipeline\n",
    "pipeline_loaded = pipeline(\n",
    "    'text-classification',\n",
    "    model=model_loaded,\n",
    "    tokenizer=tokenizer_loaded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x7fd6c93cac40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9960368275642395},\n",
       " {'label': 'LABEL_0', 'score': 0.9783793687820435},\n",
       " {'label': 'LABEL_1', 'score': 0.9958920478820801},\n",
       " {'label': 'LABEL_1', 'score': 0.9889470934867859},\n",
       " {'label': 'LABEL_0', 'score': 0.9972668886184692}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verifying the probability result output by the model\n",
    "# and the pipeline\n",
    "# it's correct\n",
    "\n",
    "pipeline_loaded(\n",
    "    ds_test.shuffle(seed=1).select(range(5))['text'],\n",
    "    padding='max_length', max_length=tokenizer.model_max_length, truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.6495449542999268}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_loaded(\"After I bought Postal 2, i saw that there was a third one and it had modern-day graphics and i thought that was pretty cool and then I saw all the negative reviews except for the literally only postive review and i thought to myself, hmmm I wonder how bad this game is? Seeing as how the second one was badass and amazingly better than some of the AAA games we get served today, so i bought it, played it and beat it, and now i now why this game is slandered, even RWS has given up on this trainwreck but before i trash this game more i have to say there are parts of this game that are good but far from being awesome.  Like my review of Postal 2, this review will broken down in the same areas  Gameplay, Graphics, Sound and most importantly Story  Warning: this is my opinon and if anything offends you, i deeply apolgize  Gameplay 5/10: the overall gmeplay isn't all to bad, the controls aren't bad but they are a big difference from its predecessor, the main control feature i like about this game is you don't have to use the mouse scroll for changing your guns, you can now use X and C for that which for me (being a console gamer for 16 years) is a big up for me since i'm still new to the PC controls. The game praises it's so called 'advanced AI' but in reality the so-called 'advanced AI' isn't so advanced unless advanced AI means running in your line of fire and killing all the 'advanced AI' in like two seconds  Graphics 6/10: the graphics of this game are not that bad but also not that good for a game that was realeased in 2011. there are times where the graphics acutally look complete and then other times your face palming with more regert than Patrick Stewert.  Sound 8/10: this is the only department that this game did good in. The BGM of this game is amazing, the only time the BGM is bad, is that it is stuck in a never-ending loop, so in the longer levels in the game it starts to become a ear ache. The sound F/X of this game is why the sound gets an 8 out of 10 instead of being a perfect score, the gun noises in this aren't as realistic as they were Postal 2, but they aren't horrible either, at least they are actually sounds instead of one guy making 'pew-pew' noises.  Story 6/10: the only reason this game's story did better than Postal 2 is because it's more of a linear based game, so there is more room for story, but the main reason ot only did a point better was becasue it's story was all over the f***ing place, when i finally beat the game, i had no f***ing idea what the f*** happend, the story has more holes in it than swiss cheese or a Micheal Bay movie (uggh don't get me started on him)  Overall 4.5/10: Overall this game needs A LOT more work and if it was developed on for a little bit longer (i'd say like two more years) it may have been a better game but I personally think that it should have been the exact same game as the second but with a new story and graphics like Halo 3. (ahhh if only) But before i go, the nic people over at Anthology Production, are making a mod for Postal 2, and it's basically taking Postal 3 and putting it into the engine of the second game and more! So i cannot wait for it to come out. so as you can see I do NOT reccomend this game but if you really want to play this s***ty game than wait for that mod.\",\n",
    "                padding='max_length', max_length=tokenizer_loaded.model_max_length, truncation=True)   # just pass the arguments of the tokeinzer like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9889022707939148}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_loaded('I like this game!!!',\n",
    "                padding='max_length', max_length=tokenizer_loaded.model_max_length, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9889022707939148},\n",
       " {'label': 'LABEL_0', 'score': 0.9885446429252625}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_loaded([\"I like this game!!!\", \"This game sucks.\"],\n",
    "                padding='max_length', max_length=tokenizer_loaded.model_max_length, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_loaded.save_pretrained('test_trainer_save_pipeline_24-11-2023_v5')\n",
    "\n",
    "# trainer.save_model('model_test_trainer_13-11-2023')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-test-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
