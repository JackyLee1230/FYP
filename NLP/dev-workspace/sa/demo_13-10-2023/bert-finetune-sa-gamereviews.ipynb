{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4891928 entries, 0 to 4891927\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   index         int64 \n",
      " 1   app_id        int64 \n",
      " 2   app_name      object\n",
      " 3   review_text   object\n",
      " 4   review_score  int64 \n",
      " 5   review_votes  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 223.9+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "\n",
    "random.seed(13)\n",
    "\n",
    "# full dataset from Kaggle\n",
    "# dataset_path = Path('../../dataset/sa/dataset.csv').resolve()\n",
    "\n",
    "# cleaned dataset\n",
    "# dataset_heartless_path = Path('../../dataset/sa/dataset_cleaned_heartless.pkl').resolve()\n",
    "\n",
    "# load with random selection\n",
    "# dataset = pd.read_csv(dataset_path, skiprows=lambda i: i > 0 and random.random() > p)\n",
    "# dataset.head()\n",
    "\n",
    "\n",
    "# load cleaned dataset\n",
    "# dataset = pd.read_pickle(dataset_heartless_path)\n",
    "# dataset = dataset.sample(frac=p)\n",
    "# dataset.info()\n",
    "\n",
    "# copied from the first cell of eda.ipynb\n",
    "\n",
    "dataset_heartless_path = Path('../../dataset/sa/dataset_cleaned_heartless.pkl').resolve()\n",
    "\n",
    "dataset = pd.read_pickle(dataset_heartless_path)\n",
    "# dataset = dataset.sample(frac=p)      # no sampling is needed\n",
    "\n",
    "# convert the text to string object\n",
    "dataset['review_text'] = dataset['review_text'].astype('str')\n",
    "\n",
    "# drop any duplicate just in case\n",
    "dataset = dataset.drop_duplicates(keep='first')\n",
    "\n",
    "# replace -1 to 0\n",
    "# then 0 = negative, 1 = positive\n",
    "# for easier processing\n",
    "dataset['review_score'] = dataset['review_score'].replace(-1, 0)\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first do data cleaning\n",
    "\n",
    "# convert to string\n",
    "dataset['review_text'] = dataset['review_text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove some characters, but not do the stop words, stemming and lemmatizing (no doing them in Keras example ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(raw):\n",
    "    \"\"\" Remove hyperlinks and markup \"\"\"\n",
    "    result = re.sub(\"<[a][^>]*>(.+?)</[a]>\", 'Link.', raw)\n",
    "    result = re.sub('&gt;', \"\", result)\n",
    "    result = re.sub('&#x27;', \"'\", result)\n",
    "    result = re.sub('&quot;', '\"', result)\n",
    "    result = re.sub('&#x2F;', ' ', result)\n",
    "    result = re.sub('<p>', ' ', result)\n",
    "    result = re.sub('</i>', '', result)\n",
    "    result = re.sub('&#62;', '', result)\n",
    "    result = re.sub('<i>', ' ', result)\n",
    "    result = re.sub(\"\\n\", '', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(texts):\n",
    "   output = re.sub(r'\\d+', '', texts)\n",
    "   return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(x):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_whitespaces(x):\n",
    "    cleaned_string = re.sub(' +', ' ', x)\n",
    "    return cleaned_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(x):\n",
    "    cleaned_string = re.sub(r\"[^a-zA-Z0-9?!.,]+\", ' ', x)\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\",  \"!\",'\"',','))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(df, review_col_name):\n",
    "    df[review_col_name] = df[review_col_name].apply(clean)\n",
    "    df[review_col_name] = df[review_col_name].apply(deEmojify)\n",
    "    # df[review_col_name] = df[review_col_name].str.lower()\n",
    "    # df[review_col_name] = df[review_col_name].apply(remove_num)\n",
    "    # df[review_col_name] = df[review_col_name].apply(remove_symbols)\n",
    "    # df[review_col_name] = df[review_col_name].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for cleaning\n",
    "\n",
    "4.8M rows, two functions -> 30.5 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Ruined my life.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>This will be more of a ''my experience with th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>This game saved my virginity.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>• Do you like original games? • Do you like ga...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Easy to learn, hard to master.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>No r8 revolver, 10/10 will play again.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Still better than Call of Duty: Ghosts...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>cant buy skins, cases, keys, stickers - gaben ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Counter-Strike: Ok, after 9 years of unlimited...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Every server is spanish or french. I can now f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Fire in the Hole Simulator 1999</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>I never played a better first person shooter. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>WARNING : DO NOT buy this game, if you do, you...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>CS is one of the greatest and undermined games...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>It's... aight.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>I met a lot of people who slept with my mother...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Ruined my life. 10/10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>When you crouch, you lift your feet in the air...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Best shooter for 10 years. Steam evolved becau...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>One of the best FPS Games, the 1.6 Version wil...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  app_id        app_name  \\\n",
       "0       0      10  Counter-Strike   \n",
       "1       1      10  Counter-Strike   \n",
       "2       2      10  Counter-Strike   \n",
       "3       3      10  Counter-Strike   \n",
       "4       4      10  Counter-Strike   \n",
       "5       5      10  Counter-Strike   \n",
       "6       6      10  Counter-Strike   \n",
       "7       7      10  Counter-Strike   \n",
       "8       8      10  Counter-Strike   \n",
       "9       9      10  Counter-Strike   \n",
       "10     10      10  Counter-Strike   \n",
       "11     11      10  Counter-Strike   \n",
       "12     12      10  Counter-Strike   \n",
       "13     13      10  Counter-Strike   \n",
       "14     14      10  Counter-Strike   \n",
       "15     15      10  Counter-Strike   \n",
       "16     16      10  Counter-Strike   \n",
       "17     17      10  Counter-Strike   \n",
       "18     18      10  Counter-Strike   \n",
       "19     19      10  Counter-Strike   \n",
       "\n",
       "                                          review_text  review_score  \\\n",
       "0                                     Ruined my life.             1   \n",
       "1   This will be more of a ''my experience with th...             1   \n",
       "2                       This game saved my virginity.             1   \n",
       "3   • Do you like original games? • Do you like ga...             1   \n",
       "4            Easy to learn, hard to master.                       1   \n",
       "5              No r8 revolver, 10/10 will play again.             1   \n",
       "6           Still better than Call of Duty: Ghosts...             1   \n",
       "7   cant buy skins, cases, keys, stickers - gaben ...             1   \n",
       "8   Counter-Strike: Ok, after 9 years of unlimited...             1   \n",
       "9   Every server is spanish or french. I can now f...             1   \n",
       "10                    Fire in the Hole Simulator 1999             1   \n",
       "11  I never played a better first person shooter. ...             1   \n",
       "12  WARNING : DO NOT buy this game, if you do, you...             1   \n",
       "13  CS is one of the greatest and undermined games...             1   \n",
       "14                                     It's... aight.             1   \n",
       "15  I met a lot of people who slept with my mother...             1   \n",
       "16                              Ruined my life. 10/10             1   \n",
       "17  When you crouch, you lift your feet in the air...             1   \n",
       "18  Best shooter for 10 years. Steam evolved becau...             1   \n",
       "19  One of the best FPS Games, the 1.6 Version wil...             1   \n",
       "\n",
       "    review_votes  \n",
       "0              0  \n",
       "1              1  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "5              1  \n",
       "6              1  \n",
       "7              1  \n",
       "8              1  \n",
       "9              0  \n",
       "10             0  \n",
       "11             1  \n",
       "12             1  \n",
       "13             1  \n",
       "14             1  \n",
       "15             1  \n",
       "16             1  \n",
       "17             1  \n",
       "18             0  \n",
       "19             1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning(dataset, 'review_text')\n",
    "\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4891259 entries, 0 to 4891927\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   index         int64 \n",
      " 1   app_id        int64 \n",
      " 2   app_name      object\n",
      " 3   review_text   object\n",
      " 4   review_score  int64 \n",
      " 5   review_votes  int64 \n",
      " 6   num_of_words  int64 \n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 298.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# remove rows have all whitespaces\n",
    "dataset['num_of_words'] = dataset['review_text'].apply(lambda x:len(str(x).split()))\n",
    "dataset = dataset[dataset['num_of_words'] > 0]\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0abf5_row0_col0, #T_0abf5_row0_col1 {\n",
       "  background-color: #fcfbfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0abf5_row1_col0, #T_0abf5_row1_col1 {\n",
       "  background-color: #3f007d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0abf5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0abf5_level0_col0\" class=\"col_heading level0 col0\" >review_score</th>\n",
       "      <th id=\"T_0abf5_level0_col1\" class=\"col_heading level0 col1\" >review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0abf5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0abf5_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_0abf5_row0_col1\" class=\"data row0 col1\" >780927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0abf5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0abf5_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_0abf5_row1_col1\" class=\"data row1 col1\" >4110332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb7bf9e250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = dataset.groupby('review_score').count()['review_text'].reset_index().sort_values(by='review_score',ascending=True)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, testing and validation dataset\n",
    "\n",
    "(Training + Testing) : Validation = 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['review_text']\n",
    "y = dataset['review_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_test, X_valid, y_train_test, y_valid = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978252\n",
      "978252\n",
      "3913007\n",
      "3913007\n"
     ]
    }
   ],
   "source": [
    "print(len(X_valid))\n",
    "print(len(y_valid))\n",
    "print(len(X_train_test))\n",
    "print(len(y_train_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a balanced dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "# oversampling = RandomOverSampler(sampling_strategy=0.5)     # raise the ratio minority_data:majority_data as 1 (i.e. equal number of samples)\n",
    "under = RandomUnderSampler(sampling_strategy=1.0, random_state=13)          # then select ? of it\n",
    "\n",
    "# X_train_resampled, y_train_resampled = oversampling.fit_resample(X_train.to_numpy().reshape(-1, 1), y_train.to_numpy().reshape(-1, 1))\n",
    "# X_train_resampled, y_train_resampled = under.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "X_train_test_resampled, y_train__test_resampled = under.fit_resample(X_train_test.to_numpy().reshape(-1, 1), y_train_test.to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train_test_resampled, y_train__test_resampled, random_state=13, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1124715\n",
      "1124715\n",
      "124969\n",
      "124969\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging face stuff :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original shape of X_train does not match with the requirement (need a flattened 1-D array of strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1124715, 1)\n",
      "(1124715,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# create a dataset object for handling large amount of data\n",
    "from datasets import Dataset\n",
    "\n",
    "ds_train = Dataset.from_dict({\n",
    "    \"text\": [str(s) for s in list(X_train.flatten())],\n",
    "    \"label\": list(y_train)\n",
    "})\n",
    "\n",
    "ds_test = Dataset.from_dict({\n",
    "    \"text\": [str(s) for s in list(X_test.flatten())],\n",
    "    \"label\": list(y_test)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"text\"], max_length=tokenizer.model_max_length, truncation=True)\n",
    "\n",
    "# the tokenizer only accept list of strings\n",
    "# tokenized_data = tokenizer(ds_train['text'], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "# tokenized_data = dict(tokenized_data)\n",
    "\n",
    "# labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 1124715\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 124969\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Trainer API backend\n",
    "\n",
    "Copy from https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer\n",
    "\n",
    "TODO: experiment on the TrainingArguments class, e.g. hyperparameters  \n",
    "and also different parameters in 'Evaluate' library\n",
    "\n",
    "Also do logging about the training loss for each epoch (read: https://discuss.huggingface.co/t/using-tensorboard-summarywriter-with-huggingface-trainerapi/23015/5 later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1124715/1124715 [01:48<00:00, 10342.70 examples/s]\n",
      "Map: 100%|██████████| 124969/124969 [00:12<00:00, 10237.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"text\"], padding='max_length', max_length=tokenizer.model_max_length, truncation=True)\n",
    "\n",
    "# apply tokenizer to the dataset\n",
    "ds_train = ds_train.map(tokenize_dataset, batched=True)\n",
    "ds_test = ds_test.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting 100k samples (roughly 10%) for training (and testing the program flow\n",
    "# and 10k samples for testing (reduce time)\n",
    "n_train = 100 * 1000\n",
    "n_test = 10 * 1000\n",
    "\n",
    "ds_train_small = ds_train.shuffle(seed=42).select(range(n_train))\n",
    "ds_test_small = ds_test.shuffle(seed=13).select(range(n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot both training and testing loss on the same graph\n",
    "\n",
    "https://stackoverflow.com/questions/73281901/is-there-a-way-to-plot-training-and-validation-losses-on-the-same-graph-with-hug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import TrainerCallback\n",
    "from transformers.integrations import is_tensorboard_available\n",
    "\n",
    "def custom_rewrite_logs(d, mode):\n",
    "    '''If you want to combine train and eval for other metrics besides the loss then custom_rewrite_logs should be modified accordingly.'''\n",
    "    new_d = {}\n",
    "    eval_prefix = \"eval_\"\n",
    "    eval_prefix_len = len(eval_prefix)\n",
    "    test_prefix = \"test_\"\n",
    "    test_prefix_len = len(test_prefix)\n",
    "\n",
    "    # we combine loss, accuracy, recall and f1\n",
    "\n",
    "    for k, v in d.items():\n",
    "        if mode == 'eval' and k.startswith(eval_prefix):\n",
    "            if k[eval_prefix_len:] == 'loss':\n",
    "                new_d[\"combined/\" + k[eval_prefix_len:]] = v\n",
    "\n",
    "            elif k[eval_prefix_len:] == 'accuracy':\n",
    "                new_d[\"combined/\" + k[eval_prefix_len:]] = v\n",
    "\n",
    "            elif k[eval_prefix_len:] == 'recall':\n",
    "                new_d[\"combined/\" + k[eval_prefix_len:]] = v\n",
    "\n",
    "            elif k[eval_prefix_len:] == 'f1':\n",
    "                new_d[\"combined/\" + k[eval_prefix_len:]] = v\n",
    "            \n",
    "        elif mode == 'test' and k.startswith(test_prefix):\n",
    "            if k[test_prefix_len:] == 'loss':\n",
    "                new_d[\"combined/\" + k[test_prefix_len:]] = v\n",
    "\n",
    "            elif k[test_prefix_len:] == 'accuracy':\n",
    "                new_d[\"combined/\" + k[test_prefix_len:]] = v\n",
    "\n",
    "            elif k[test_prefix_len:] == 'recall':\n",
    "                new_d[\"combined/\" + k[test_prefix_len:]] = v    \n",
    "\n",
    "            elif k[test_prefix_len:] == 'f1':\n",
    "                new_d[\"combined/\" + k[test_prefix_len:]] = v\n",
    "\n",
    "        elif mode == 'train':\n",
    "            if k == 'loss':\n",
    "                new_d[\"combined/\" + k] = v\n",
    "\n",
    "            elif k == 'train_accuracy':\n",
    "                new_d[\"combined/\" + 'accuracy'] = v\n",
    "\n",
    "            elif k == 'train_recall':\n",
    "                new_d[\"combined/\" + 'recall'] = v\n",
    "\n",
    "            elif k == 'train_f1':\n",
    "                new_d[\"combined/\" + 'f1'] = v\n",
    "                \n",
    "    return new_d\n",
    "\n",
    "\n",
    "class CombinedTensorBoardCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A [`TrainerCallback`] that sends the logs to [TensorBoard](https://www.tensorflow.org/tensorboard).\n",
    "    Args:\n",
    "        tb_writer (`SummaryWriter`, *optional*):\n",
    "            The writer to use. Will instantiate one if not set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tb_writers=None):\n",
    "        has_tensorboard = is_tensorboard_available()\n",
    "        if not has_tensorboard:\n",
    "            raise RuntimeError(\n",
    "                \"TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or\"\n",
    "                \" install tensorboardX.\"\n",
    "            )\n",
    "        if has_tensorboard:\n",
    "            try:\n",
    "                from torch.utils.tensorboard import SummaryWriter  # noqa: F401\n",
    "\n",
    "                self._SummaryWriter = SummaryWriter\n",
    "            except ImportError:\n",
    "                try:\n",
    "                    from tensorboardX import SummaryWriter\n",
    "\n",
    "                    self._SummaryWriter = SummaryWriter\n",
    "                except ImportError:\n",
    "                    self._SummaryWriter = None\n",
    "        else:\n",
    "            self._SummaryWriter = None\n",
    "        self.tb_writers = tb_writers\n",
    "\n",
    "    def _init_summary_writer(self, args, log_dir=None):\n",
    "        log_dir = log_dir or args.logging_dir\n",
    "        if self._SummaryWriter is not None:\n",
    "            self.tb_writers = dict(train=self._SummaryWriter(log_dir=os.path.join(log_dir, 'train')),\n",
    "                                   eval=self._SummaryWriter(log_dir=os.path.join(log_dir, 'eval')))\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        if not state.is_world_process_zero:\n",
    "            return\n",
    "\n",
    "        log_dir = None\n",
    "\n",
    "        if state.is_hyper_param_search:\n",
    "            trial_name = state.trial_name\n",
    "            if trial_name is not None:\n",
    "                log_dir = os.path.join(args.logging_dir, trial_name)\n",
    "\n",
    "        if self.tb_writers is None:\n",
    "            self._init_summary_writer(args, log_dir)\n",
    "\n",
    "        for k, tbw in self.tb_writers.items():\n",
    "            tbw.add_text(\"args\", args.to_json_string())\n",
    "            if \"model\" in kwargs:\n",
    "                model = kwargs[\"model\"]\n",
    "                if hasattr(model, \"config\") and model.config is not None:\n",
    "                    model_config_json = model.config.to_json_string()\n",
    "                    tbw.add_text(\"model_config\", model_config_json)\n",
    "            # Version of TensorBoard coming from tensorboardX does not have this method.\n",
    "            if hasattr(tbw, \"add_hparams\"):\n",
    "                tbw.add_hparams(args.to_sanitized_dict(), metric_dict={})\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not state.is_world_process_zero:\n",
    "            return\n",
    "\n",
    "        if self.tb_writers is None:\n",
    "            self._init_summary_writer(args)\n",
    "\n",
    "        for tbk, tbw in self.tb_writers.items():\n",
    "            logs_new = custom_rewrite_logs(logs, mode=tbk)\n",
    "            for k, v in logs_new.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    tbw.add_scalar(k, v, state.global_step)\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"Trainer is attempting to log a value of \"\n",
    "                        f'\"{v}\" of type {type(v)} for key \"{k}\" as a scalar. '\n",
    "                        \"This invocation of Tensorboard's writer.add_scalar() \"\n",
    "                        \"is incorrect so we dropped this attribute.\"\n",
    "                    )\n",
    "            tbw.flush()\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        for tbw in self.tb_writers.values():\n",
    "            tbw.close()\n",
    "        self.tb_writers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Specify where to save the checkpoints from your training:\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer_24-11-2023_v5\",\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  per_device_eval_batch_size=32,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  weight_decay=0.01,\n",
    "                                #   warmup_steps=10000,   # seems not required in fine-tuning, as we have less than 10k steps\n",
    "                                  num_train_epochs=3,\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  save_strategy=\"steps\",\n",
    "                                  save_steps=500,\n",
    "                                  eval_steps=500,\n",
    "                                  metric_for_best_model='eval_loss',\n",
    "                                  load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_recall = evaluate.load('recall')\n",
    "metric_f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred      # logits: an n*num_of_class array with probability, e.g. [[ 1.9851098, -1.6966375],[ 2.7240963, -2.372472 ],...], labels = true labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = metric_acc.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels)['recall']\n",
    "    f1_score = metric_f1.compute(predictions=predictions, references=labels, pos_label=1)['f1']\n",
    "    # rocauc = metric_rocauc.compute(predictions=predictions, references=labels)['roc_auc']\n",
    "    return {'accuracy': acc, \"recall\": recall, \"f1\": f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override original trainer to add evaluation on training dataset as well\n",
    "\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from transformers.trainer_utils import speed_metrics\n",
    "from transformers.debug_utils import DebugOption\n",
    "from transformers.utils import  is_torch_tpu_available\n",
    "\n",
    "if is_torch_tpu_available(check_device=False):\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.debug.metrics as met\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def evaluate(self, \n",
    "                 eval_dataset: Optional[Dataset] = None,\n",
    "                 ignore_keys: Optional[List[str]] = None,\n",
    "                 metric_key_prefix: str = \"eval\"\n",
    "                 ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "            Run evaluation and returns metrics.\n",
    "            The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n",
    "            (pass it to the init `compute_metrics` argument).\n",
    "            You can also subclass and override this method to inject custom behavior.\n",
    "            Args:\n",
    "                eval_dataset (`Dataset`, *optional*):\n",
    "                    Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n",
    "                    not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n",
    "                    method.\n",
    "                ignore_keys (`Lst[str]`, *optional*):\n",
    "                    A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
    "                    gathering predictions.\n",
    "                metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n",
    "                    An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
    "                    \"eval_bleu\" if the prefix is \"eval\" (default)\n",
    "            Returns:\n",
    "                A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n",
    "                dictionary also contains the epoch number which comes from the training state.\n",
    "            \"\"\"\n",
    "        # memory metrics - must set up as early as possible\n",
    "        self._memory_tracker.start()\n",
    "\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        train_dataloader = self.get_train_dataloader()\n",
    "        start_time = time.time()\n",
    "\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        eval_output = eval_loop(\n",
    "            eval_dataloader,\n",
    "            description=\"Evaluation\",\n",
    "            # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "            # self.args.prediction_loss_only\n",
    "            prediction_loss_only=True if self.compute_metrics is None else None,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=metric_key_prefix,\n",
    "        )\n",
    "\n",
    "        train_output = eval_loop(\n",
    "            train_dataloader,\n",
    "            description='Training Evaluation',\n",
    "            prediction_loss_only=True if self.compute_metrics is None else None,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=\"train\",\n",
    "        )\n",
    "\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in eval_output.metrics:\n",
    "            start_time += eval_output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        eval_output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=eval_output.num_samples,\n",
    "                num_steps=math.ceil(eval_output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        train_n_samples = len(self.train_dataset)\n",
    "        train_output.metrics.update(speed_metrics('train', start_time, train_n_samples))\n",
    "        self.log(train_output.metrics | eval_output.metrics)\n",
    "\n",
    "        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, train_output.metrics)\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, eval_output.metrics)\n",
    "\n",
    "        self._memory_tracker.stop_and_update_metrics(eval_output.metrics)\n",
    "        self._memory_tracker.stop_and_update_metrics(train_output.metrics)\n",
    "\n",
    "        # only works in Python >= 3.9\n",
    "        return train_output.metrics | eval_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=ds_train_small,\n",
    "#     eval_dataset=ds_test_small,\n",
    "#     # train_dataset=ds_train,\n",
    "#     # eval_dataset=ds_test,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[CombinedTensorBoardCallback]\n",
    "# )\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train_small,\n",
    "    eval_dataset=ds_test_small,\n",
    "    # train_dataset=ds_train,\n",
    "    # eval_dataset=ds_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CombinedTensorBoardCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 3:30:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.282712</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>0.851230</td>\n",
       "      <td>0.884830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.243170</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.877001</td>\n",
       "      <td>0.899840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>0.233188</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.891449</td>\n",
       "      <td>0.903532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>0.231922</td>\n",
       "      <td>0.906500</td>\n",
       "      <td>0.895744</td>\n",
       "      <td>0.907526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.242500</td>\n",
       "      <td>0.226868</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.899453</td>\n",
       "      <td>0.910204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.240500</td>\n",
       "      <td>0.235578</td>\n",
       "      <td>0.912100</td>\n",
       "      <td>0.897696</td>\n",
       "      <td>0.912754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.184200</td>\n",
       "      <td>0.260092</td>\n",
       "      <td>0.914200</td>\n",
       "      <td>0.901015</td>\n",
       "      <td>0.914948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.243925</td>\n",
       "      <td>0.910900</td>\n",
       "      <td>0.882273</td>\n",
       "      <td>0.910263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.231016</td>\n",
       "      <td>0.915500</td>\n",
       "      <td>0.915072</td>\n",
       "      <td>0.917311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.165400</td>\n",
       "      <td>0.220146</td>\n",
       "      <td>0.916900</td>\n",
       "      <td>0.926786</td>\n",
       "      <td>0.919516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.224646</td>\n",
       "      <td>0.919700</td>\n",
       "      <td>0.901796</td>\n",
       "      <td>0.920028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.165200</td>\n",
       "      <td>0.220918</td>\n",
       "      <td>0.917200</td>\n",
       "      <td>0.894768</td>\n",
       "      <td>0.917150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.267870</td>\n",
       "      <td>0.917300</td>\n",
       "      <td>0.909606</td>\n",
       "      <td>0.918482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>0.294476</td>\n",
       "      <td>0.918700</td>\n",
       "      <td>0.900234</td>\n",
       "      <td>0.918984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.919900</td>\n",
       "      <td>0.912534</td>\n",
       "      <td>0.921076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.279903</td>\n",
       "      <td>0.920100</td>\n",
       "      <td>0.904920</td>\n",
       "      <td>0.920648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.265304</td>\n",
       "      <td>0.919300</td>\n",
       "      <td>0.910387</td>\n",
       "      <td>0.920359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>0.273199</td>\n",
       "      <td>0.917900</td>\n",
       "      <td>0.908239</td>\n",
       "      <td>0.918914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.179933759765625, metrics={'train_runtime': 12653.4358, 'train_samples_per_second': 23.709, 'train_steps_per_second': 0.741, 'total_flos': 7.8933316608e+16, 'train_loss': 0.179933759765625, 'epoch': 3.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# save model only saves the tokenizer with the model\n",
    "# https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/trainer#transformers.Trainer\n",
    "\n",
    "trainer.save_model(\"test_trainer_save_model_24-11-2023_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.11214113980531693,\n",
       " 'train_accuracy': 0.96127,\n",
       " 'train_recall': 0.9673028879784151,\n",
       " 'train_f1': 0.9615281461394046,\n",
       " 'train_runtime': 518.6035,\n",
       " 'train_samples_per_second': 192.826,\n",
       " 'eval_loss': 0.22014589607715607,\n",
       " 'eval_accuracy': 0.9169,\n",
       " 'eval_recall': 0.9267864115579851,\n",
       " 'eval_f1': 0.9195157384987893,\n",
       " 'eval_runtime': 518.6035,\n",
       " 'eval_samples_per_second': 19.283,\n",
       " 'eval_steps_per_second': 0.604}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(\n",
    "    ds_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.5350044,  2.991728 ],\n",
       "       [ 1.0069603, -2.805289 ],\n",
       "       [-2.4684582,  3.0222573],\n",
       "       [-2.1462834,  2.3476632],\n",
       "       [ 1.9798595, -3.9196928]], dtype=float32), label_ids=array([1, 0, 1, 1, 0]), metrics={'test_loss': 0.008759320713579655, 'test_accuracy': 1.0, 'test_recall': 1.0, 'test_f1': 1.0, 'test_runtime': 0.0466, 'test_samples_per_second': 107.387, 'test_steps_per_second': 21.477})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(\n",
    "    ds_test.shuffle(seed=1).select(range(5))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final probability of each class is calculated by applying softmax to the values for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here marks the end of training :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the trained model from directory, and create a pipeline for direct inferencing\n",
    "\n",
    "https://discuss.huggingface.co/t/using-trainer-at-inference-time/9378/7\n",
    "\n",
    "https://discuss.huggingface.co/t/how-to-load-a-pipeline-saved-with-pipeline-save-pretrained/5373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer_loaded = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model_loaded = AutoModelForSequenceClassification.from_pretrained('test_trainer_save_model_24-11-2023_v5', output_hidden_states=True)\n",
    "\n",
    "# just load the model\n",
    "# trainer_test = Trainer(\n",
    "#     model = model_loaded\n",
    "# )\n",
    "\n",
    "# can input TrainingArgument class to make custom evaluation on a testing dataset\n",
    "# or without, then just predict\n",
    "# need to form a dataset object, cannot accept string I guess ??\n",
    "# if with a true label -> automatically perform evaluation\n",
    "# else just predict\n",
    "\n",
    "# load the pipeline\n",
    "pipeline_loaded = pipeline(\n",
    "    'text-classification',\n",
    "    model=model_loaded,\n",
    "    tokenizer=tokenizer_loaded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x7fd6c93cac40>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9960368275642395},\n",
       " {'label': 'LABEL_0', 'score': 0.9783793687820435},\n",
       " {'label': 'LABEL_1', 'score': 0.9958920478820801},\n",
       " {'label': 'LABEL_1', 'score': 0.9889470934867859},\n",
       " {'label': 'LABEL_0', 'score': 0.9972668886184692}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying the probability result output by the model\n",
    "# and the pipeline\n",
    "# it's correct\n",
    "\n",
    "pipeline_loaded(\n",
    "    ds_test.shuffle(seed=1).select(range(5))['text'],\n",
    "    padding='max_length', max_length=tokenizer.model_max_length, truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.6495449542999268}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_loaded(\"After I bought Postal 2, i saw that there was a third one and it had modern-day graphics and i thought that was pretty cool and then I saw all the negative reviews except for the literally only postive review and i thought to myself, hmmm I wonder how bad this game is? Seeing as how the second one was badass and amazingly better than some of the AAA games we get served today, so i bought it, played it and beat it, and now i now why this game is slandered, even RWS has given up on this trainwreck but before i trash this game more i have to say there are parts of this game that are good but far from being awesome.  Like my review of Postal 2, this review will broken down in the same areas  Gameplay, Graphics, Sound and most importantly Story  Warning: this is my opinon and if anything offends you, i deeply apolgize  Gameplay 5/10: the overall gmeplay isn't all to bad, the controls aren't bad but they are a big difference from its predecessor, the main control feature i like about this game is you don't have to use the mouse scroll for changing your guns, you can now use X and C for that which for me (being a console gamer for 16 years) is a big up for me since i'm still new to the PC controls. The game praises it's so called 'advanced AI' but in reality the so-called 'advanced AI' isn't so advanced unless advanced AI means running in your line of fire and killing all the 'advanced AI' in like two seconds  Graphics 6/10: the graphics of this game are not that bad but also not that good for a game that was realeased in 2011. there are times where the graphics acutally look complete and then other times your face palming with more regert than Patrick Stewert.  Sound 8/10: this is the only department that this game did good in. The BGM of this game is amazing, the only time the BGM is bad, is that it is stuck in a never-ending loop, so in the longer levels in the game it starts to become a ear ache. The sound F/X of this game is why the sound gets an 8 out of 10 instead of being a perfect score, the gun noises in this aren't as realistic as they were Postal 2, but they aren't horrible either, at least they are actually sounds instead of one guy making 'pew-pew' noises.  Story 6/10: the only reason this game's story did better than Postal 2 is because it's more of a linear based game, so there is more room for story, but the main reason ot only did a point better was becasue it's story was all over the f***ing place, when i finally beat the game, i had no f***ing idea what the f*** happend, the story has more holes in it than swiss cheese or a Micheal Bay movie (uggh don't get me started on him)  Overall 4.5/10: Overall this game needs A LOT more work and if it was developed on for a little bit longer (i'd say like two more years) it may have been a better game but I personally think that it should have been the exact same game as the second but with a new story and graphics like Halo 3. (ahhh if only) But before i go, the nic people over at Anthology Production, are making a mod for Postal 2, and it's basically taking Postal 3 and putting it into the engine of the second game and more! So i cannot wait for it to come out. so as you can see I do NOT reccomend this game but if you really want to play this s***ty game than wait for that mod.\",\n",
    "                padding='max_length', max_length=tokenizer_loaded.model_max_length, truncation=True)   # just pass the arguments of the tokeinzer like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9889022707939148}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_loaded('I like this game!!!',\n",
    "                padding='max_length', max_length=tokenizer_loaded.model_max_length, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9889022707939148},\n",
       " {'label': 'LABEL_0', 'score': 0.9885446429252625}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_loaded([\"I like this game!!!\", \"This game sucks.\"],\n",
    "                padding='max_length', max_length=tokenizer_loaded.model_max_length, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_loaded.save_pretrained('test_trainer_save_pipeline_24-11-2023_v5')\n",
    "\n",
    "# trainer.save_model('model_test_trainer_13-11-2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF/Keras backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 23:50:04.841218: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-13 23:50:04.862347: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-13 23:50:04.862379: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-13 23:50:04.862396: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-13 23:50:04.866986: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 23:50:05.859066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-13 23:50:05.862170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-13 23:50:05.862203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-13 23:50:05.862993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-13 23:50:05.863018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-13 23:50:05.863030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-13 23:50:05.939568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-13 23:50:05.939613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-13 23:50:05.939619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-11-13 23:50:05.939641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-13 23:50:05.939654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21472 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7436/125063 [>.............................] - ETA: 3:24:53 - loss: 0.2965 - accuracy: 0.8741"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/FYP/NLP/dev-workspace/sa/demo_13-10-2023/bert-finetune-sa-gamereviews.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/FYP/NLP/dev-workspace/sa/demo_13-10-2023/bert-finetune-sa-gamereviews.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Lower learning rates are often better for fine-tuning transformers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/FYP/NLP/dev-workspace/sa/demo_13-10-2023/bert-finetune-sa-gamereviews.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mAdam(\u001b[39m3e-5\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])  \u001b[39m# No loss argument!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/FYP/NLP/dev-workspace/sa/demo_13-10-2023/bert-finetune-sa-gamereviews.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model_history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(tf_dataset)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/engine/training.py:1789\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1787\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1788\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1789\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1790\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1791\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m-> 1170\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogbar\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseen, \u001b[39mlist\u001b[39;49m(logs\u001b[39m.\u001b[39;49mitems()), finalize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/utils/generic_utils.py:296\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    293\u001b[0m         info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m info\n\u001b[0;32m--> 296\u001b[0m     io_utils\u001b[39m.\u001b[39;49mprint_msg(message, line_break\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    297\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/keras/src/utils/io_utils.py:81\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(message)\n\u001b[0;32m---> 81\u001b[0m     sys\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49mflush()\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     logging\u001b[39m.\u001b[39minfo(message)\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/site-packages/ipykernel/iostream.py:580\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(evt\u001b[39m.\u001b[39mset)\n\u001b[1;32m    579\u001b[0m     \u001b[39m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_timeout):\n\u001b[1;32m    581\u001b[0m         \u001b[39m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    582\u001b[0m         \u001b[39m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIOStream.flush timed out\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39m__stderr__)\n\u001b[1;32m    584\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniforge3/envs/fyp-test-wsl/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# prepare tf dataset\n",
    "tf_dataset = model.prepare_tf_dataset(ds_train, batch_size=16, shuffle=True, tokenizer=tokenizer)\n",
    "\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5), metrics=['accuracy'])  # No loss argument!\n",
    "\n",
    "model_history = model.fit(tf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model.save_pretrained(f'bert-finetune-sa-gamereviews_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-test-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
