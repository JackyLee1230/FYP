{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the notebook \"steam-games-reviews-analysis-sentiment-analysis.ipynb\"\n",
    "\n",
    "Previous sections and Section 5.5\n",
    "\n",
    "We build our own testing script of the model for building the API and other stuff.\n",
    "\n",
    "Should provide interface for dataframe, as well as single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Cleaning For Sentiment Processing\n",
    "\n",
    "Includes null values cleaning, duplicate values removing etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michaelcheng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But after we've seen that the missing values are only in app name feature, we will ignore them anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Values Removing\n",
    "\n",
    "Appear in the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review = review.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make a function to clean some basic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(raw):\n",
    "    \"\"\" Remove hyperlinks and markup \"\"\"\n",
    "    result = re.sub(\"<[a][^>]*>(.+?)</[a]>\", 'Link.', raw)\n",
    "    result = re.sub('&gt;', \"\", result)\n",
    "    result = re.sub('&#x27;', \"'\", result)\n",
    "    result = re.sub('&quot;', '\"', result)\n",
    "    result = re.sub('&#x2F;', ' ', result)\n",
    "    result = re.sub('<p>', ' ', result)\n",
    "    result = re.sub('</i>', '', result)\n",
    "    result = re.sub('&#62;', '', result)\n",
    "    result = re.sub('<i>', ' ', result)\n",
    "    result = re.sub(\"\\n\", '', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make function to remove numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(texts):\n",
    "   output = re.sub(r'\\d+', '', texts)\n",
    "   return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make function to remove emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(x):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make function to unify whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_whitespaces(x):\n",
    "    cleaned_string = re.sub(' +', ' ', x)\n",
    "    return cleaned_string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make function to remove symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(x):\n",
    "    cleaned_string = re.sub(r\"[^a-zA-Z0-9?!.,]+\", ' ', x)\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make function to remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\",  \"!\",'\"',','))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make function to remove stopwords\n",
    "\n",
    "Stopwords are words that occur frequently in a language and are often grammatical in nature, such as articles (e.g., \"the,\" \"a\"), prepositions (e.g., \"in,\" \"on\"), conjunctions (e.g., \"and,\" \"but\"), and pronouns (e.g., \"he,\" \"she\"). These words are necessary for constructing sentences and conveying grammatical structure, but they often do not contribute much to the overall meaning of the text.\n",
    "\n",
    "- noise reduction\n",
    "- Improved Analysis Accuracy\n",
    "- faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=set(stopwords.words(\"english\"))\n",
    "stemmer=PorterStemmer()\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "def remove_stopword(text):\n",
    "   text=[word.lower() for word in text.split() if word.lower() not in stop]\n",
    "   return \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make function to use stemming to normalize words\n",
    "\n",
    "Reduce the words to its stem to reduce dimension.\n",
    "e.g. \n",
    "\n",
    "cared ----> care\n",
    "\n",
    "university ----> univers\n",
    "\n",
    "fairly ----> fair\n",
    "\n",
    "easily ----> easili\n",
    "\n",
    "singing ----> sing\n",
    "\n",
    "sings ----> sing\n",
    "\n",
    "sung ----> sung\n",
    "\n",
    "singer ----> singer\n",
    "\n",
    "sportingly ----> sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "def stemming(text):\n",
    "   stem=[]\n",
    "   # stopword = stopwords.words('english')\n",
    "   snowball_stemmer = SnowballStemmer('english')\n",
    "   word_tokens = nltk.word_tokenize(text)\n",
    "   stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
    "   stem=' '.join(stemmed_word)\n",
    "   return stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we combine all the cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(df,review):\n",
    "    df[review] = df[review].apply(clean)\n",
    "    df[review] = df[review].apply(deEmojify)\n",
    "    df[review] = df[review].str.lower()\n",
    "    df[review] = df[review].apply(remove_num)\n",
    "    df[review] = df[review].apply(remove_symbols)\n",
    "    df[review] = df[review].apply(remove_punctuation)\n",
    "    df[review] = df[review].apply(remove_stopword)\n",
    "    df[review] = df[review].apply(unify_whitespaces)\n",
    "    df[review] = df[review].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of applying text cleaning to a text\n",
    "\n",
    "# cleaning(review,'review_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Creating pipeline for tokenizing and modelling\n",
    "\n",
    "We create an end to end pipeline for training, testing and future usage of the models.\n",
    "\n",
    "It involves Count Vectorizer, Tfidf Transformer and a ML classifier (e.g. Random Forest Classifier)\n",
    "\n",
    "We can build another pipeline using TfidfVectorizer (a combination of Count Vectorizer and Tfidf Transformer). It is the same as applying Count Vectorizer then Tfidf Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features: If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "# Otherwise, all features are used.\n",
    "# vect = CountVectorizer(stop_words= \"english\",max_features=3000)\n",
    "# tfidf = TfidfTransformer()\n",
    "# model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# load the model to the pipeline\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "filename = Path(\"steam-games-reviews-analysis-sentiment-analysis_model_12-09-2023.sav\")\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# we save the count vectorizer in section 5.5 also\n",
    "filename_count_vec = Path('steam-games-reviews-analysis-sentiment-analysis_count_vectorizer_12-09-2023.pkl')\n",
    "loaded_count_vec = pickle.load(open(filename_count_vec, \"rb\"))\n",
    "\n",
    "# we save the fit tfidf (fit in pipeline2.fit())\n",
    "filename_tfidf = Path('steam-games-reviews-analysis-sentiment-analysis_tfidf_12-09-2023.pkl')\n",
    "loaded_tfidf = pickle.load(open(filename_tfidf, \"rb\"))\n",
    "\n",
    "pipeline_target = Pipeline([\n",
    "    ('count_vectorizer', loaded_count_vec),\n",
    "    ('tfidf', loaded_tfidf),\n",
    "    ('model', loaded_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction, the list of text should first passed through data-cleaning\n",
    "\n",
    "Then pass the list of text to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(s_list:list[str]):\n",
    "    _s_list = list(map(clean, s_list))\n",
    "    _s_list = list(map(deEmojify, _s_list))\n",
    "    _s_list = list(map(lambda x: x.lower(), _s_list))\n",
    "    _s_list = list(map(remove_num, _s_list))\n",
    "    _s_list = list(map(remove_symbols, _s_list))\n",
    "    _s_list = list(map(remove_punctuation, _s_list))\n",
    "    _s_list = list(map(remove_stopword, _s_list))\n",
    "    _s_list = list(map(unify_whitespaces, _s_list))\n",
    "    _s_list = list(map(stemming, _s_list))\n",
    "    return _s_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input\n",
    "\n",
    "testing_list = [\n",
    "    'This game is freaking good! I enjoy the graphics.',\n",
    "    'GOAT !!!',\n",
    "    'The only reason this game exists is for us to put shit on it.',\n",
    "    'Disappointing. It feels an EA game with lots of DLC to be released… Not recommending.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['game freak good enjoy graphic',\n",
       " 'goat',\n",
       " 'reason game exist us put shit',\n",
       " 'disappoint feel ea game lot dlc releas recommend']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_list_2 = cleaning(testing_list)\n",
    "testing_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1, -1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_target.predict(testing_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: positive\n",
    "\n",
    "-1: negative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
